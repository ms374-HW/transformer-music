{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install music21\n!pip install miditoolkit \n!pip install miditok","metadata":{"execution":{"iopub.status.busy":"2021-09-05T19:28:23.275314Z","iopub.execute_input":"2021-09-05T19:28:23.275803Z","iopub.status.idle":"2021-09-05T19:29:08.295917Z","shell.execute_reply.started":"2021-09-05T19:28:23.275693Z","shell.execute_reply":"2021-09-05T19:29:08.294920Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting music21\n  Downloading music21-6.7.1.tar.gz (19.2 MB)\n\u001b[K     |████████████████████████████████| 19.2 MB 4.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from music21) (4.0.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from music21) (1.0.1)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from music21) (8.8.0)\nCollecting webcolors\n  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: music21\n  Building wheel for music21 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for music21: filename=music21-6.7.1-py3-none-any.whl size=21941692 sha256=0a65f5374395f9f028d8170aeb52c9d41072f64dfe25f14209e42c45f6573b03\n  Stored in directory: /root/.cache/pip/wheels/72/44/61/90e4e65262ca1b4d9f707527b540729ce3f64e00fc6b38d54c\nSuccessfully built music21\nInstalling collected packages: webcolors, music21\nSuccessfully installed music21-6.7.1 webcolors-1.11.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting miditoolkit\n  Downloading miditoolkit-0.1.14.tar.gz (18 kB)\nRequirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from miditoolkit) (1.19.5)\nCollecting mido>=1.1.16\n  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n\u001b[K     |████████████████████████████████| 51 kB 2.9 MB/s eta 0:00:011\n\u001b[?25hBuilding wheels for collected packages: miditoolkit\n  Building wheel for miditoolkit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for miditoolkit: filename=miditoolkit-0.1.14-py3-none-any.whl size=19537 sha256=cf5f07f24bbf7945e037f06dfdc564b90cc10129eff4330bd83fc88b41013398\n  Stored in directory: /root/.cache/pip/wheels/00/ac/58/1527193cdf6d74e3d056086f1be24f4fcb465c45212624b60c\nSuccessfully built miditoolkit\nInstalling collected packages: mido, miditoolkit\nSuccessfully installed miditoolkit-0.1.14 mido-1.2.10\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting miditok\n  Downloading miditok-0.1.7-py3-none-any.whl (50 kB)\n\u001b[K     |████████████████████████████████| 50 kB 2.5 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: miditoolkit>=0.1.14 in /opt/conda/lib/python3.7/site-packages (from miditok) (0.1.14)\nCollecting numpy>=1.21\n  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n\u001b[K     |████████████████████████████████| 15.7 MB 7.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: mido>=1.1.16 in /opt/conda/lib/python3.7/site-packages (from miditoolkit>=0.1.14->miditok) (1.2.10)\nInstalling collected packages: numpy, miditok\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.19.5\n    Uninstalling numpy-1.19.5:\n      Successfully uninstalled numpy-1.19.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-cloud 0.1.13 requires tensorflow<3.0,>=1.15.0, which is not installed.\nfancyimpute 0.5.5 requires tensorflow, which is not installed.\ndask-cudf 21.6.1+2.g101fc0fda4 requires cupy-cuda112, which is not installed.\ncudf 21.6.1+2.g101fc0fda4 requires cupy-cuda110, which is not installed.\nyellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.2 which is incompatible.\ntensorflow-gpu 2.4.1 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.4.2 which is incompatible.\nmatrixprofile 1.1.10 requires protobuf==3.11.2, but you have protobuf 3.17.3 which is incompatible.\nkornia 0.5.5 requires numpy<=1.19, but you have numpy 1.21.2 which is incompatible.\nimbalanced-learn 0.8.0 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\ndask-cudf 21.6.1+2.g101fc0fda4 requires dask<=2021.5.1,>=2021.4.0, but you have dask 2021.6.2 which is incompatible.\ndask-cudf 21.6.1+2.g101fc0fda4 requires distributed<=2021.5.1,>=2.22.0, but you have distributed 2021.6.2 which is incompatible.\u001b[0m\nSuccessfully installed miditok-0.1.7 numpy-1.21.2\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom io import open\nimport tensorflow as tf\nimport glob\nimport pickle\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport time\nimport copy\nfrom pathlib import Path, PurePath, PurePosixPath\nfrom music21 import converter, instrument, note, chord, stream\nfrom miditok import CPWordEncoding, REMIEncoding, get_midi_programs\nfrom miditoolkit import MidiFile","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-05T19:36:33.948959Z","iopub.execute_input":"2021-09-05T19:36:33.949288Z","iopub.status.idle":"2021-09-05T19:36:40.057839Z","shell.execute_reply.started":"2021-09-05T19:36:33.949257Z","shell.execute_reply":"2021-09-05T19:36:40.057004Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Creates the tokenizer and list the file paths\nremi_enc = REMIEncoding()  # uses defaults parameters\n# filename = 'chopin'\nfiles_paths = list(glob.glob('../input/maestropianomidi/maestro-v3.0.0/2018/*.midi'))","metadata":{"execution":{"iopub.status.busy":"2021-09-05T19:36:40.059226Z","iopub.execute_input":"2021-09-05T19:36:40.059567Z","iopub.status.idle":"2021-09-05T19:36:40.098449Z","shell.execute_reply.started":"2021-09-05T19:36:40.059530Z","shell.execute_reply":"2021-09-05T19:36:40.097610Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"midi = MidiFile('../input/blues-genre-midi-melodies/blues/035d2131e824eb51878007013786806a.mid')\nremi_enc.current_midi_metadata = {'time_division': midi.ticks_per_beat, 'tempo_changes': midi.tempo_changes}\nmidi","metadata":{"execution":{"iopub.status.busy":"2021-09-05T19:36:42.088292Z","iopub.execute_input":"2021-09-05T19:36:42.088662Z","iopub.status.idle":"2021-09-05T19:36:42.105617Z","shell.execute_reply.started":"2021-09-05T19:36:42.088634Z","shell.execute_reply":"2021-09-05T19:36:42.104736Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"ticks per beat: 192\nmax tick: 9483\ntempo changes: 1\ntime sig: 1\nkey sig: 0\nmarkers: 0\nlyrics: False\ninstruments: 1"},"metadata":{}}]},{"cell_type":"code","source":"notes = []\n\nremi_enc = REMIEncoding()\n\nfor file in files_paths:\n\n    # read the MIDI file\n    midi = MidiFile(file)\n\n    # Converts MIDI to tokens\n    tokens = remi_enc.midi_to_tokens(midi)\n    \n    notes.append(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T19:36:43.691613Z","iopub.execute_input":"2021-09-05T19:36:43.691930Z","iopub.status.idle":"2021-09-05T19:45:35.589336Z","shell.execute_reply.started":"2021-09-05T19:36:43.691902Z","shell.execute_reply":"2021-09-05T19:45:35.588473Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# all the beethoven files have 2 tracks (instruments) - Piano left and right\n# for now, we will only be using for piano right since it determines the melody\nmidi.instruments","metadata":{"execution":{"iopub.status.busy":"2021-09-05T19:45:35.590944Z","iopub.execute_input":"2021-09-05T19:45:35.591300Z","iopub.status.idle":"2021-09-05T19:45:35.598549Z","shell.execute_reply.started":"2021-09-05T19:45:35.591264Z","shell.execute_reply":"2021-09-05T19:45:35.597647Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[Instrument(program=0, is_drum=False, name=\"13 to 15\")]"},"metadata":{}}]},{"cell_type":"code","source":"# piano_right_notes = [note[0] for note in notes]\n# piano_right_notes = [np.asarray(note) for note in piano_right_notes]\n# piano_right_notes = np.array(piano_right_notes, dtype='object')","metadata":{"execution":{"iopub.status.busy":"2021-09-05T19:46:53.003005Z","iopub.execute_input":"2021-09-05T19:46:53.003326Z","iopub.status.idle":"2021-09-05T19:46:53.007617Z","shell.execute_reply.started":"2021-09-05T19:46:53.003296Z","shell.execute_reply":"2021-09-05T19:46:53.006752Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"midi","metadata":{"execution":{"iopub.status.busy":"2021-09-05T19:46:54.190873Z","iopub.execute_input":"2021-09-05T19:46:54.191192Z","iopub.status.idle":"2021-09-05T19:46:54.196485Z","shell.execute_reply.started":"2021-09-05T19:46:54.191163Z","shell.execute_reply":"2021-09-05T19:46:54.195652Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"ticks per beat: 384\nmax tick: 276806\ntempo changes: 1\ntime sig: 1\nkey sig: 0\nmarkers: 0\nlyrics: False\ninstruments: 1"},"metadata":{}}]},{"cell_type":"code","source":"len(notes[0][0])","metadata":{"execution":{"iopub.status.busy":"2021-09-05T19:52:19.446948Z","iopub.execute_input":"2021-09-05T19:52:19.447271Z","iopub.status.idle":"2021-09-05T19:52:19.453470Z","shell.execute_reply.started":"2021-09-05T19:52:19.447242Z","shell.execute_reply":"2021-09-05T19:52:19.452538Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"45661"},"metadata":{}}]},{"cell_type":"code","source":"notes = [note[0] for note in notes]","metadata":{"execution":{"iopub.status.busy":"2021-09-05T19:52:20.701051Z","iopub.execute_input":"2021-09-05T19:52:20.701380Z","iopub.status.idle":"2021-09-05T19:52:20.706067Z","shell.execute_reply.started":"2021-09-05T19:52:20.701343Z","shell.execute_reply":"2021-09-05T19:52:20.704871Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"len(notes[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:11:34.386302Z","iopub.execute_input":"2021-09-05T15:11:34.386636Z","iopub.status.idle":"2021-09-05T15:11:34.395917Z","shell.execute_reply.started":"2021-09-05T15:11:34.386604Z","shell.execute_reply":"2021-09-05T15:11:34.395080Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"45661"},"metadata":{}}]},{"cell_type":"code","source":"class Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n    \n    def words(self):\n        return self.idx2word\n\n\nclass Corpus(object):\n    def __init__(self, notes):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(notes[:-9])\n        self.valid = self.tokenize(notes[-9:])\n#         self.test = self.tokenize(notes[-2:])\n     \n    def tokenize(self, notes):\n        \"\"\"Tokenizes a note sequence\"\"\"\n        assert len(notes) > 0\n        \n        for song in notes:\n            for note in song:\n                self.dictionary.add_word(note)\n        idss = []\n        \n        for song in notes:\n            ids = []\n            for note in song:\n                ids.append(self.dictionary.word2idx[note])\n        idss.append(torch.tensor(ids).type(torch.int64))\n        ids = torch.cat(idss)\n            \n\n        return ids","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:11:49.760221Z","iopub.execute_input":"2021-09-05T15:11:49.760538Z","iopub.status.idle":"2021-09-05T15:11:49.769194Z","shell.execute_reply.started":"2021-09-05T15:11:49.760508Z","shell.execute_reply":"2021-09-05T15:11:49.768382Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"corpus = Corpus(notes)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:11:50.537757Z","iopub.execute_input":"2021-09-05T15:11:50.538095Z","iopub.status.idle":"2021-09-05T15:11:51.791959Z","shell.execute_reply.started":"2021-09-05T15:11:50.538064Z","shell.execute_reply":"2021-09-05T15:11:51.790893Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"len(notes)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:11:52.872317Z","iopub.execute_input":"2021-09-05T15:11:52.872648Z","iopub.status.idle":"2021-09-05T15:11:52.879364Z","shell.execute_reply.started":"2021-09-05T15:11:52.872616Z","shell.execute_reply":"2021-09-05T15:11:52.878449Z"},"trusted":true},"execution_count":111,"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"93"},"metadata":{}}]},{"cell_type":"code","source":"corpus.train.size(0)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:11:58.503852Z","iopub.execute_input":"2021-09-05T15:11:58.504203Z","iopub.status.idle":"2021-09-05T15:11:58.510307Z","shell.execute_reply.started":"2021-09-05T15:11:58.504171Z","shell.execute_reply":"2021-09-05T15:11:58.509503Z"},"trusted":true},"execution_count":113,"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"51477"},"metadata":{}}]},{"cell_type":"code","source":"device = 'cpu'\ndef batchify(data, bsz):\n    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n    that wouldn't cleanly fit.\n\n    Args:\n        data: Tensor, shape [N]\n        bsz: int, batch size\n\n    Returns:\n        Tensor of shape [N // bsz, bsz]\n    \"\"\"\n    seq_len = data.size(0) // bsz\n    data = data[:seq_len * bsz]\n    data = data.view(bsz, seq_len).t().contiguous()\n    return data.to(device)\n\neval_batch_size = 10\nbatch_size = 10\ntrain_data = batchify(corpus.train, batch_size)\nval_data = batchify(corpus.valid, batch_size)\n# test_data = batchify(corpus.test, batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:01.163475Z","iopub.execute_input":"2021-09-05T15:12:01.163810Z","iopub.status.idle":"2021-09-05T15:12:01.171764Z","shell.execute_reply.started":"2021-09-05T15:12:01.163778Z","shell.execute_reply":"2021-09-05T15:12:01.170740Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"val_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:08.201656Z","iopub.execute_input":"2021-09-05T15:12:08.202018Z","iopub.status.idle":"2021-09-05T15:12:08.210221Z","shell.execute_reply.started":"2021-09-05T15:12:08.201985Z","shell.execute_reply":"2021-09-05T15:12:08.209430Z"},"trusted":true},"execution_count":116,"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"torch.Size([2075, 10])"},"metadata":{}}]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"\"\"Inject some information about the relative or absolute position of the tokens\n        in the sequence. The positional encodings have the same dimension as\n        the embeddings, so that the two can be summed. Here, we use sine and cosine\n        functions of different frequencies.\n    .. math::\n        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    Examples:\n        >>> pos_encoder = PositionalEncoding(d_model)\n    \"\"\"\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.d_model = d_model\n\n        # PE is the Positional Encoding matrix \n        # THIS STORES THE POSITIONS OF THE SEQUENCE\n        pe = torch.zeros(max_len, d_model)\n        \n        # Arange - RETURNS A RANGE BETWEEN VALUES, HERE IT IS 0 - max_len\n        # unsqueeze - adds a dimension, 1 means that each element in the first list is now in a list\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        \n        # division term, here it is (10000 ** ((2 * i)/d_model))\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        # calculating the position encoding for the even and odd terms\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        # Unsqueeze 0 will put PE in one list\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        Examples:\n            >>> output = pos_encoder(x)\n        \"\"\"\n        # make embeddings relatively larger\n        # This is so we do not lose the importance of the embedding\n        x = x * math.sqrt(self.d_model)\n        # we add the embedding to the PE \n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:28.330626Z","iopub.execute_input":"2021-08-29T17:11:28.330944Z","iopub.status.idle":"2021-08-29T17:11:28.340154Z","shell.execute_reply.started":"2021-08-29T17:11:28.330916Z","shell.execute_reply":"2021-08-29T17:11:28.339348Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        \n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n        self.d_model = d_model\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        # make embeddings relatively larger\n        # This is so we do not lose the importance of the embedding\n        x = x * math.sqrt(self.d_model)\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:15.019672Z","iopub.execute_input":"2021-09-05T15:12:15.020097Z","iopub.status.idle":"2021-09-05T15:12:15.028827Z","shell.execute_reply.started":"2021-09-05T15:12:15.020063Z","shell.execute_reply":"2021-09-05T15:12:15.027988Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\"Parallel self attention given the number of heads and size\n    .. math::\n        X * Wk = K\n        X * Wq = Q\n        X * Wv = V\n        \\text{attention} = softmax(Q * transpose(K) / sqrt(d_model)) * V\n    Args:\n        d_model: the embed dim (default = 256).\n        heads: number of heads (default = 4)\n        max_length = max length of sequences (default = 2048)\n        dropout: the dropout value (default = 0.1).\n    Examples:\n        >>> attention = MultiHeadAttention(d_model, heads)\n    \"\"\"\n    def __init__(self, d_model = 256, heads = 4, max_length = 2048, dropout = 0.1):\n        super().__init__()\n        \n        self.d = d_model\n        self.h = heads\n        self.dh = d_model // heads\n        self.max_length = max_length\n        self.E = torch.randn([heads, self.max_length, self.dh])\n        \n        self.q_linear = nn.Linear(self.dh, self.dh)\n        self.v_linear = nn.Linear(self.dh, self.dh)\n        self.k_linear = nn.Linear(self.dh, self.dh)\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(d_model, d_model)\n    \n    def forward(self, q, k, v, mask=None):\n        \n        batch_size = q.size(0)\n        T = q.size(1)\n        dh = q.size(2) // self.h\n        \n        k = k.view(batch_size, T, self.h, dh)\n        q = q.view(batch_size, T, self.h, dh)\n        v = v.view(batch_size, T, self.h, dh)\n        # perform linear operation and split into h heads\n        \n        K = self.k_linear(k)\n        Q = self.q_linear(q)\n        V = self.v_linear(v)\n        \n        # transpose to get dimensions bs * h * sl * d_model\n       \n        K = K.transpose(1,2)\n        Q = Q.transpose(1,2)\n        V = V.transpose(1,2)\n\n        #start index of position embedding\n        \n        embedding_start = self.max_length - T\n        \n        #apply same position embeddings across the batch\n        \n        Er  = self.E[:, embedding_start:, :].unsqueeze(0)\n        \n        QE = torch.matmul(Q, K.transpose(-2, -1))\n        QE = self.mask_attention_positions(QE)\n        \n        #Get relative position attention scores\n        #combine batch with head dimension\n        SRel = self.skew_padding_position(QE)\n\n#         Q = Q.contiguous().view(batch_size*self.h, T, self.dh)\n#         K = K.contiguous().view(batch_size*self.h, T, self.dh)\n#         V = V.contiguous().view(batch_size*self.h, T, self.dh)\n        \n        #Compute scaled dot-product self-attention\n        #scale pre-matrix multiplication   \n        Q = Q / (q.size(2) ** (1/4))\n        K = K / (q.size(2) ** (1/4))\n\n        # calculate attention \n        scores, weights = self.attention(QE, SRel, V, mask, self.dropout)\n        \n        # concatenate heads and put through final linear layer\n        concat = scores.transpose(1,2).contiguous().view(batch_size, -1, self.d)\n        \n        output = self.out(concat)\n    \n        return output\n\n    def attention(self, QE, Srel, V, mask=None, dropout=None):\n        log = QE + Srel\n        log = log / math.sqrt(self.dh)\n\n        if mask is not None:\n#             mask = mask.unsqueeze(1)\n            log += (mask.to(torch.int64) * -1e9).to(log.dtype)\n\n#             print(mask.shape)\n#             log = log.masked_fill(mask == 0, float('-inf'))\n\n        scores = F.softmax(log, -1)\n        \n        if dropout is not None:\n            scores = dropout(scores)\n            \n#         print(scores.shape, V.shape)\n        attention = torch.matmul(scores, V)\n        \n#         scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n\n#         if mask is not None:\n#             print(scores.shape)\n\n#     #         mask = mask.unsqueeze(1)\n#             print(mask.shape)\n#     #         scores = scores.masked_fill(mask == 0, float('-inf'))\n#         scores = F.softmax(scores, dim=-1)\n\n#         \n\n#         output = torch.matmul(scores, v)\n        return attention, scores\n    \n    def mask_attention_positions(self, qe):\n        # to avoid looking backward by masking the positions\n        index = qe.shape[-1]\n        mask = torch.triu(torch.ones(index, index), 1).flip(1)\n        return qe.masked_fill((mask == 1), 0)\n    \n    def skew_padding_position(self, qe):\n        # to add padding to the skewed result after masking the matrix\n        # column of zeros on left\n        padded_qe = F.pad(qe, [1,0])\n        s = padded_qe.shape\n        padded_qe = padded_qe.view(s[0], s[1], s[3], s[2])\n        #take out first (padded) row\n        return padded_qe[:,:,1:,:]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:29.221271Z","iopub.execute_input":"2021-08-29T17:11:29.221605Z","iopub.status.idle":"2021-08-29T17:11:29.240108Z","shell.execute_reply.started":"2021-08-29T17:11:29.221574Z","shell.execute_reply":"2021-08-29T17:11:29.239141Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n#     def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n#         super().__init__() \n#         # We set d_ff as a default to 2048\n#         self.linear_1 = nn.Linear(d_model, d_ff)\n#         self.dropout = nn.Dropout(dropout)\n#         self.linear_2 = nn.Linear(d_ff, d_model)\n#     def forward(self, x):\n#         x = self.dropout(F.relu(self.linear_1(x)))\n#         x = self.linear_2(x)\n#         return x\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear_2(self.dropout(F.relu(self.linear_1(x))))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:29.51977Z","iopub.execute_input":"2021-08-29T17:11:29.520083Z","iopub.status.idle":"2021-08-29T17:11:29.525789Z","shell.execute_reply.started":"2021-08-29T17:11:29.520034Z","shell.execute_reply":"2021-08-29T17:11:29.524865Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build an encoder layer with one multi-head attention layer and one # feed-forward layer\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout = 0.1):\n        super().__init__()\n        self.norm_1 = nn.LayerNorm(d_model)\n        self.norm_2 = nn.LayerNorm(d_model)\n        \n        self.attn = MultiHeadAttention(d_model, heads)\n        self.ff = FeedForward(d_model)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        x2 = self.norm_1(x)\n        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n        x2 = self.norm_2(x)\n        x = x + self.dropout_2(self.ff(x2))\n        return x\n    \n# build a decoder layer with two multi-head attention layers and\n# one feed-forward layer\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout=0.1):\n        super().__init__()\n        self.norm_1 = nn.LayerNorm(d_model)\n        self.norm_2 = nn.LayerNorm(d_model)\n        self.norm_3 = nn.LayerNorm(d_model)\n        \n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n        \n        self.attn_1 = MultiHeadAttention(d_model, heads)\n        self.attn_2 = MultiHeadAttention(d_model, heads)\n        self.ff = FeedForward(d_model)\n    def forward(self, x, memory, mask = None):\n#         x2 = self.norm_1(x)\n#         x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n#         x2 = self.norm_2(x)\n#         x = x + self.dropout_2(self.attn_2(x2, x2, x2,\n#         src_mask))\n#         x2 = self.norm_3(x)\n#         x = x + self.dropout_3(self.ff(x2))\n        \n        #perform masked attention on input\n        #masked so queries cannot attend to subsequent keys\n        #Pass through sublayers of attention and feedforward.\n        #Apply dropout to sublayer output, add it to input, and norm.\n        attn = self.attn_1(x, x, x, mask)\n        x = x + self.dropout_1(attn)\n        x = self.norm_1(x)\n        x = x + self.dropout_2(self.attn_2(x, memory, memory, mask))\n        ff = self.ff(x)\n        x = x + self.dropout_2(ff)\n        x = self.norm_2(x)\n\n        return x\n#         return x\n# We can then build a convenient cloning function that can generate multiple layers:\ndef get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:29.785832Z","iopub.execute_input":"2021-08-29T17:11:29.786141Z","iopub.status.idle":"2021-08-29T17:11:29.802339Z","shell.execute_reply.started":"2021-08-29T17:11:29.786111Z","shell.execute_reply":"2021-08-29T17:11:29.801479Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, d_model, N, heads):\n        super().__init__()\n        self.N = N\n        self.layers = get_clones(EncoderLayer(d_model, heads), self.N)\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, src, mask):\n        x = self.layers[0](src, mask)\n        for i in range(1,self.N):\n            x = self.layers[i](x, mask)\n        return self.norm(x)\n    \nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, d_model, N, heads):\n        super().__init__()\n        self.N = N\n        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, src, trg, src_mask = None):\n        x = self.layers[0](src, trg, src_mask)\n        for i in range(1,self.N):\n            x = self.layers[i](x, trg, src_mask)\n        return self.norm(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:30.089969Z","iopub.execute_input":"2021-08-29T17:11:30.090276Z","iopub.status.idle":"2021-08-29T17:11:30.099241Z","shell.execute_reply.started":"2021-08-29T17:11:30.090245Z","shell.execute_reply":"2021-08-29T17:11:30.09815Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, d_model, nhead, nhid, nlayers, dropout=0.5, max_length = 2048):\n        super(TransformerModel, self).__init__()\n        try:\n            from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n        except:\n            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n        self.model_type = 'Transformer'\n        # original mask\n        self.src_mask = None\n        self.max_length = max_length\n        self.d_model = d_model\n        \n        # embedding encoding\n        self.embedding = nn.Embedding(ntoken, d_model)\n        \n        # positional encoding\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        \n        # encoder\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, nhid, dropout)\n        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n        \n#         self.decoder = nn.Linear(d_model, ntoken)\n\n\n        \n#         self.encoder = Encoder(d_model, nlayers, nhead)\n#         self.encoder.eval()\n        # decoder\n#         decoder_layers = TransformerDecoderLayer(d_model, nhead, nhid, dropout)\n#         self.decoder = TransformerDecoder(decoder_layers, nlayers)\n#         self.decoder = Decoder(ntoken, d_model, nlayers, nhead)\n#         self.decoder.eval()\n        # classification layer\n        self.classification_layer = nn.Linear(d_model, ntoken)\n        \n        self.init_weights()\n    def _generate_square_subsequent_mask(self, sz):\n        \n        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n#         mask = utils.get_masked_with_pad_tensor(self.max_length, x, x, config.pad_token)\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        nn.init.uniform_(self.embedding.weight, -initrange, initrange)\n        self.classification_layer.bias.data.zero_()\n        self.classification_layer.weight.data.uniform_(-initrange, initrange)\n#         nn.init.zeros_(self.decoder.weight)\n#         nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n\n    def forward(self, src, src_mask):\n        \n        if src_mask == None:\n            src_mask = self._generate_square_subsequent_mask(len(src))\n        self.src_mask = src_mask\n\n        src = self.pos_encoder(self.embedding(src))\n        output = self.encoder(src, self.src_mask)\n#         output = self.decoder(output, src)\n        \n        #Flatten:\n#         shape = output.shape\n#         tensor_reshaped = output.reshape(shape[0],-1)\n#         #Drop all rows containing any nan:\n#         tensor_reshaped = tensor_reshaped[~torch.any(tensor_reshaped.isnan(),dim=1)]\n#         #Reshape back:\n#         output = tensor_reshaped.reshape(tensor_reshaped.shape[0],*shape[1:])\n#         print(output)\n        output = self.classification_layer(output)\n#         print(output)\n        return F.log_softmax(output, dim=-1)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:17.659922Z","iopub.execute_input":"2021-09-05T15:12:17.660296Z","iopub.status.idle":"2021-09-05T15:12:17.670837Z","shell.execute_reply.started":"2021-09-05T15:12:17.660264Z","shell.execute_reply":"2021-09-05T15:12:17.670074Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"len(corpus.dictionary)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:18.583176Z","iopub.execute_input":"2021-09-05T15:12:18.583499Z","iopub.status.idle":"2021-09-05T15:12:18.590751Z","shell.execute_reply.started":"2021-09-05T15:12:18.583468Z","shell.execute_reply":"2021-09-05T15:12:18.589715Z"},"trusted":true},"execution_count":119,"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"222"},"metadata":{}}]},{"cell_type":"code","source":"ntokens = len(corpus.dictionary)\nemsize = 200\nnhead = 2\nnhid = 200\nnlayer = 2\ndropout = 0.2\n# Loop over epochs.\nlr = 5\nbest_val_loss = None\nepochs = 100\nsave = './model.pt'\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:22.990090Z","iopub.execute_input":"2021-09-05T15:12:22.990421Z","iopub.status.idle":"2021-09-05T15:12:22.995406Z","shell.execute_reply.started":"2021-09-05T15:12:22.990393Z","shell.execute_reply":"2021-09-05T15:12:22.994402Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"final_losses = []\nemsizes = []\nnheads = []\nnhids = []\nnlayers = []\n\nfor em in emsize:\n    for head in nhead:\n        for hid in nhid:\n            for layer in nlayer:\n                emsizes.append(em)\n                nheads.append(head)\n                nhids.append(hid)\n                nlayers.append(layer)\n                best_val_loss = None\n\n                model = TransformerModel(ntokens, em, head, hid, layer, dropout).to(device)\n\n                # At any point you can hit Ctrl + C to break out of training early.\n                try:\n                    for epoch in range(1, epochs+1):\n                        epoch_start_time = time.time()\n                        train()\n                        val_loss = evaluate(test_data)\n                        train_loss = evaluate(train_data)\n                        print('-' * 89)\n                        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | train loss {:5.2f}'\n                                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                                           val_loss, train_loss, math.exp(val_loss)))\n                        print('-' * 89)\n                        # Save the model if the validation loss is the best we've seen so far.\n                        if not best_val_loss or val_loss < best_val_loss:\n                            with open(save, 'wb') as f:\n                                torch.save(model, f)\n                            best_val_loss = val_loss\n                        else:\n                            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n                            lr /= 2.0\n                except KeyboardInterrupt:\n                    print('-' * 89)\n                    print('Exiting from training early')\n                final_losses.append(best_val_loss)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-05T15:12:24.247456Z","iopub.execute_input":"2021-09-05T15:12:24.247766Z","iopub.status.idle":"2021-09-05T15:12:24.278517Z","shell.execute_reply.started":"2021-09-05T15:12:24.247738Z","shell.execute_reply":"2021-09-05T15:12:24.276863Z"},"trusted":true},"execution_count":121,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-121-23a949f785ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnhid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"],"ename":"TypeError","evalue":"'int' object is not iterable","output_type":"error"}]},{"cell_type":"code","source":"scores = list(zip(emsizes, nheads, nhids, nlayers, final_losses))\nlosses_df = pd.DataFrame(scores, columns=['embedding_size', 'num_heads', 'num_hidden', 'num_layers', 'best_loss'])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:12:10.237712Z","iopub.execute_input":"2021-08-29T17:12:10.238023Z","iopub.status.idle":"2021-08-29T17:12:10.251059Z","shell.execute_reply.started":"2021-08-29T17:12:10.237993Z","shell.execute_reply":"2021-08-29T17:12:10.250173Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses_df","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:12:12.034177Z","iopub.execute_input":"2021-08-29T17:12:12.034511Z","iopub.status.idle":"2021-08-29T17:12:12.05353Z","shell.execute_reply.started":"2021-08-29T17:12:12.03448Z","shell.execute_reply":"2021-08-29T17:12:12.052781Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model = TransformerModel(ntokens, emsize, nhead, nhid, nlayer, dropout).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:30.361017Z","iopub.execute_input":"2021-09-05T15:12:30.361342Z","iopub.status.idle":"2021-09-05T15:12:30.377714Z","shell.execute_reply.started":"2021-09-05T15:12:30.361313Z","shell.execute_reply":"2021-09-05T15:12:30.376965Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"def repackage_hidden(h):\n    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:31.600952Z","iopub.execute_input":"2021-09-05T15:12:31.601296Z","iopub.status.idle":"2021-09-05T15:12:31.606048Z","shell.execute_reply.started":"2021-09-05T15:12:31.601265Z","shell.execute_reply":"2021-09-05T15:12:31.604850Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"# get_batch subdivides the source data into chunks of length args.bptt.\n# If source is equal to the example output of the batchify function, with\n# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n# ┌ a g m s ┐ ┌ b h n t ┐\n# └ b h n t ┘ └ c i o u ┘\n# Note that despite the name of the function, the subdivison of data is not\n# done along the batch dimension (i.e. dimension 1), since that was handled\n# by the batchify function. The chunks are along dimension 0, corresponding\n# to the seq_len dimension in the LSTM.\nseq_length = 35\ndef get_batch(source, i):\n    seq_len = min(seq_length, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:32.964525Z","iopub.execute_input":"2021-09-05T15:12:32.964853Z","iopub.status.idle":"2021-09-05T15:12:32.972270Z","shell.execute_reply.started":"2021-09-05T15:12:32.964823Z","shell.execute_reply":"2021-09-05T15:12:32.971457Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz):\n    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:33.474733Z","iopub.execute_input":"2021-09-05T15:12:33.475088Z","iopub.status.idle":"2021-09-05T15:12:33.479342Z","shell.execute_reply.started":"2021-09-05T15:12:33.475041Z","shell.execute_reply":"2021-09-05T15:12:33.478435Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, eval_data):\n    model.eval()  # turn on evaluation mode\n    total_loss = 0.\n    src_mask = generate_square_subsequent_mask(seq_length).to(device)\n    with torch.no_grad():\n        for i in range(0, eval_data.size(0) - 1, seq_length):\n            data, targets = get_batch(eval_data, i)\n            batch_size = data.size(0)\n            if batch_size != seq_length:\n                src_mask = src_mask[:batch_size, :batch_size]\n            output = model(data, src_mask)\n            output_flat = output.view(-1, ntokens)\n            total_loss += batch_size * criterion(output_flat, targets).item()\n    return total_loss / (len(eval_data) - 1)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:34.218828Z","iopub.execute_input":"2021-09-05T15:12:34.219230Z","iopub.status.idle":"2021-09-05T15:12:34.226029Z","shell.execute_reply.started":"2021-09-05T15:12:34.219196Z","shell.execute_reply":"2021-09-05T15:12:34.225108Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\ndef train(model):\n    model.train()  # turn on train mode\n    total_loss = 0.\n    log_interval = 200\n    start_time = time.time()\n    src_mask = generate_square_subsequent_mask(seq_length).to(device)\n\n    num_batches = len(train_data) // seq_length\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_length)):\n        data, targets = get_batch(train_data, i)\n        batch_size = data.size(0)\n        if batch_size != seq_length:  # only on last batch\n            src_mask = src_mask[:batch_size, :batch_size]\n        output = model(data, src_mask)\n        loss = criterion(output.view(-1, ntokens), targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        total_loss += loss.item()\n        if batch % log_interval == 0 and batch > 0:\n            lr = scheduler.get_last_lr()[0]\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            ppl = math.exp(cur_loss)\n            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n            total_loss = 0\n            start_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:34.932679Z","iopub.execute_input":"2021-09-05T15:12:34.933008Z","iopub.status.idle":"2021-09-05T15:12:34.943492Z","shell.execute_reply.started":"2021-09-05T15:12:34.932977Z","shell.execute_reply":"2021-09-05T15:12:34.942544Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    for epoch in range(1, epochs+1):\n        epoch_start_time = time.time()\n        train(model)\n        val_loss = evaluate(model, val_data)\n        train_loss = evaluate(model, train_data)\n        print('-' * 89)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | train loss {:5.2f} | '\n                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                           val_loss, train_loss, math.exp(val_loss)))\n        print('-' * 89)\n        # Save the model if the validation loss is the best we've seen so far.\n        if not best_val_loss or val_loss < best_val_loss:\n            with open(save, 'wb') as f:\n                torch.save(model, f)\n            best_val_loss = val_loss\n        else:\n            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n            lr /= 2.0\nexcept KeyboardInterrupt:\n    print('-' * 89)\n    print('Exiting from training early')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:35.908248Z","iopub.execute_input":"2021-09-05T15:12:35.908559Z","iopub.status.idle":"2021-09-05T15:22:03.797600Z","shell.execute_reply.started":"2021-09-05T15:12:35.908530Z","shell.execute_reply":"2021-09-05T15:22:03.796658Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"-----------------------------------------------------------------------------------------\n| end of epoch   1 | time:  5.38s | valid loss  3.27 | train loss  3.39 | valid ppl    26.37\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time:  5.57s | valid loss  3.24 | train loss  3.31 | valid ppl    25.54\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time:  7.86s | valid loss  2.98 | train loss  3.16 | valid ppl    19.69\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   4 | time:  8.96s | valid loss  2.97 | train loss  3.10 | valid ppl    19.49\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   5 | time:  8.15s | valid loss  2.94 | train loss  3.10 | valid ppl    18.88\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   6 | time:  7.66s | valid loss  3.15 | train loss  3.25 | valid ppl    23.35\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   7 | time:  7.69s | valid loss  2.99 | train loss  3.17 | valid ppl    19.82\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   8 | time:  6.77s | valid loss  3.02 | train loss  3.17 | valid ppl    20.39\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   9 | time:  6.96s | valid loss  2.87 | train loss  3.13 | valid ppl    17.71\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  10 | time:  6.70s | valid loss  2.92 | train loss  3.06 | valid ppl    18.60\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  11 | time:  7.03s | valid loss  2.90 | train loss  3.07 | valid ppl    18.17\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  12 | time:  6.79s | valid loss  2.89 | train loss  3.05 | valid ppl    17.95\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  13 | time:  6.01s | valid loss  2.97 | train loss  3.10 | valid ppl    19.46\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  14 | time:  6.57s | valid loss  2.88 | train loss  3.21 | valid ppl    17.78\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  15 | time:  6.36s | valid loss  2.95 | train loss  3.05 | valid ppl    19.18\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  16 | time:  6.57s | valid loss  2.89 | train loss  3.03 | valid ppl    17.95\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  17 | time:  6.16s | valid loss  2.91 | train loss  3.07 | valid ppl    18.43\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  18 | time:  6.15s | valid loss  2.85 | train loss  3.01 | valid ppl    17.33\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  19 | time:  5.92s | valid loss  4.44 | train loss  3.91 | valid ppl    84.95\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  20 | time:  5.35s | valid loss  3.09 | train loss  3.18 | valid ppl    21.87\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  21 | time:  5.80s | valid loss  2.84 | train loss  3.01 | valid ppl    17.19\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  22 | time:  6.43s | valid loss  2.89 | train loss  3.04 | valid ppl    17.93\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  23 | time:  5.95s | valid loss  2.95 | train loss  3.13 | valid ppl    19.16\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  24 | time:  5.71s | valid loss  2.96 | train loss  3.06 | valid ppl    19.24\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  25 | time:  5.73s | valid loss  2.91 | train loss  3.03 | valid ppl    18.43\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  26 | time:  5.75s | valid loss  2.94 | train loss  3.08 | valid ppl    19.00\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  27 | time:  5.40s | valid loss  2.94 | train loss  3.08 | valid ppl    18.91\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  28 | time:  6.24s | valid loss  3.83 | train loss  3.54 | valid ppl    46.08\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  29 | time:  5.67s | valid loss  2.95 | train loss  3.08 | valid ppl    19.07\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  30 | time:  5.53s | valid loss  2.90 | train loss  3.04 | valid ppl    18.25\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  31 | time:  5.29s | valid loss  2.98 | train loss  3.11 | valid ppl    19.73\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  32 | time:  5.35s | valid loss  2.88 | train loss  3.00 | valid ppl    17.74\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  33 | time:  5.08s | valid loss  2.87 | train loss  3.02 | valid ppl    17.60\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  34 | time:  5.91s | valid loss  2.89 | train loss  3.08 | valid ppl    18.03\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  35 | time:  5.15s | valid loss  2.95 | train loss  3.04 | valid ppl    19.05\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  36 | time:  5.46s | valid loss  2.92 | train loss  3.04 | valid ppl    18.54\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  37 | time:  5.05s | valid loss  3.41 | train loss  3.35 | valid ppl    30.29\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  38 | time:  5.25s | valid loss  2.90 | train loss  2.99 | valid ppl    18.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  39 | time:  5.19s | valid loss  2.89 | train loss  2.97 | valid ppl    17.93\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  40 | time:  5.03s | valid loss  2.98 | train loss  3.06 | valid ppl    19.68\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  41 | time:  5.77s | valid loss  2.94 | train loss  3.03 | valid ppl    18.94\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  42 | time:  5.11s | valid loss  2.98 | train loss  2.99 | valid ppl    19.66\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  43 | time:  5.59s | valid loss  2.82 | train loss  2.97 | valid ppl    16.76\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  44 | time:  5.18s | valid loss  2.95 | train loss  3.04 | valid ppl    19.01\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  45 | time:  5.32s | valid loss  3.02 | train loss  3.06 | valid ppl    20.49\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  46 | time:  5.10s | valid loss  2.82 | train loss  2.96 | valid ppl    16.83\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  47 | time:  5.76s | valid loss  2.89 | train loss  3.03 | valid ppl    18.08\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  48 | time:  5.00s | valid loss  2.87 | train loss  2.98 | valid ppl    17.66\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  49 | time:  5.49s | valid loss  2.95 | train loss  3.07 | valid ppl    19.02\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  50 | time:  5.21s | valid loss  2.92 | train loss  2.99 | valid ppl    18.50\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  51 | time:  5.35s | valid loss  2.83 | train loss  2.95 | valid ppl    16.97\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  52 | time:  5.20s | valid loss  2.92 | train loss  3.05 | valid ppl    18.56\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  53 | time:  5.93s | valid loss  2.88 | train loss  3.05 | valid ppl    17.90\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  54 | time:  5.17s | valid loss  2.89 | train loss  3.04 | valid ppl    17.97\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  55 | time:  5.39s | valid loss  2.93 | train loss  3.12 | valid ppl    18.71\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  56 | time:  5.28s | valid loss  2.88 | train loss  3.00 | valid ppl    17.83\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  57 | time:  5.51s | valid loss  2.89 | train loss  3.02 | valid ppl    18.05\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  58 | time:  5.18s | valid loss  2.81 | train loss  2.97 | valid ppl    16.67\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  59 | time:  5.82s | valid loss  2.93 | train loss  3.10 | valid ppl    18.81\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  60 | time:  5.19s | valid loss  2.88 | train loss  3.01 | valid ppl    17.85\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  61 | time:  5.39s | valid loss  3.21 | train loss  3.17 | valid ppl    24.73\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  62 | time:  5.23s | valid loss  3.06 | train loss  3.23 | valid ppl    21.43\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  63 | time:  5.35s | valid loss  2.95 | train loss  3.11 | valid ppl    19.14\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  64 | time:  5.28s | valid loss  2.85 | train loss  2.96 | valid ppl    17.29\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  65 | time:  5.33s | valid loss  2.87 | train loss  2.97 | valid ppl    17.62\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  66 | time:  5.68s | valid loss  2.84 | train loss  2.95 | valid ppl    17.14\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  67 | time:  5.35s | valid loss  2.96 | train loss  3.06 | valid ppl    19.38\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  68 | time:  5.30s | valid loss  2.85 | train loss  2.95 | valid ppl    17.31\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  69 | time:  5.32s | valid loss  2.84 | train loss  2.94 | valid ppl    17.06\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  70 | time:  5.10s | valid loss  2.88 | train loss  2.99 | valid ppl    17.87\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  71 | time:  5.43s | valid loss  2.82 | train loss  2.92 | valid ppl    16.84\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  72 | time:  5.61s | valid loss  2.88 | train loss  2.99 | valid ppl    17.82\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  73 | time:  5.64s | valid loss  2.92 | train loss  3.02 | valid ppl    18.53\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  74 | time:  5.55s | valid loss  3.00 | train loss  3.09 | valid ppl    20.11\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  75 | time:  5.61s | valid loss  3.00 | train loss  3.16 | valid ppl    19.99\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  76 | time:  5.22s | valid loss  2.94 | train loss  3.02 | valid ppl    18.85\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  77 | time:  5.42s | valid loss  2.90 | train loss  2.98 | valid ppl    18.21\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  78 | time:  5.56s | valid loss  2.97 | train loss  3.10 | valid ppl    19.48\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  79 | time:  5.45s | valid loss  2.85 | train loss  3.04 | valid ppl    17.32\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  80 | time:  5.25s | valid loss  2.83 | train loss  2.93 | valid ppl    16.96\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  81 | time:  5.35s | valid loss  2.88 | train loss  2.97 | valid ppl    17.87\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  82 | time:  5.23s | valid loss  2.96 | train loss  3.06 | valid ppl    19.23\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  83 | time:  5.17s | valid loss  3.21 | train loss  3.16 | valid ppl    24.67\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  84 | time:  5.78s | valid loss  2.92 | train loss  3.02 | valid ppl    18.59\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  85 | time:  4.98s | valid loss  2.97 | train loss  3.07 | valid ppl    19.40\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  86 | time:  5.52s | valid loss  2.90 | train loss  3.01 | valid ppl    18.09\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  87 | time:  5.09s | valid loss  3.06 | train loss  3.14 | valid ppl    21.37\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  88 | time:  5.25s | valid loss  3.13 | train loss  3.08 | valid ppl    22.91\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  89 | time:  5.13s | valid loss  2.91 | train loss  3.03 | valid ppl    18.42\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  90 | time:  5.52s | valid loss  2.88 | train loss  3.00 | valid ppl    17.88\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  91 | time:  5.30s | valid loss  2.89 | train loss  3.02 | valid ppl    18.06\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  92 | time:  5.27s | valid loss  2.85 | train loss  2.94 | valid ppl    17.34\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  93 | time:  5.06s | valid loss  2.93 | train loss  3.02 | valid ppl    18.65\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  94 | time:  5.50s | valid loss  2.91 | train loss  2.99 | valid ppl    18.34\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  95 | time:  5.12s | valid loss  2.89 | train loss  3.00 | valid ppl    17.99\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  96 | time:  5.43s | valid loss  2.94 | train loss  3.06 | valid ppl    18.85\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  97 | time:  5.52s | valid loss  2.90 | train loss  3.20 | valid ppl    18.21\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  98 | time:  5.47s | valid loss  2.98 | train loss  3.06 | valid ppl    19.72\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  99 | time:  5.39s | valid loss  3.08 | train loss  3.03 | valid ppl    21.80\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 100 | time:  5.45s | valid loss  2.98 | train loss  3.07 | valid ppl    19.77\n-----------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"n_generate = 2000\ntemperature = 1\nsequence = []\nlog_interval = 50 # interval between logs","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:25:01.640869Z","iopub.execute_input":"2021-09-03T19:25:01.64134Z","iopub.status.idle":"2021-09-03T19:25:01.646024Z","shell.execute_reply.started":"2021-09-03T19:25:01.641282Z","shell.execute_reply":"2021-09-03T19:25:01.645124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\ninput = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\nsrc_mask = generate_square_subsequent_mask(len(input)).to(device)\nwith open('./output', 'w') as outf:\n    with torch.no_grad():  # no tracking history\n        for i in range(n_generate):\n            src_mask = generate_square_subsequent_mask(len(input)).to(device)\n            output = model(input, src_mask)\n            word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n            word_idx = torch.multinomial(word_weights, 1)[0]\n            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n            input = torch.cat([input, word_tensor], 0)\n\n\n            word = corpus.dictionary.idx2word[word_idx]\n\n            outf.write(str(word) + ('\\n' if i % 20 == 19 else ' '))\n            \n            sequence.append(word)\n\n            if i % log_interval == 0:\n                print('| Generated {}/{} notes'.format(i, n_generate))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:25:02.714654Z","iopub.execute_input":"2021-09-03T19:25:02.715Z","iopub.status.idle":"2021-09-03T19:31:29.512435Z","shell.execute_reply.started":"2021-09-03T19:25:02.714968Z","shell.execute_reply":"2021-09-03T19:31:29.511549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence","metadata":{"execution":{"iopub.status.busy":"2021-09-03T20:08:13.652688Z","iopub.execute_input":"2021-09-03T20:08:13.653029Z","iopub.status.idle":"2021-09-03T20:08:13.675203Z","shell.execute_reply.started":"2021-09-03T20:08:13.653001Z","shell.execute_reply":"2021-09-03T20:08:13.674185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence = [sequence]","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:42:23.052259Z","iopub.execute_input":"2021-09-03T19:42:23.052674Z","iopub.status.idle":"2021-09-03T19:42:23.05919Z","shell.execute_reply.started":"2021-09-03T19:42:23.05264Z","shell.execute_reply":"2021-09-03T19:42:23.058369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converted_back_midi = remi_enc.tokens_to_midi(sequence, get_midi_programs(midi))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:42:24.306695Z","iopub.execute_input":"2021-09-03T19:42:24.307023Z","iopub.status.idle":"2021-09-03T19:42:24.314384Z","shell.execute_reply.started":"2021-09-03T19:42:24.306992Z","shell.execute_reply":"2021-09-03T19:42:24.313082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converted_back_midi.dump('blues_gen_2.mid')","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:42:37.46414Z","iopub.execute_input":"2021-09-03T19:42:37.464496Z","iopub.status.idle":"2021-09-03T19:42:37.469033Z","shell.execute_reply.started":"2021-09-03T19:42:37.464464Z","shell.execute_reply":"2021-09-03T19:42:37.467973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from miditok import REMIEncoding, get_midi_programs\nfrom miditoolkit import MidiFile\n\n# Our parameters\npitch_range = range(21, 109)\nbeat_res = {(0, 4): 8, (4, 12): 4}\nnb_velocities = 32\nadditional_tokens = {'Chord': True,\n                     'Empty': True,\n                     'Tempo': True,\n                     'nb_tempos': 32,  # nb of tempo bins\n                     'tempo_range': (40, 250)}  # (min_tempo, max_tempo)\n\n# Creates the tokenizer and loads a MIDI\nremi_enc = REMIEncoding(pitch_range, beat_res, nb_velocities, additional_tokens)\nmidi = MidiFile('../input/blues-genre-midi-melodies/blues/035d2131e824eb51878007013786806a.mid')\n\n# Converts MIDI to tokens, and back to a MIDI\ntokens = remi_enc.midi_to_tokens(midi)\nconverted_back_midi = remi_enc.tokens_to_midi(tokens, get_midi_programs(midi))\n\n# Converts just a selected track\nremi_enc.current_midi_metadata = {'time_division': midi.ticks_per_beat, 'tempo_changes': midi.tempo_changes}\npiano_tokens = remi_enc.track_to_tokens(midi.instruments[0])\n\n# And convert it back (the last arg stands for (program number, is drum))\nconverted_back_track, tempo_changes = remi_enc.tokens_to_track(piano_tokens, midi.ticks_per_beat, (0, False))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T18:33:40.072817Z","iopub.execute_input":"2021-09-03T18:33:40.073137Z","iopub.status.idle":"2021-09-03T18:33:40.113076Z","shell.execute_reply.started":"2021-09-03T18:33:40.073106Z","shell.execute_reply":"2021-09-03T18:33:40.112184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converted_back_midi.dump('blues.mid')","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:24:01.050615Z","iopub.execute_input":"2021-09-03T19:24:01.050965Z","iopub.status.idle":"2021-09-03T19:24:01.055989Z","shell.execute_reply.started":"2021-09-03T19:24:01.050936Z","shell.execute_reply":"2021-09-03T19:24:01.055079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def quantize_note_times(notes, time_division, beat_res):\n    \"\"\" Quantize the notes items start and end values.\n    It shifts the notes so they start at times that match the quantization (e.g. 16 frames per bar)\n    :param notes: notes to quantize\n    :param time_division: MIDI time division / resolution, in ticks/beat (of the MIDI being parsed)\n    :param beat_res: number of frames (time steps, or positions) per beat\n    \"\"\"\n    ticks = int(time_division / beat_res)\n    quantized_ticks = np.arange(0, max([n.end for n in notes]) + 2 * ticks, ticks, dtype=int)\n    for i, note in enumerate(notes):  # items are notes\n        note.start = quantized_ticks[np.argmin(np.abs(quantized_ticks - note.start))]\n        note.end = quantized_ticks[np.argmin(np.abs(quantized_ticks - note.end))]\n\n        if note.start == note.end:  # if this happens to often, consider using a higher beat resolution\n            note.end += ticks  # like 8 frames per beat or 24 frames per bar\n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T18:33:41.473247Z","iopub.execute_input":"2021-09-03T18:33:41.473635Z","iopub.status.idle":"2021-09-03T18:33:41.480054Z","shell.execute_reply.started":"2021-09-03T18:33:41.473589Z","shell.execute_reply":"2021-09-03T18:33:41.479233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_duplicated_notes(notes):\n    \"\"\" Remove possible duplicated notes, i.e. with the same pitch, starting and ending times.\n    Before running this function make sure the notes has been sorted by start and pitch:\n    notes.sort(key=lambda x: (x.start, x.pitch))\n    :param notes: notes to analyse\n    \"\"\"\n    for i in range(len(notes) - 1, 0, -1):  # removing possible duplicated notes\n        if notes[i].pitch == notes[i - 1].pitch and notes[i].start == notes[i - 1].start and \\\n                notes[i].end == notes[i - 1].end:\n            del notes[i]\n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T18:33:42.51638Z","iopub.execute_input":"2021-09-03T18:33:42.518703Z","iopub.status.idle":"2021-09-03T18:33:42.526916Z","shell.execute_reply.started":"2021-09-03T18:33:42.518656Z","shell.execute_reply":"2021-09-03T18:33:42.525951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = []\nfor track in midi.instruments:\n    quantize_note_times(track.notes, remi_enc.current_midi_metadata['time_division'], max(remi_enc.beat_res.values()))\n    track.notes.sort(key=lambda x: (x.start, x.pitch))  # sort notes\n    remove_duplicated_notes(track.notes)  # remove possible duplicated notes\n#     print(track.notes)\n    # Convert track to tokens\n    print(remi_enc.track_to_tokens(track))\n    tokens.append(remi_enc.track_to_tokens(track))\ntokens","metadata":{"execution":{"iopub.status.busy":"2021-09-03T18:47:05.313609Z","iopub.execute_input":"2021-09-03T18:47:05.313958Z","iopub.status.idle":"2021-09-03T18:47:05.355098Z","shell.execute_reply.started":"2021-09-03T18:47:05.313927Z","shell.execute_reply":"2021-09-03T18:47:05.354196Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"programs = get_midi_programs(midi)\ninst, temp = remi_enc.tokens_to_track(tokens[0])\nprint(inst)\nprint(temp)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T18:54:47.37914Z","iopub.execute_input":"2021-09-03T18:54:47.379535Z","iopub.status.idle":"2021-09-03T18:54:47.386202Z","shell.execute_reply.started":"2021-09-03T18:54:47.379504Z","shell.execute_reply":"2021-09-03T18:54:47.384997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi.dump('orig.midi')","metadata":{"execution":{"iopub.status.busy":"2021-09-02T15:59:51.434338Z","iopub.execute_input":"2021-09-02T15:59:51.434729Z","iopub.status.idle":"2021-09-02T15:59:51.445193Z","shell.execute_reply.started":"2021-09-02T15:59:51.434696Z","shell.execute_reply":"2021-09-02T15:59:51.444194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converted_back_midi.dump('blues.midi')","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:23:21.543553Z","iopub.execute_input":"2021-09-03T19:23:21.543882Z","iopub.status.idle":"2021-09-03T19:23:21.548443Z","shell.execute_reply.started":"2021-09-03T19:23:21.543852Z","shell.execute_reply":"2021-09-03T19:23:21.547402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a MIDI file from prediction\ndef create_MIDI(gen, name=\"\"):\n    \n    # the offset is the time difference between notes, we assume its 0.5 here\n    offset = 0\n    music = []\n    \n    for seq in gen:\n        # chords are seperated by .\n#         if ('.' in seq) or seq.isdigit():\n#             chordnotes = seq.split('.')\n#             n = []\n            \n#             for cur_note in chordnotes:\n#                 new_n = note.Note(int(cur_note))\n#                 new_n.storedInstrument = instrument.Piano() # single piano instrument only\n#                 n.append(new_n)\n            \n#             new_c = chord.Chord(n)\n#             new_c.offset = offset\n#             music.append(new_c)\n        \n#         else:            \n        new_n = note.Note(seq)\n#         new_n.storedInstrument = instrument.Piano() # single piano instrument only\n        new_n.storedInstrument = midi.instruments[0]\n        new_n.offset = offset\n        music.append(new_n)\n        \n        offset += 0.5\n    \n    print(notes)\n    # producing a MIDI stream\n    midi_file = stream.Stream(music)\n    \n    midi_file.write('midi', fp='new_music_'+name+'.mid')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T16:32:33.380868Z","iopub.execute_input":"2021-09-01T16:32:33.381208Z","iopub.status.idle":"2021-09-01T16:32:33.386867Z","shell.execute_reply.started":"2021-09-01T16:32:33.381176Z","shell.execute_reply":"2021-09-01T16:32:33.38605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_MIDI(sequence, name='blues_all_1.87_temphalf')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T16:32:41.432303Z","iopub.execute_input":"2021-09-01T16:32:41.43271Z","iopub.status.idle":"2021-09-01T16:32:41.884504Z","shell.execute_reply.started":"2021-09-01T16:32:41.432679Z","shell.execute_reply":"2021-09-01T16:32:41.883506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_input(filename, folder=False):\n    # master list of notes\n    notes = []\n    \n    # converting folders with multiple MIDI files\n    if folder == True:\n        assert os.path.exists('../input/classical-music-midi/'+filename)\n        for file in glob.glob('../input/classical-music-midi/'+filename+'/*.mid'):\n            notes_per_piece = []\n            # read the MIDI file\n            midi = converter.parse(file)            \n            \n            try: # file has instrument parts\n                s2 = instrument.partitionByInstrument(midi)\n                notes_to_parse = s2.parts[1].recurse() \n            except: # file has notes in a flat structure\n                notes_to_parse = midi.flat.notes\n            \n#             print(notes_to_parse)\n            for element in notes_to_parse:\n                if isinstance(element, note.Note):\n                    notes_per_piece.append(str(element.pitch))\n                elif isinstance(element, chord.Chord):\n                    notes_per_piece.append('.'.join(str(n) for n in element.normalOrder))\n            notes.append(notes_per_piece)\n    else:\n        assert os.path.exists(filename)\n        midi = converter.parse(filename)\n        try: # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[1].recurse() \n        except: # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n#         print(notes_to_parse)\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n        \n\n    with open('./notes', 'wb') as filepath:\n        pickle.dump(notes, filepath)\n#     print(notes)\n    return notes","metadata":{"execution":{"iopub.status.busy":"2021-08-29T15:39:57.254503Z","iopub.execute_input":"2021-08-29T15:39:57.254905Z","iopub.status.idle":"2021-08-29T15:39:57.268593Z","shell.execute_reply.started":"2021-08-29T15:39:57.254859Z","shell.execute_reply":"2021-08-29T15:39:57.266854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notes = []\nwith (open(\"../input/beethoven-notes/notes\", \"rb\")) as openfile:\n    while True:\n        try:\n            notes = (pickle.load(openfile))\n        except EOFError:\n            break\n# notes = ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:30:06.488151Z","iopub.execute_input":"2021-08-29T14:30:06.488517Z","iopub.status.idle":"2021-08-29T14:30:06.523884Z","shell.execute_reply.started":"2021-08-29T14:30:06.488486Z","shell.execute_reply":"2021-08-29T14:30:06.523092Z"},"trusted":true},"execution_count":null,"outputs":[]}]}