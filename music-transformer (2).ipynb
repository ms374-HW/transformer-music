{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install music21\n!pip install miditoolkit \n!pip install miditok","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:34:18.284897Z","iopub.execute_input":"2021-09-07T12:34:18.285196Z","iopub.status.idle":"2021-09-07T12:35:03.641939Z","shell.execute_reply.started":"2021-09-07T12:34:18.285124Z","shell.execute_reply":"2021-09-07T12:35:03.640990Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting music21\n  Downloading music21-6.7.1.tar.gz (19.2 MB)\n\u001b[K     |████████████████████████████████| 19.2 MB 806 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from music21) (4.0.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from music21) (1.0.1)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from music21) (8.8.0)\nCollecting webcolors\n  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: music21\n  Building wheel for music21 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for music21: filename=music21-6.7.1-py3-none-any.whl size=21941692 sha256=be9f4d10dc5beffe3ff360ea1064d7e6ac8cd5ca8a5a8c6a1d95d1273a41cd3a\n  Stored in directory: /root/.cache/pip/wheels/72/44/61/90e4e65262ca1b4d9f707527b540729ce3f64e00fc6b38d54c\nSuccessfully built music21\nInstalling collected packages: webcolors, music21\nSuccessfully installed music21-6.7.1 webcolors-1.11.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting miditoolkit\n  Downloading miditoolkit-0.1.14.tar.gz (18 kB)\nRequirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from miditoolkit) (1.19.5)\nCollecting mido>=1.1.16\n  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n\u001b[K     |████████████████████████████████| 51 kB 754 kB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: miditoolkit\n  Building wheel for miditoolkit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for miditoolkit: filename=miditoolkit-0.1.14-py3-none-any.whl size=19537 sha256=3c34e357654a8a81d8806399cf4a4f1de77c4949256e1ab08da3527a380c8a25\n  Stored in directory: /root/.cache/pip/wheels/00/ac/58/1527193cdf6d74e3d056086f1be24f4fcb465c45212624b60c\nSuccessfully built miditoolkit\nInstalling collected packages: mido, miditoolkit\nSuccessfully installed miditoolkit-0.1.14 mido-1.2.10\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting miditok\n  Downloading miditok-0.1.8-py3-none-any.whl (50 kB)\n\u001b[K     |████████████████████████████████| 50 kB 708 kB/s eta 0:00:01\n\u001b[?25hCollecting numpy>=1.21\n  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n\u001b[K     |████████████████████████████████| 15.7 MB 1.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: miditoolkit>=0.1.14 in /opt/conda/lib/python3.7/site-packages (from miditok) (0.1.14)\nRequirement already satisfied: mido>=1.1.16 in /opt/conda/lib/python3.7/site-packages (from miditoolkit>=0.1.14->miditok) (1.2.10)\nInstalling collected packages: numpy, miditok\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.19.5\n    Uninstalling numpy-1.19.5:\n      Successfully uninstalled numpy-1.19.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-cloud 0.1.13 requires tensorflow<3.0,>=1.15.0, which is not installed.\nfancyimpute 0.5.5 requires tensorflow, which is not installed.\ndask-cudf 21.6.1+2.g101fc0fda4 requires cupy-cuda112, which is not installed.\ncudf 21.6.1+2.g101fc0fda4 requires cupy-cuda110, which is not installed.\nyellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.2 which is incompatible.\ntensorflow-gpu 2.4.1 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.4.2 which is incompatible.\nmatrixprofile 1.1.10 requires protobuf==3.11.2, but you have protobuf 3.17.3 which is incompatible.\nkornia 0.5.5 requires numpy<=1.19, but you have numpy 1.21.2 which is incompatible.\nimbalanced-learn 0.8.0 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\ndask-cudf 21.6.1+2.g101fc0fda4 requires dask<=2021.5.1,>=2021.4.0, but you have dask 2021.6.2 which is incompatible.\ndask-cudf 21.6.1+2.g101fc0fda4 requires distributed<=2021.5.1,>=2.22.0, but you have distributed 2021.6.2 which is incompatible.\u001b[0m\nSuccessfully installed miditok-0.1.8 numpy-1.21.2\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom io import open\nimport tensorflow as tf\nimport glob\nimport pickle\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport time\nimport copy\nfrom pathlib import Path, PurePath, PurePosixPath\nfrom music21 import converter, instrument, note, chord, stream\nfrom miditok import CPWordEncoding, REMIEncoding, get_midi_programs\nfrom miditoolkit import MidiFile","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-07T12:39:27.683694Z","iopub.execute_input":"2021-09-07T12:39:27.684051Z","iopub.status.idle":"2021-09-07T12:39:33.381711Z","shell.execute_reply.started":"2021-09-07T12:39:27.684018Z","shell.execute_reply":"2021-09-07T12:39:33.380818Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Creates the tokenizer and list the file paths\nremi_enc = REMIEncoding()  # uses defaults parameters\n# filename = 'chopin'\nfiles_paths = list(glob.glob('../input/maestropianomidi/maestro-v3.0.0/2018/*.midi'))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:39:33.383022Z","iopub.execute_input":"2021-09-07T12:39:33.383350Z","iopub.status.idle":"2021-09-07T12:39:33.421206Z","shell.execute_reply.started":"2021-09-07T12:39:33.383316Z","shell.execute_reply":"2021-09-07T12:39:33.420510Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"midi = MidiFile('../input/blues-genre-midi-melodies/blues/035d2131e824eb51878007013786806a.mid')\nremi_enc.current_midi_metadata = {'time_division': midi.ticks_per_beat, 'tempo_changes': midi.tempo_changes}\nmidi","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:56:29.163937Z","iopub.execute_input":"2021-09-06T19:56:29.164229Z","iopub.status.idle":"2021-09-06T19:56:29.191597Z","shell.execute_reply.started":"2021-09-06T19:56:29.164196Z","shell.execute_reply":"2021-09-06T19:56:29.190632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notes = []\n\nremi_enc = REMIEncoding()\n\nfor file in files_paths:\n\n    # read the MIDI file\n    midi = MidiFile(file)\n\n    # Converts MIDI to tokens\n    tokens = remi_enc.midi_to_tokens(midi)\n    \n    notes.append(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:39:38.615457Z","iopub.execute_input":"2021-09-07T12:39:38.615816Z","iopub.status.idle":"2021-09-07T12:48:13.348434Z","shell.execute_reply.started":"2021-09-07T12:39:38.615785Z","shell.execute_reply":"2021-09-07T12:48:13.347569Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# all the beethoven files have 2 tracks (instruments) - Piano left and right\n# for now, we will only be using for piano right since it determines the melody\nmidi.instruments","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:52:22.053806Z","iopub.execute_input":"2021-09-07T12:52:22.054204Z","iopub.status.idle":"2021-09-07T12:52:22.068924Z","shell.execute_reply.started":"2021-09-07T12:52:22.054171Z","shell.execute_reply":"2021-09-07T12:52:22.067875Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[Instrument(program=0, is_drum=False, name=\"13 to 15\")]"},"metadata":{}}]},{"cell_type":"code","source":"# piano_right_notes = [note[0] for note in notes]\n# piano_right_notes = [np.asarray(note) for note in piano_right_notes]\n# piano_right_notes = np.array(piano_right_notes, dtype='object')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T20:06:42.350158Z","iopub.execute_input":"2021-09-06T20:06:42.35051Z","iopub.status.idle":"2021-09-06T20:06:42.354549Z","shell.execute_reply.started":"2021-09-06T20:06:42.35048Z","shell.execute_reply":"2021-09-06T20:06:42.353333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:52:29.904104Z","iopub.execute_input":"2021-09-07T12:52:29.904433Z","iopub.status.idle":"2021-09-07T12:52:29.911073Z","shell.execute_reply.started":"2021-09-07T12:52:29.904402Z","shell.execute_reply":"2021-09-07T12:52:29.909960Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"ticks per beat: 384\nmax tick: 276806\ntempo changes: 1\ntime sig: 1\nkey sig: 0\nmarkers: 0\nlyrics: False\ninstruments: 1"},"metadata":{}}]},{"cell_type":"code","source":"len(notes[0][0])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:52:32.470163Z","iopub.execute_input":"2021-09-07T12:52:32.470467Z","iopub.status.idle":"2021-09-07T12:52:32.475636Z","shell.execute_reply.started":"2021-09-07T12:52:32.470439Z","shell.execute_reply":"2021-09-07T12:52:32.474565Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"45661"},"metadata":{}}]},{"cell_type":"code","source":"notes = [note[0] for note in notes]","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:52:33.749770Z","iopub.execute_input":"2021-09-07T12:52:33.750115Z","iopub.status.idle":"2021-09-07T12:52:33.755412Z","shell.execute_reply.started":"2021-09-07T12:52:33.750087Z","shell.execute_reply":"2021-09-07T12:52:33.754543Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"len(notes[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:52:35.108226Z","iopub.execute_input":"2021-09-07T12:52:35.108535Z","iopub.status.idle":"2021-09-07T12:52:35.116184Z","shell.execute_reply.started":"2021-09-07T12:52:35.108505Z","shell.execute_reply":"2021-09-07T12:52:35.115333Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"45661"},"metadata":{}}]},{"cell_type":"code","source":"class Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n    \n    def words(self):\n        return self.idx2word\n\n\nclass Corpus(object):\n    def __init__(self, notes, num):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(notes[:-num])\n        self.valid = self.tokenize(notes[-num:])\n#         self.test = self.tokenize(notes[-2:])\n     \n    def tokenize(self, notes):\n        \"\"\"Tokenizes a note sequence\"\"\"\n        assert len(notes) > 0\n        \n        for song in notes:\n            for note in song:\n                self.dictionary.add_word(note)\n        idss = []\n        \n        for song in notes:\n            ids = []\n            for note in song:\n                ids.append(self.dictionary.word2idx[note])\n        idss.append(torch.tensor(ids).type(torch.int64))\n        ids = torch.cat(idss)\n            \n\n        return ids","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:53:39.612269Z","iopub.execute_input":"2021-09-07T12:53:39.612585Z","iopub.status.idle":"2021-09-07T12:53:39.621549Z","shell.execute_reply.started":"2021-09-07T12:53:39.612555Z","shell.execute_reply":"2021-09-07T12:53:39.620764Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"corpus = Corpus(notes, 7)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:18.029775Z","iopub.execute_input":"2021-09-07T12:54:18.030131Z","iopub.status.idle":"2021-09-07T12:54:19.250056Z","shell.execute_reply.started":"2021-09-07T12:54:18.030100Z","shell.execute_reply":"2021-09-07T12:54:19.249202Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"len(notes)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:19.251385Z","iopub.execute_input":"2021-09-07T12:54:19.251725Z","iopub.status.idle":"2021-09-07T12:54:19.256799Z","shell.execute_reply.started":"2021-09-07T12:54:19.251690Z","shell.execute_reply":"2021-09-07T12:54:19.256003Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"93"},"metadata":{}}]},{"cell_type":"code","source":"corpus.train.size(0)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:25.408568Z","iopub.execute_input":"2021-09-07T12:54:25.408907Z","iopub.status.idle":"2021-09-07T12:54:25.414146Z","shell.execute_reply.started":"2021-09-07T12:54:25.408877Z","shell.execute_reply":"2021-09-07T12:54:25.413168Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"13874"},"metadata":{}}]},{"cell_type":"code","source":"device = 'cpu'\ndef batchify(data, bsz):\n    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n    that wouldn't cleanly fit.\n\n    Args:\n        data: Tensor, shape [N]\n        bsz: int, batch size\n\n    Returns:\n        Tensor of shape [N // bsz, bsz]\n    \"\"\"\n    seq_len = data.size(0) // bsz\n    data = data[:seq_len * bsz]\n    data = data.view(bsz, seq_len).t().contiguous()\n    return data.to(device)\n\neval_batch_size = 10\nbatch_size = 100\ntrain_data = batchify(corpus.train, batch_size)\nval_data = batchify(corpus.valid, batch_size)\n# test_data = batchify(corpus.test, batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:31.357558Z","iopub.execute_input":"2021-09-07T12:54:31.358045Z","iopub.status.idle":"2021-09-07T12:54:31.367560Z","shell.execute_reply.started":"2021-09-07T12:54:31.358003Z","shell.execute_reply":"2021-09-07T12:54:31.366715Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:32.788412Z","iopub.execute_input":"2021-09-07T12:54:32.788780Z","iopub.status.idle":"2021-09-07T12:54:32.794842Z","shell.execute_reply.started":"2021-09-07T12:54:32.788748Z","shell.execute_reply":"2021-09-07T12:54:32.793710Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([138, 100])"},"metadata":{}}]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"\"\"Inject some information about the relative or absolute position of the tokens\n        in the sequence. The positional encodings have the same dimension as\n        the embeddings, so that the two can be summed. Here, we use sine and cosine\n        functions of different frequencies.\n    .. math::\n        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    Examples:\n        >>> pos_encoder = PositionalEncoding(d_model)\n    \"\"\"\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.d_model = d_model\n\n        # PE is the Positional Encoding matrix \n        # THIS STORES THE POSITIONS OF THE SEQUENCE\n        pe = torch.zeros(max_len, d_model)\n        \n        # Arange - RETURNS A RANGE BETWEEN VALUES, HERE IT IS 0 - max_len\n        # unsqueeze - adds a dimension, 1 means that each element in the first list is now in a list\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        \n        # division term, here it is (10000 ** ((2 * i)/d_model))\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        # calculating the position encoding for the even and odd terms\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        # Unsqueeze 0 will put PE in one list\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        Examples:\n            >>> output = pos_encoder(x)\n        \"\"\"\n        # make embeddings relatively larger\n        # This is so we do not lose the importance of the embedding\n        x = x * math.sqrt(self.d_model)\n        # we add the embedding to the PE \n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:28.330626Z","iopub.execute_input":"2021-08-29T17:11:28.330944Z","iopub.status.idle":"2021-08-29T17:11:28.340154Z","shell.execute_reply.started":"2021-08-29T17:11:28.330916Z","shell.execute_reply":"2021-08-29T17:11:28.339348Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        \n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n        self.d_model = d_model\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        # make embeddings relatively larger\n        # This is so we do not lose the importance of the embedding\n        x = x * math.sqrt(self.d_model)\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:36.108288Z","iopub.execute_input":"2021-09-07T12:54:36.108611Z","iopub.status.idle":"2021-09-07T12:54:36.116922Z","shell.execute_reply.started":"2021-09-07T12:54:36.108579Z","shell.execute_reply":"2021-09-07T12:54:36.115438Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\"Parallel self attention given the number of heads and size\n    .. math::\n        X * Wk = K\n        X * Wq = Q\n        X * Wv = V\n        \\text{attention} = softmax(Q * transpose(K) / sqrt(d_model)) * V\n    Args:\n        d_model: the embed dim (default = 256).\n        heads: number of heads (default = 4)\n        max_length = max length of sequences (default = 2048)\n        dropout: the dropout value (default = 0.1).\n    Examples:\n        >>> attention = MultiHeadAttention(d_model, heads)\n    \"\"\"\n    def __init__(self, d_model = 256, heads = 4, max_length = 2048, dropout = 0.1):\n        super().__init__()\n        \n        self.d = d_model\n        self.h = heads\n        self.dh = d_model // heads\n        self.max_length = max_length\n        self.E = torch.randn([heads, self.max_length, self.dh])\n        \n        self.q_linear = nn.Linear(self.dh, self.dh)\n        self.v_linear = nn.Linear(self.dh, self.dh)\n        self.k_linear = nn.Linear(self.dh, self.dh)\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(d_model, d_model)\n    \n    def forward(self, q, k, v, mask=None):\n        \n        batch_size = q.size(0)\n        T = q.size(1)\n        dh = q.size(2) // self.h\n        \n        k = k.view(batch_size, T, self.h, dh)\n        q = q.view(batch_size, T, self.h, dh)\n        v = v.view(batch_size, T, self.h, dh)\n        # perform linear operation and split into h heads\n        \n        K = self.k_linear(k)\n        Q = self.q_linear(q)\n        V = self.v_linear(v)\n        \n        # transpose to get dimensions bs * h * sl * d_model\n       \n        K = K.transpose(1,2)\n        Q = Q.transpose(1,2)\n        V = V.transpose(1,2)\n\n        #start index of position embedding\n        \n        embedding_start = self.max_length - T\n        \n        #apply same position embeddings across the batch\n        \n        Er  = self.E[:, embedding_start:, :].unsqueeze(0)\n        \n        QE = torch.matmul(Q, K.transpose(-2, -1))\n        QE = self.mask_attention_positions(QE)\n        \n        #Get relative position attention scores\n        #combine batch with head dimension\n        SRel = self.skew_padding_position(QE)\n\n#         Q = Q.contiguous().view(batch_size*self.h, T, self.dh)\n#         K = K.contiguous().view(batch_size*self.h, T, self.dh)\n#         V = V.contiguous().view(batch_size*self.h, T, self.dh)\n        \n        #Compute scaled dot-product self-attention\n        #scale pre-matrix multiplication   \n        Q = Q / (q.size(2) ** (1/4))\n        K = K / (q.size(2) ** (1/4))\n\n        # calculate attention \n        scores, weights = self.attention(QE, SRel, V, mask, self.dropout)\n        \n        # concatenate heads and put through final linear layer\n        concat = scores.transpose(1,2).contiguous().view(batch_size, -1, self.d)\n        \n        output = self.out(concat)\n    \n        return output\n\n    def attention(self, QE, Srel, V, mask=None, dropout=None):\n        log = QE + Srel\n        log = log / math.sqrt(self.dh)\n\n        if mask is not None:\n#             mask = mask.unsqueeze(1)\n            log += (mask.to(torch.int64) * -1e9).to(log.dtype)\n\n#             print(mask.shape)\n#             log = log.masked_fill(mask == 0, float('-inf'))\n\n        scores = F.softmax(log, -1)\n        \n        if dropout is not None:\n            scores = dropout(scores)\n            \n#         print(scores.shape, V.shape)\n        attention = torch.matmul(scores, V)\n        \n#         scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n\n#         if mask is not None:\n#             print(scores.shape)\n\n#     #         mask = mask.unsqueeze(1)\n#             print(mask.shape)\n#     #         scores = scores.masked_fill(mask == 0, float('-inf'))\n#         scores = F.softmax(scores, dim=-1)\n\n#         \n\n#         output = torch.matmul(scores, v)\n        return attention, scores\n    \n    def mask_attention_positions(self, qe):\n        # to avoid looking backward by masking the positions\n        index = qe.shape[-1]\n        mask = torch.triu(torch.ones(index, index), 1).flip(1)\n        return qe.masked_fill((mask == 1), 0)\n    \n    def skew_padding_position(self, qe):\n        # to add padding to the skewed result after masking the matrix\n        # column of zeros on left\n        padded_qe = F.pad(qe, [1,0])\n        s = padded_qe.shape\n        padded_qe = padded_qe.view(s[0], s[1], s[3], s[2])\n        #take out first (padded) row\n        return padded_qe[:,:,1:,:]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:29.221271Z","iopub.execute_input":"2021-08-29T17:11:29.221605Z","iopub.status.idle":"2021-08-29T17:11:29.240108Z","shell.execute_reply.started":"2021-08-29T17:11:29.221574Z","shell.execute_reply":"2021-08-29T17:11:29.239141Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n#     def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n#         super().__init__() \n#         # We set d_ff as a default to 2048\n#         self.linear_1 = nn.Linear(d_model, d_ff)\n#         self.dropout = nn.Dropout(dropout)\n#         self.linear_2 = nn.Linear(d_ff, d_model)\n#     def forward(self, x):\n#         x = self.dropout(F.relu(self.linear_1(x)))\n#         x = self.linear_2(x)\n#         return x\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear_2(self.dropout(F.relu(self.linear_1(x))))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:29.51977Z","iopub.execute_input":"2021-08-29T17:11:29.520083Z","iopub.status.idle":"2021-08-29T17:11:29.525789Z","shell.execute_reply.started":"2021-08-29T17:11:29.520034Z","shell.execute_reply":"2021-08-29T17:11:29.524865Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build an encoder layer with one multi-head attention layer and one # feed-forward layer\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout = 0.1):\n        super().__init__()\n        self.norm_1 = nn.LayerNorm(d_model)\n        self.norm_2 = nn.LayerNorm(d_model)\n        \n        self.attn = MultiHeadAttention(d_model, heads)\n        self.ff = FeedForward(d_model)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        x2 = self.norm_1(x)\n        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n        x2 = self.norm_2(x)\n        x = x + self.dropout_2(self.ff(x2))\n        return x\n    \n# build a decoder layer with two multi-head attention layers and\n# one feed-forward layer\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout=0.1):\n        super().__init__()\n        self.norm_1 = nn.LayerNorm(d_model)\n        self.norm_2 = nn.LayerNorm(d_model)\n        self.norm_3 = nn.LayerNorm(d_model)\n        \n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n        \n        self.attn_1 = MultiHeadAttention(d_model, heads)\n        self.attn_2 = MultiHeadAttention(d_model, heads)\n        self.ff = FeedForward(d_model)\n    def forward(self, x, memory, mask = None):\n#         x2 = self.norm_1(x)\n#         x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n#         x2 = self.norm_2(x)\n#         x = x + self.dropout_2(self.attn_2(x2, x2, x2,\n#         src_mask))\n#         x2 = self.norm_3(x)\n#         x = x + self.dropout_3(self.ff(x2))\n        \n        #perform masked attention on input\n        #masked so queries cannot attend to subsequent keys\n        #Pass through sublayers of attention and feedforward.\n        #Apply dropout to sublayer output, add it to input, and norm.\n        attn = self.attn_1(x, x, x, mask)\n        x = x + self.dropout_1(attn)\n        x = self.norm_1(x)\n        x = x + self.dropout_2(self.attn_2(x, memory, memory, mask))\n        ff = self.ff(x)\n        x = x + self.dropout_2(ff)\n        x = self.norm_2(x)\n\n        return x\n#         return x\n# We can then build a convenient cloning function that can generate multiple layers:\ndef get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:29.785832Z","iopub.execute_input":"2021-08-29T17:11:29.786141Z","iopub.status.idle":"2021-08-29T17:11:29.802339Z","shell.execute_reply.started":"2021-08-29T17:11:29.786111Z","shell.execute_reply":"2021-08-29T17:11:29.801479Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, d_model, N, heads):\n        super().__init__()\n        self.N = N\n        self.layers = get_clones(EncoderLayer(d_model, heads), self.N)\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, src, mask):\n        x = self.layers[0](src, mask)\n        for i in range(1,self.N):\n            x = self.layers[i](x, mask)\n        return self.norm(x)\n    \nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, d_model, N, heads):\n        super().__init__()\n        self.N = N\n        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, src, trg, src_mask = None):\n        x = self.layers[0](src, trg, src_mask)\n        for i in range(1,self.N):\n            x = self.layers[i](x, trg, src_mask)\n        return self.norm(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:30.089969Z","iopub.execute_input":"2021-08-29T17:11:30.090276Z","iopub.status.idle":"2021-08-29T17:11:30.099241Z","shell.execute_reply.started":"2021-08-29T17:11:30.090245Z","shell.execute_reply":"2021-08-29T17:11:30.09815Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, d_model, nhead, nhid, nlayers, dropout=0.5, max_length = 2048):\n        super(TransformerModel, self).__init__()\n        try:\n            from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n        except:\n            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n        self.model_type = 'Transformer'\n        # original mask\n        self.src_mask = None\n        self.max_length = max_length\n        self.d_model = d_model\n        \n        # embedding encoding\n        self.embedding = nn.Embedding(ntoken, d_model)\n        \n        # positional encoding\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        \n        # encoder\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, nhid, dropout)\n        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n        \n#         self.decoder = nn.Linear(d_model, ntoken)\n\n\n        \n#         self.encoder = Encoder(d_model, nlayers, nhead)\n#         self.encoder.eval()\n        # decoder\n        decoder_layers = TransformerDecoderLayer(d_model, nhead, nhid, dropout)\n        self.decoder = TransformerDecoder(decoder_layers, nlayers)\n#         self.decoder = Decoder(ntoken, d_model, nlayers, nhead)\n#         self.decoder.eval()\n        # classification layer\n        self.classification_layer = nn.Linear(d_model, ntoken)\n        \n        self.init_weights()\n    def _generate_square_subsequent_mask(self, sz):\n        \n        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n#         mask = utils.get_masked_with_pad_tensor(self.max_length, x, x, config.pad_token)\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        nn.init.uniform_(self.embedding.weight, -initrange, initrange)\n        self.classification_layer.bias.data.zero_()\n        self.classification_layer.weight.data.uniform_(-initrange, initrange)\n#         nn.init.zeros_(self.decoder.weight)\n#         nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n\n    def forward(self, src, src_mask):\n        \n        if src_mask == None:\n            src_mask = self._generate_square_subsequent_mask(len(src))\n        self.src_mask = src_mask\n\n        src = self.pos_encoder(self.embedding(src))\n        output = self.encoder(src, self.src_mask)\n        output = self.decoder(output, src)\n        \n        #Flatten:\n#         shape = output.shape\n#         tensor_reshaped = output.reshape(shape[0],-1)\n#         #Drop all rows containing any nan:\n#         tensor_reshaped = tensor_reshaped[~torch.any(tensor_reshaped.isnan(),dim=1)]\n#         #Reshape back:\n#         output = tensor_reshaped.reshape(tensor_reshaped.shape[0],*shape[1:])\n#         print(output)\n        output = self.classification_layer(output)\n#         print(output)\n        return F.log_softmax(output, dim=-1)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:52.636262Z","iopub.execute_input":"2021-09-07T12:54:52.636571Z","iopub.status.idle":"2021-09-07T12:54:52.648911Z","shell.execute_reply.started":"2021-09-07T12:54:52.636543Z","shell.execute_reply":"2021-09-07T12:54:52.647828Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"len(corpus.dictionary)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:53.719122Z","iopub.execute_input":"2021-09-07T12:54:53.719447Z","iopub.status.idle":"2021-09-07T12:54:53.725725Z","shell.execute_reply.started":"2021-09-07T12:54:53.719418Z","shell.execute_reply":"2021-09-07T12:54:53.724730Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"222"},"metadata":{}}]},{"cell_type":"code","source":"ntokens = len(corpus.dictionary)\nemsize = 200\nnhead = 2\nnhid = 200\nnlayer = 2\ndropout = 0.2\n# Loop over epochs.\nlr = 5\nbest_val_loss = None\nepochs = 200\nsave = './model.pt'\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:54:57.750763Z","iopub.execute_input":"2021-09-07T12:54:57.751086Z","iopub.status.idle":"2021-09-07T12:54:57.756348Z","shell.execute_reply.started":"2021-09-07T12:54:57.751059Z","shell.execute_reply":"2021-09-07T12:54:57.755120Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_losses = []\nemsizes = []\nnheads = []\nnhids = []\nnlayers = []\n\nfor em in emsize:\n    for head in nhead:\n        for hid in nhid:\n            for layer in nlayer:\n                emsizes.append(em)\n                nheads.append(head)\n                nhids.append(hid)\n                nlayers.append(layer)\n                best_val_loss = None\n\n                model = TransformerModel(ntokens, em, head, hid, layer, dropout).to(device)\n\n                # At any point you can hit Ctrl + C to break out of training early.\n                try:\n                    for epoch in range(1, epochs+1):\n                        epoch_start_time = time.time()\n                        train()\n                        val_loss = evaluate(test_data)\n                        train_loss = evaluate(train_data)\n                        print('-' * 89)\n                        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | train loss {:5.2f}'\n                                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                                           val_loss, train_loss, math.exp(val_loss)))\n                        print('-' * 89)\n                        # Save the model if the validation loss is the best we've seen so far.\n                        if not best_val_loss or val_loss < best_val_loss:\n                            with open(save, 'wb') as f:\n                                torch.save(model, f)\n                            best_val_loss = val_loss\n                        else:\n                            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n                            lr /= 2.0\n                except KeyboardInterrupt:\n                    print('-' * 89)\n                    print('Exiting from training early')\n                final_losses.append(best_val_loss)","metadata":{"execution":{"iopub.status.busy":"2021-09-05T15:12:24.247456Z","iopub.execute_input":"2021-09-05T15:12:24.247766Z","iopub.status.idle":"2021-09-05T15:12:24.278517Z","shell.execute_reply.started":"2021-09-05T15:12:24.247738Z","shell.execute_reply":"2021-09-05T15:12:24.276863Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = list(zip(emsizes, nheads, nhids, nlayers, final_losses))\nlosses_df = pd.DataFrame(scores, columns=['embedding_size', 'num_heads', 'num_hidden', 'num_layers', 'best_loss'])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:12:10.237712Z","iopub.execute_input":"2021-08-29T17:12:10.238023Z","iopub.status.idle":"2021-08-29T17:12:10.251059Z","shell.execute_reply.started":"2021-08-29T17:12:10.237993Z","shell.execute_reply":"2021-08-29T17:12:10.250173Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses_df","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:12:12.034177Z","iopub.execute_input":"2021-08-29T17:12:12.034511Z","iopub.status.idle":"2021-08-29T17:12:12.05353Z","shell.execute_reply.started":"2021-08-29T17:12:12.03448Z","shell.execute_reply":"2021-08-29T17:12:12.052781Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model = TransformerModel(ntokens, emsize, nhead, nhid, nlayer, dropout).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:55:03.039773Z","iopub.execute_input":"2021-09-07T12:55:03.040162Z","iopub.status.idle":"2021-09-07T12:55:03.132508Z","shell.execute_reply.started":"2021-09-07T12:55:03.040122Z","shell.execute_reply":"2021-09-07T12:55:03.131706Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def repackage_hidden(h):\n    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:55:04.418931Z","iopub.execute_input":"2021-09-07T12:55:04.419272Z","iopub.status.idle":"2021-09-07T12:55:04.424297Z","shell.execute_reply.started":"2021-09-07T12:55:04.419238Z","shell.execute_reply":"2021-09-07T12:55:04.422921Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# get_batch subdivides the source data into chunks of length args.bptt.\n# If source is equal to the example output of the batchify function, with\n# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n# ┌ a g m s ┐ ┌ b h n t ┐\n# └ b h n t ┘ └ c i o u ┘\n# Note that despite the name of the function, the subdivison of data is not\n# done along the batch dimension (i.e. dimension 1), since that was handled\n# by the batchify function. The chunks are along dimension 0, corresponding\n# to the seq_len dimension in the LSTM.\nseq_length = 35\ndef get_batch(source, i):\n    seq_len = min(seq_length, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:55:05.055981Z","iopub.execute_input":"2021-09-07T12:55:05.056303Z","iopub.status.idle":"2021-09-07T12:55:05.061598Z","shell.execute_reply.started":"2021-09-07T12:55:05.056273Z","shell.execute_reply":"2021-09-07T12:55:05.060542Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz):\n    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:55:05.904187Z","iopub.execute_input":"2021-09-07T12:55:05.904519Z","iopub.status.idle":"2021-09-07T12:55:05.908841Z","shell.execute_reply.started":"2021-09-07T12:55:05.904487Z","shell.execute_reply":"2021-09-07T12:55:05.907843Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, eval_data):\n    model.eval()  # turn on evaluation mode\n    total_loss = 0.\n    src_mask = generate_square_subsequent_mask(seq_length).to(device)\n    with torch.no_grad():\n        for i in range(0, eval_data.size(0) - 1, seq_length):\n            data, targets = get_batch(eval_data, i)\n            batch_size = data.size(0)\n            if batch_size != seq_length:\n                src_mask = src_mask[:batch_size, :batch_size]\n            output = model(data, src_mask)\n            output_flat = output.view(-1, ntokens)\n            total_loss += batch_size * criterion(output_flat, targets).item()\n    return total_loss / (len(eval_data) - 1)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:55:06.514180Z","iopub.execute_input":"2021-09-07T12:55:06.514494Z","iopub.status.idle":"2021-09-07T12:55:06.522602Z","shell.execute_reply.started":"2021-09-07T12:55:06.514464Z","shell.execute_reply":"2021-09-07T12:55:06.520052Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\ndef train(model):\n    model.train()  # turn on train mode\n    total_loss = 0.\n    log_interval = 200\n    start_time = time.time()\n    src_mask = generate_square_subsequent_mask(seq_length).to(device)\n\n    num_batches = len(train_data) // seq_length\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_length)):\n        data, targets = get_batch(train_data, i)\n        batch_size = data.size(0)\n        if batch_size != seq_length:  # only on last batch\n            src_mask = src_mask[:batch_size, :batch_size]\n        output = model(data, src_mask)\n        loss = criterion(output.view(-1, ntokens), targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        total_loss += loss.item()\n        if batch % log_interval == 0 and batch > 0:\n            lr = scheduler.get_last_lr()[0]\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            ppl = math.exp(cur_loss)\n            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n            total_loss = 0\n            start_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:55:09.114487Z","iopub.execute_input":"2021-09-07T12:55:09.114860Z","iopub.status.idle":"2021-09-07T12:55:09.127431Z","shell.execute_reply.started":"2021-09-07T12:55:09.114827Z","shell.execute_reply":"2021-09-07T12:55:09.126389Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    for epoch in range(1, epochs+1):\n        epoch_start_time = time.time()\n        train(model)\n        val_loss = evaluate(model, val_data)\n        train_loss = evaluate(model, train_data)\n        print('-' * 89)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | train loss {:5.2f} | '\n                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                           val_loss, train_loss, math.exp(val_loss)))\n        print('-' * 89)\n        # Save the model if the validation loss is the best we've seen so far.\n        if not best_val_loss or val_loss < best_val_loss:\n            with open(save, 'wb') as f:\n                torch.save(model, f)\n            best_val_loss = val_loss\n        else:\n            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n            lr /= 2.0\nexcept KeyboardInterrupt:\n    print('-' * 89)\n    print('Exiting from training early')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-07T12:55:09.931690Z","iopub.execute_input":"2021-09-07T12:55:09.932024Z","iopub.status.idle":"2021-09-07T13:06:14.716837Z","shell.execute_reply.started":"2021-09-07T12:55:09.931995Z","shell.execute_reply":"2021-09-07T13:06:14.715965Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"-----------------------------------------------------------------------------------------\n| end of epoch   1 | time:  3.44s | valid loss  6.44 | train loss  6.32 | valid ppl   629.14\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time:  3.10s | valid loss  7.50 | train loss  8.36 | valid ppl  1812.40\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time:  3.35s | valid loss  6.16 | train loss  5.66 | valid ppl   471.89\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   4 | time:  3.14s | valid loss  4.77 | train loss  4.97 | valid ppl   117.91\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   5 | time:  3.59s | valid loss  6.04 | train loss  5.51 | valid ppl   419.90\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   6 | time:  3.39s | valid loss  6.92 | train loss  7.66 | valid ppl  1015.42\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   7 | time:  3.26s | valid loss  4.43 | train loss  4.70 | valid ppl    84.21\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   8 | time:  3.14s | valid loss  4.42 | train loss  4.74 | valid ppl    82.82\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch   9 | time:  3.06s | valid loss  4.31 | train loss  4.69 | valid ppl    74.72\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  10 | time:  3.32s | valid loss  4.44 | train loss  4.66 | valid ppl    84.87\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  11 | time:  3.10s | valid loss  4.54 | train loss  4.94 | valid ppl    93.27\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  12 | time:  3.13s | valid loss  6.38 | train loss  5.70 | valid ppl   588.96\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  13 | time:  3.36s | valid loss  5.25 | train loss  5.03 | valid ppl   191.07\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  14 | time:  3.08s | valid loss  4.97 | train loss  5.48 | valid ppl   143.43\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  15 | time:  3.53s | valid loss  8.19 | train loss  9.03 | valid ppl  3614.15\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  16 | time:  3.33s | valid loss  4.35 | train loss  4.71 | valid ppl    77.30\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  17 | time:  3.12s | valid loss  5.19 | train loss  4.98 | valid ppl   179.81\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  18 | time:  3.06s | valid loss  4.62 | train loss  5.03 | valid ppl   101.92\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  19 | time:  3.04s | valid loss  4.48 | train loss  4.76 | valid ppl    87.87\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  20 | time:  3.31s | valid loss  4.66 | train loss  5.11 | valid ppl   105.78\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  21 | time:  3.10s | valid loss  4.83 | train loss  5.36 | valid ppl   125.34\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  22 | time:  3.06s | valid loss  4.88 | train loss  5.40 | valid ppl   131.73\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  23 | time:  3.32s | valid loss  4.58 | train loss  4.71 | valid ppl    97.46\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  24 | time:  3.07s | valid loss  4.51 | train loss  4.62 | valid ppl    91.17\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  25 | time:  3.37s | valid loss  5.08 | train loss  4.91 | valid ppl   161.19\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  26 | time:  3.26s | valid loss  4.31 | train loss  4.54 | valid ppl    74.51\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  27 | time:  3.30s | valid loss  4.89 | train loss  4.87 | valid ppl   132.58\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  28 | time:  3.10s | valid loss  4.42 | train loss  4.89 | valid ppl    83.20\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  29 | time:  3.15s | valid loss  5.07 | train loss  5.70 | valid ppl   158.95\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  30 | time:  3.35s | valid loss  4.72 | train loss  4.77 | valid ppl   112.67\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  31 | time:  3.21s | valid loss  4.22 | train loss  4.53 | valid ppl    68.15\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  32 | time:  3.19s | valid loss  3.99 | train loss  4.34 | valid ppl    54.20\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  33 | time:  3.25s | valid loss  4.00 | train loss  4.35 | valid ppl    54.48\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  34 | time:  3.32s | valid loss  4.87 | train loss  4.64 | valid ppl   129.78\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  35 | time:  3.17s | valid loss  4.59 | train loss  4.46 | valid ppl    98.06\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  36 | time:  3.55s | valid loss  4.41 | train loss  4.36 | valid ppl    82.57\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  37 | time:  3.38s | valid loss  4.58 | train loss  4.35 | valid ppl    97.49\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  38 | time:  3.15s | valid loss  3.91 | train loss  4.16 | valid ppl    49.74\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  39 | time:  3.05s | valid loss  3.64 | train loss  4.12 | valid ppl    38.02\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  40 | time:  3.29s | valid loss  4.34 | train loss  4.53 | valid ppl    76.79\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  41 | time:  3.21s | valid loss  3.45 | train loss  3.62 | valid ppl    31.36\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  42 | time:  3.05s | valid loss  3.36 | train loss  3.82 | valid ppl    28.91\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  43 | time:  3.11s | valid loss  3.46 | train loss  3.68 | valid ppl    31.97\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  44 | time:  3.33s | valid loss  3.45 | train loss  3.54 | valid ppl    31.58\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  45 | time:  3.15s | valid loss  3.53 | train loss  3.50 | valid ppl    34.19\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  46 | time:  3.21s | valid loss  3.04 | train loss  3.35 | valid ppl    20.97\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  47 | time:  3.80s | valid loss  3.44 | train loss  3.33 | valid ppl    31.18\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  48 | time:  3.20s | valid loss  3.25 | train loss  3.45 | valid ppl    25.83\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  49 | time:  3.16s | valid loss  3.23 | train loss  3.31 | valid ppl    25.40\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  50 | time:  3.40s | valid loss  3.03 | train loss  3.19 | valid ppl    20.62\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  51 | time:  3.14s | valid loss  3.27 | train loss  3.07 | valid ppl    26.34\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  52 | time:  3.16s | valid loss  2.86 | train loss  2.89 | valid ppl    17.46\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  53 | time:  3.14s | valid loss  2.82 | train loss  2.82 | valid ppl    16.76\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  54 | time:  3.30s | valid loss  2.83 | train loss  2.69 | valid ppl    16.95\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  55 | time:  3.12s | valid loss  2.90 | train loss  2.57 | valid ppl    18.09\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  56 | time:  3.17s | valid loss  2.33 | train loss  2.15 | valid ppl    10.29\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  57 | time:  3.37s | valid loss  2.63 | train loss  2.13 | valid ppl    13.88\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  58 | time:  3.21s | valid loss  2.02 | train loss  1.90 | valid ppl     7.57\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  59 | time:  3.73s | valid loss  1.99 | train loss  1.61 | valid ppl     7.34\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  60 | time:  3.41s | valid loss  1.50 | train loss  1.40 | valid ppl     4.47\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  61 | time:  3.19s | valid loss  1.74 | train loss  1.41 | valid ppl     5.69\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  62 | time:  3.20s | valid loss  1.22 | train loss  0.88 | valid ppl     3.40\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  63 | time:  3.18s | valid loss  1.02 | train loss  0.74 | valid ppl     2.79\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  64 | time:  3.35s | valid loss  1.33 | train loss  0.81 | valid ppl     3.78\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  65 | time:  3.12s | valid loss  0.90 | train loss  0.59 | valid ppl     2.45\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  66 | time:  3.12s | valid loss  0.94 | train loss  0.79 | valid ppl     2.55\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  67 | time:  3.43s | valid loss  0.95 | train loss  0.59 | valid ppl     2.59\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  68 | time:  3.14s | valid loss  0.73 | train loss  0.44 | valid ppl     2.07\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  69 | time:  3.61s | valid loss  0.61 | train loss  0.40 | valid ppl     1.83\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  70 | time:  3.31s | valid loss  1.05 | train loss  0.72 | valid ppl     2.84\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  71 | time:  3.31s | valid loss  0.74 | train loss  0.39 | valid ppl     2.10\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  72 | time:  3.27s | valid loss  0.77 | train loss  0.44 | valid ppl     2.17\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  73 | time:  3.15s | valid loss  0.45 | train loss  0.28 | valid ppl     1.56\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  74 | time:  3.55s | valid loss  0.51 | train loss  0.41 | valid ppl     1.67\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  75 | time:  3.20s | valid loss  0.53 | train loss  0.39 | valid ppl     1.71\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  76 | time:  3.20s | valid loss  0.63 | train loss  0.54 | valid ppl     1.88\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  77 | time:  3.47s | valid loss  0.27 | train loss  0.18 | valid ppl     1.30\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  78 | time:  3.28s | valid loss  0.84 | train loss  0.42 | valid ppl     2.32\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  79 | time:  3.17s | valid loss  0.66 | train loss  0.38 | valid ppl     1.94\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  80 | time:  3.18s | valid loss  0.31 | train loss  0.36 | valid ppl     1.37\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  81 | time:  4.00s | valid loss  0.52 | train loss  0.28 | valid ppl     1.68\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  82 | time:  3.23s | valid loss  0.49 | train loss  0.29 | valid ppl     1.64\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  83 | time:  3.24s | valid loss  0.66 | train loss  0.30 | valid ppl     1.94\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  84 | time:  3.43s | valid loss  0.44 | train loss  0.26 | valid ppl     1.55\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  85 | time:  3.20s | valid loss  0.28 | train loss  0.20 | valid ppl     1.32\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  86 | time:  3.25s | valid loss  0.27 | train loss  0.19 | valid ppl     1.31\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  87 | time:  3.54s | valid loss  0.24 | train loss  0.13 | valid ppl     1.27\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  88 | time:  3.31s | valid loss  0.34 | train loss  0.20 | valid ppl     1.40\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  89 | time:  3.24s | valid loss  0.47 | train loss  0.25 | valid ppl     1.61\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  90 | time:  3.57s | valid loss  0.31 | train loss  0.17 | valid ppl     1.36\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  91 | time:  3.51s | valid loss  0.29 | train loss  0.14 | valid ppl     1.33\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  92 | time:  3.25s | valid loss  0.24 | train loss  0.26 | valid ppl     1.27\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  93 | time:  3.29s | valid loss  0.64 | train loss  0.36 | valid ppl     1.89\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  94 | time:  3.40s | valid loss  0.29 | train loss  0.13 | valid ppl     1.33\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  95 | time:  3.26s | valid loss  0.35 | train loss  0.19 | valid ppl     1.41\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  96 | time:  3.28s | valid loss  0.56 | train loss  0.29 | valid ppl     1.75\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  97 | time:  3.45s | valid loss  0.27 | train loss  0.15 | valid ppl     1.31\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  98 | time:  3.26s | valid loss  0.34 | train loss  0.15 | valid ppl     1.40\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch  99 | time:  3.23s | valid loss  0.30 | train loss  0.15 | valid ppl     1.35\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 100 | time:  3.67s | valid loss  0.34 | train loss  0.19 | valid ppl     1.40\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 101 | time:  3.47s | valid loss  0.19 | train loss  0.10 | valid ppl     1.21\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 102 | time:  3.26s | valid loss  0.23 | train loss  0.18 | valid ppl     1.26\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 103 | time:  3.46s | valid loss  0.20 | train loss  0.13 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 104 | time:  3.18s | valid loss  0.20 | train loss  0.10 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 105 | time:  3.19s | valid loss  0.25 | train loss  0.18 | valid ppl     1.28\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 106 | time:  3.21s | valid loss  0.33 | train loss  0.14 | valid ppl     1.40\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 107 | time:  3.45s | valid loss  0.19 | train loss  0.07 | valid ppl     1.21\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 108 | time:  3.27s | valid loss  0.23 | train loss  0.11 | valid ppl     1.25\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 109 | time:  3.31s | valid loss  0.18 | train loss  0.08 | valid ppl     1.19\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 110 | time:  4.00s | valid loss  0.19 | train loss  0.10 | valid ppl     1.21\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 111 | time:  3.26s | valid loss  0.24 | train loss  0.19 | valid ppl     1.27\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 112 | time:  3.27s | valid loss  0.37 | train loss  0.15 | valid ppl     1.45\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 113 | time:  3.56s | valid loss  0.30 | train loss  0.12 | valid ppl     1.36\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 114 | time:  3.30s | valid loss  0.32 | train loss  0.18 | valid ppl     1.38\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 115 | time:  3.26s | valid loss  0.24 | train loss  0.10 | valid ppl     1.27\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 116 | time:  3.56s | valid loss  0.16 | train loss  0.05 | valid ppl     1.18\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 117 | time:  3.25s | valid loss  0.29 | train loss  0.17 | valid ppl     1.34\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 118 | time:  3.22s | valid loss  0.20 | train loss  0.06 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 119 | time:  3.31s | valid loss  0.28 | train loss  0.09 | valid ppl     1.32\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 120 | time:  3.89s | valid loss  0.33 | train loss  0.12 | valid ppl     1.39\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 121 | time:  3.32s | valid loss  0.40 | train loss  0.17 | valid ppl     1.49\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 122 | time:  3.36s | valid loss  0.24 | train loss  0.24 | valid ppl     1.27\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 123 | time:  3.60s | valid loss  0.22 | train loss  0.13 | valid ppl     1.25\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 124 | time:  3.32s | valid loss  0.19 | train loss  0.07 | valid ppl     1.21\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 125 | time:  3.29s | valid loss  0.38 | train loss  0.14 | valid ppl     1.47\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 126 | time:  3.41s | valid loss  0.32 | train loss  0.13 | valid ppl     1.38\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 127 | time:  3.28s | valid loss  0.20 | train loss  0.09 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 128 | time:  3.29s | valid loss  0.22 | train loss  0.06 | valid ppl     1.24\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 129 | time:  3.81s | valid loss  0.15 | train loss  0.05 | valid ppl     1.17\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 130 | time:  3.41s | valid loss  0.16 | train loss  0.05 | valid ppl     1.18\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 131 | time:  3.32s | valid loss  0.26 | train loss  0.09 | valid ppl     1.30\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 132 | time:  3.43s | valid loss  0.33 | train loss  0.12 | valid ppl     1.40\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 133 | time:  3.45s | valid loss  0.25 | train loss  0.08 | valid ppl     1.28\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 134 | time:  3.31s | valid loss  0.29 | train loss  0.08 | valid ppl     1.33\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 135 | time:  3.26s | valid loss  0.22 | train loss  0.06 | valid ppl     1.24\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 136 | time:  3.36s | valid loss  0.19 | train loss  0.08 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 137 | time:  3.25s | valid loss  0.30 | train loss  0.17 | valid ppl     1.35\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 138 | time:  3.26s | valid loss  0.17 | train loss  0.07 | valid ppl     1.19\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 139 | time:  3.91s | valid loss  0.18 | train loss  0.06 | valid ppl     1.19\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 140 | time:  3.22s | valid loss  0.24 | train loss  0.07 | valid ppl     1.27\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 141 | time:  3.22s | valid loss  0.26 | train loss  0.07 | valid ppl     1.30\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 142 | time:  3.50s | valid loss  0.17 | train loss  0.04 | valid ppl     1.19\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 143 | time:  3.26s | valid loss  0.21 | train loss  0.08 | valid ppl     1.24\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 144 | time:  3.24s | valid loss  0.21 | train loss  0.07 | valid ppl     1.23\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 145 | time:  3.16s | valid loss  0.30 | train loss  0.11 | valid ppl     1.34\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 146 | time:  3.41s | valid loss  0.32 | train loss  0.13 | valid ppl     1.37\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 147 | time:  3.25s | valid loss  0.15 | train loss  0.03 | valid ppl     1.17\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 148 | time:  3.25s | valid loss  0.20 | train loss  0.06 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 149 | time:  3.85s | valid loss  0.18 | train loss  0.05 | valid ppl     1.20\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 150 | time:  3.22s | valid loss  0.20 | train loss  0.05 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 151 | time:  3.24s | valid loss  0.31 | train loss  0.16 | valid ppl     1.36\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 152 | time:  3.57s | valid loss  0.22 | train loss  0.12 | valid ppl     1.25\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 153 | time:  3.29s | valid loss  0.16 | train loss  0.07 | valid ppl     1.17\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 154 | time:  3.31s | valid loss  0.18 | train loss  0.04 | valid ppl     1.20\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 155 | time:  3.41s | valid loss  0.25 | train loss  0.11 | valid ppl     1.28\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 156 | time:  3.28s | valid loss  0.21 | train loss  0.05 | valid ppl     1.24\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 157 | time:  3.29s | valid loss  0.15 | train loss  0.04 | valid ppl     1.17\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 158 | time:  3.31s | valid loss  0.25 | train loss  0.07 | valid ppl     1.28\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 159 | time:  3.96s | valid loss  0.26 | train loss  0.08 | valid ppl     1.29\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 160 | time:  3.18s | valid loss  0.20 | train loss  0.07 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 161 | time:  3.20s | valid loss  0.19 | train loss  0.08 | valid ppl     1.21\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 162 | time:  3.59s | valid loss  0.26 | train loss  0.11 | valid ppl     1.30\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 163 | time:  3.30s | valid loss  0.43 | train loss  0.21 | valid ppl     1.54\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 164 | time:  3.31s | valid loss  0.20 | train loss  0.12 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 165 | time:  3.52s | valid loss  0.20 | train loss  0.08 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 166 | time:  3.26s | valid loss  0.29 | train loss  0.07 | valid ppl     1.34\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 167 | time:  3.18s | valid loss  0.21 | train loss  0.03 | valid ppl     1.23\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 168 | time:  3.42s | valid loss  0.23 | train loss  0.10 | valid ppl     1.26\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 169 | time:  3.25s | valid loss  0.17 | train loss  0.03 | valid ppl     1.18\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 170 | time:  3.66s | valid loss  0.16 | train loss  0.03 | valid ppl     1.17\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 171 | time:  3.24s | valid loss  0.33 | train loss  0.12 | valid ppl     1.39\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 172 | time:  3.52s | valid loss  0.29 | train loss  0.09 | valid ppl     1.34\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 173 | time:  3.14s | valid loss  0.25 | train loss  0.04 | valid ppl     1.28\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 174 | time:  3.22s | valid loss  0.16 | train loss  0.01 | valid ppl     1.17\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 175 | time:  3.43s | valid loss  0.30 | train loss  0.10 | valid ppl     1.35\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 176 | time:  3.26s | valid loss  0.28 | train loss  0.06 | valid ppl     1.32\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 177 | time:  3.19s | valid loss  0.21 | train loss  0.04 | valid ppl     1.23\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 178 | time:  3.42s | valid loss  0.16 | train loss  0.03 | valid ppl     1.18\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 179 | time:  3.24s | valid loss  0.25 | train loss  0.06 | valid ppl     1.28\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 180 | time:  3.29s | valid loss  0.17 | train loss  0.03 | valid ppl     1.18\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 181 | time:  3.55s | valid loss  0.22 | train loss  0.06 | valid ppl     1.24\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 182 | time:  3.53s | valid loss  0.24 | train loss  0.07 | valid ppl     1.27\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 183 | time:  3.26s | valid loss  0.18 | train loss  0.04 | valid ppl     1.20\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 184 | time:  3.29s | valid loss  0.28 | train loss  0.15 | valid ppl     1.33\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 185 | time:  3.51s | valid loss  0.15 | train loss  0.03 | valid ppl     1.16\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 186 | time:  3.29s | valid loss  0.23 | train loss  0.08 | valid ppl     1.25\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 187 | time:  3.24s | valid loss  0.17 | train loss  0.03 | valid ppl     1.19\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 188 | time:  3.46s | valid loss  0.18 | train loss  0.03 | valid ppl     1.20\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 189 | time:  3.19s | valid loss  0.18 | train loss  0.09 | valid ppl     1.20\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 190 | time:  3.18s | valid loss  0.44 | train loss  0.16 | valid ppl     1.55\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 191 | time:  3.20s | valid loss  0.25 | train loss  0.08 | valid ppl     1.28\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 192 | time:  3.35s | valid loss  0.24 | train loss  0.05 | valid ppl     1.27\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 193 | time:  3.60s | valid loss  0.20 | train loss  0.06 | valid ppl     1.22\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 194 | time:  3.21s | valid loss  0.14 | train loss  0.02 | valid ppl     1.15\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 195 | time:  3.48s | valid loss  0.21 | train loss  0.06 | valid ppl     1.24\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 196 | time:  3.23s | valid loss  0.21 | train loss  0.06 | valid ppl     1.23\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 197 | time:  3.14s | valid loss  0.21 | train loss  0.09 | valid ppl     1.23\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 198 | time:  3.39s | valid loss  0.25 | train loss  0.07 | valid ppl     1.28\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 199 | time:  3.27s | valid loss  0.28 | train loss  0.13 | valid ppl     1.33\n-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n| end of epoch 200 | time:  3.29s | valid loss  0.37 | train loss  0.24 | valid ppl     1.44\n-----------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"n_generate = 1000\ntemperature = 1\nsequence = []\nlog_interval = 100 # interval between logs","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:36:20.677408Z","iopub.execute_input":"2021-09-07T13:36:20.677744Z","iopub.status.idle":"2021-09-07T13:36:20.681320Z","shell.execute_reply.started":"2021-09-07T13:36:20.677711Z","shell.execute_reply":"2021-09-07T13:36:20.680464Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\ninp = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\nsrc_mask = generate_square_subsequent_mask(len(inp)).to(device)\nwith open('./output', 'w') as outf:\n    with torch.no_grad():  # no tracking history\n        for i in range(n_generate):\n            src_mask = generate_square_subsequent_mask(len(inp)).to(device)\n            output = model(inp, src_mask)\n            word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n            word_idx = torch.multinomial(word_weights, 1)[0]\n            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n            inp = torch.cat([inp, word_tensor], 0)\n\n\n            word = corpus.dictionary.idx2word[word_idx]\n\n            outf.write(str(word) + ('\\n' if i % 20 == 19 else ' '))\n            \n            sequence.append(word)\n\n            if i % log_interval == 0:\n                print('| Generated {}/{} notes'.format(i, n_generate))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:36:21.717197Z","iopub.execute_input":"2021-09-07T13:36:21.717519Z","iopub.status.idle":"2021-09-07T13:37:06.524068Z","shell.execute_reply.started":"2021-09-07T13:36:21.717488Z","shell.execute_reply":"2021-09-07T13:37:06.523194Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"| Generated 0/1000 notes\n| Generated 100/1000 notes\n| Generated 200/1000 notes\n| Generated 300/1000 notes\n| Generated 400/1000 notes\n| Generated 500/1000 notes\n| Generated 600/1000 notes\n| Generated 700/1000 notes\n| Generated 800/1000 notes\n| Generated 900/1000 notes\n","output_type":"stream"}]},{"cell_type":"code","source":"sequence","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:38:09.169359Z","iopub.execute_input":"2021-09-07T13:38:09.169708Z","iopub.status.idle":"2021-09-07T13:38:09.192356Z","shell.execute_reply.started":"2021-09-07T13:38:09.169676Z","shell.execute_reply":"2021-09-07T13:38:09.191385Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"[113,\n 124,\n 46,\n 113,\n 131,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 98,\n 124,\n 46,\n 98,\n 124,\n 46,\n 98,\n 124,\n 46,\n 98,\n 124,\n 46,\n 98,\n 124,\n 46,\n 98,\n 124,\n 46,\n 98,\n 124,\n 46,\n 98,\n 124,\n 46,\n 98,\n 124,\n 46,\n 98,\n 124,\n 46,\n 113,\n 124,\n 46,\n 98,\n 124,\n 46,\n 113,\n 124,\n 46,\n 98,\n 124,\n 46,\n 113,\n 124,\n 46,\n 98,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 98,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 98,\n 124,\n 46,\n 113,\n 125,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 98,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 125,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 41,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 125,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 98,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 125,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 125,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 125,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 125,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 125,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 125,\n 46,\n 113,\n 124,\n 46,\n 113,\n 125,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 203,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 45,\n 113,\n 125,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 41,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203,\n 46,\n 113,\n 124,\n 203]"},"metadata":{}}]},{"cell_type":"code","source":"sequence = [sequence]","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:38:14.320608Z","iopub.execute_input":"2021-09-07T13:38:14.320954Z","iopub.status.idle":"2021-09-07T13:38:14.326594Z","shell.execute_reply.started":"2021-09-07T13:38:14.320925Z","shell.execute_reply":"2021-09-07T13:38:14.325691Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"converted_back_midi = remi_enc.tokens_to_midi(sequence, get_midi_programs(midi))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:38:15.967413Z","iopub.execute_input":"2021-09-07T13:38:15.967749Z","iopub.status.idle":"2021-09-07T13:38:15.976727Z","shell.execute_reply.started":"2021-09-07T13:38:15.967717Z","shell.execute_reply":"2021-09-07T13:38:15.975805Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"converted_back_midi.dump('maestro_58.midi')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:38:17.218733Z","iopub.execute_input":"2021-09-07T13:38:17.219065Z","iopub.status.idle":"2021-09-07T13:38:17.261658Z","shell.execute_reply.started":"2021-09-07T13:38:17.219036Z","shell.execute_reply":"2021-09-07T13:38:17.259700Z"},"trusted":true},"execution_count":61,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-c301a761536e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconverted_back_midi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maestro_58.midi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/miditoolkit/midi/parser.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, filename, file, segment, shift, instrument_idx)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# Write it out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mmidi_parsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mmidi_parsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/mido/midifiles/midifiles.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, file)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'requires filename or file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/mido/midifiles/midifiles.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self, outfile)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrack\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m                 \u001b[0mwrite_track\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_tracks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/mido/midifiles/midifiles.py\u001b[0m in \u001b[0;36mwrite_track\u001b[0;34m(outfile, track)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'message time must be int in MIDI file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'message time must be non-negative in MIDI file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_realtime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: message time must be non-negative in MIDI file"],"ename":"ValueError","evalue":"message time must be non-negative in MIDI file","output_type":"error"}]},{"cell_type":"code","source":"from miditok import REMIEncoding, get_midi_programs\nfrom miditoolkit import MidiFile\n\n# Our parameters\npitch_range = range(21, 109)\nbeat_res = {(0, 4): 8, (4, 12): 4}\nnb_velocities = 32\nadditional_tokens = {'Chord': True,\n                     'Empty': True,\n                     'Tempo': True,\n                     'nb_tempos': 32,  # nb of tempo bins\n                     'tempo_range': (40, 250)}  # (min_tempo, max_tempo)\n\n# Creates the tokenizer and loads a MIDI\nremi_enc = REMIEncoding(pitch_range, beat_res, nb_velocities, additional_tokens)\nmidi = MidiFile('../input/blues-genre-midi-melodies/blues/035d2131e824eb51878007013786806a.mid')\n\n# Converts MIDI to tokens, and back to a MIDI\ntokens = remi_enc.midi_to_tokens(midi)\nconverted_back_midi = remi_enc.tokens_to_midi(tokens, get_midi_programs(midi))\n\n# Converts just a selected track\nremi_enc.current_midi_metadata = {'time_division': midi.ticks_per_beat, 'tempo_changes': midi.tempo_changes}\npiano_tokens = remi_enc.track_to_tokens(midi.instruments[0])\n\n# And convert it back (the last arg stands for (program number, is drum))\nconverted_back_track, tempo_changes = remi_enc.tokens_to_track(piano_tokens, midi.ticks_per_beat, (0, False))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:38:31.413409Z","iopub.execute_input":"2021-09-07T13:38:31.413751Z","iopub.status.idle":"2021-09-07T13:38:31.456931Z","shell.execute_reply.started":"2021-09-07T13:38:31.413720Z","shell.execute_reply":"2021-09-07T13:38:31.456183Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"converted_back_midi.dump('blues.mid')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:38:33.383018Z","iopub.execute_input":"2021-09-07T13:38:33.383347Z","iopub.status.idle":"2021-09-07T13:38:33.390445Z","shell.execute_reply.started":"2021-09-07T13:38:33.383316Z","shell.execute_reply":"2021-09-07T13:38:33.389531Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def quantize_note_times(notes, time_division, beat_res):\n    \"\"\" Quantize the notes items start and end values.\n    It shifts the notes so they start at times that match the quantization (e.g. 16 frames per bar)\n    :param notes: notes to quantize\n    :param time_division: MIDI time division / resolution, in ticks/beat (of the MIDI being parsed)\n    :param beat_res: number of frames (time steps, or positions) per beat\n    \"\"\"\n    ticks = int(time_division / beat_res)\n    quantized_ticks = np.arange(0, max([n.end for n in notes]) + 2 * ticks, ticks, dtype=int)\n    for i, note in enumerate(notes):  # items are notes\n        note.start = quantized_ticks[np.argmin(np.abs(quantized_ticks - note.start))]\n        note.end = quantized_ticks[np.argmin(np.abs(quantized_ticks - note.end))]\n\n        if note.start == note.end:  # if this happens to often, consider using a higher beat resolution\n            note.end += ticks  # like 8 frames per beat or 24 frames per bar\n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T18:33:41.473247Z","iopub.execute_input":"2021-09-03T18:33:41.473635Z","iopub.status.idle":"2021-09-03T18:33:41.480054Z","shell.execute_reply.started":"2021-09-03T18:33:41.473589Z","shell.execute_reply":"2021-09-03T18:33:41.479233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_duplicated_notes(notes):\n    \"\"\" Remove possible duplicated notes, i.e. with the same pitch, starting and ending times.\n    Before running this function make sure the notes has been sorted by start and pitch:\n    notes.sort(key=lambda x: (x.start, x.pitch))\n    :param notes: notes to analyse\n    \"\"\"\n    for i in range(len(notes) - 1, 0, -1):  # removing possible duplicated notes\n        if notes[i].pitch == notes[i - 1].pitch and notes[i].start == notes[i - 1].start and \\\n                notes[i].end == notes[i - 1].end:\n            del notes[i]\n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T18:33:42.51638Z","iopub.execute_input":"2021-09-03T18:33:42.518703Z","iopub.status.idle":"2021-09-03T18:33:42.526916Z","shell.execute_reply.started":"2021-09-03T18:33:42.518656Z","shell.execute_reply":"2021-09-03T18:33:42.525951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = []\nfor track in midi.instruments:\n    quantize_note_times(track.notes, remi_enc.current_midi_metadata['time_division'], max(remi_enc.beat_res.values()))\n    track.notes.sort(key=lambda x: (x.start, x.pitch))  # sort notes\n    remove_duplicated_notes(track.notes)  # remove possible duplicated notes\n#     print(track.notes)\n    # Convert track to tokens\n    print(remi_enc.track_to_tokens(track))\n    tokens.append(remi_enc.track_to_tokens(track))\ntokens","metadata":{"execution":{"iopub.status.busy":"2021-09-03T18:47:05.313609Z","iopub.execute_input":"2021-09-03T18:47:05.313958Z","iopub.status.idle":"2021-09-03T18:47:05.355098Z","shell.execute_reply.started":"2021-09-03T18:47:05.313927Z","shell.execute_reply":"2021-09-03T18:47:05.354196Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"programs = get_midi_programs(midi)\ninst, temp = remi_enc.tokens_to_track(tokens[0])\nprint(inst)\nprint(temp)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T18:54:47.37914Z","iopub.execute_input":"2021-09-03T18:54:47.379535Z","iopub.status.idle":"2021-09-03T18:54:47.386202Z","shell.execute_reply.started":"2021-09-03T18:54:47.379504Z","shell.execute_reply":"2021-09-03T18:54:47.384997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"midi.dump('orig.midi')","metadata":{"execution":{"iopub.status.busy":"2021-09-02T15:59:51.434338Z","iopub.execute_input":"2021-09-02T15:59:51.434729Z","iopub.status.idle":"2021-09-02T15:59:51.445193Z","shell.execute_reply.started":"2021-09-02T15:59:51.434696Z","shell.execute_reply":"2021-09-02T15:59:51.444194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converted_back_midi.dump('blues.midi')","metadata":{"execution":{"iopub.status.busy":"2021-09-03T19:23:21.543553Z","iopub.execute_input":"2021-09-03T19:23:21.543882Z","iopub.status.idle":"2021-09-03T19:23:21.548443Z","shell.execute_reply.started":"2021-09-03T19:23:21.543852Z","shell.execute_reply":"2021-09-03T19:23:21.547402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a MIDI file from prediction\ndef create_MIDI(gen, name=\"\"):\n    \n    # the offset is the time difference between notes, we assume its 0.5 here\n    offset = 0\n    music = []\n    \n    for seq in gen:\n        # chords are seperated by .\n#         if ('.' in seq) or seq.isdigit():\n#             chordnotes = seq.split('.')\n#             n = []\n            \n#             for cur_note in chordnotes:\n#                 new_n = note.Note(int(cur_note))\n#                 new_n.storedInstrument = instrument.Piano() # single piano instrument only\n#                 n.append(new_n)\n            \n#             new_c = chord.Chord(n)\n#             new_c.offset = offset\n#             music.append(new_c)\n        \n#         else:            \n        new_n = note.Note(seq)\n#         new_n.storedInstrument = instrument.Piano() # single piano instrument only\n        new_n.storedInstrument = midi.instruments[0]\n        new_n.offset = offset\n        music.append(new_n)\n        \n        offset += 0.5\n    \n    print(notes)\n    # producing a MIDI stream\n    midi_file = stream.Stream(music)\n    \n    midi_file.write('midi', fp='new_music_'+name+'.mid')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T16:32:33.380868Z","iopub.execute_input":"2021-09-01T16:32:33.381208Z","iopub.status.idle":"2021-09-01T16:32:33.386867Z","shell.execute_reply.started":"2021-09-01T16:32:33.381176Z","shell.execute_reply":"2021-09-01T16:32:33.38605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_MIDI(sequence, name='blues_all_1.87_temphalf')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T16:32:41.432303Z","iopub.execute_input":"2021-09-01T16:32:41.43271Z","iopub.status.idle":"2021-09-01T16:32:41.884504Z","shell.execute_reply.started":"2021-09-01T16:32:41.432679Z","shell.execute_reply":"2021-09-01T16:32:41.883506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_input(filename, folder=False):\n    # master list of notes\n    notes = []\n    \n    # converting folders with multiple MIDI files\n    if folder == True:\n        assert os.path.exists('../input/classical-music-midi/'+filename)\n        for file in glob.glob('../input/classical-music-midi/'+filename+'/*.mid'):\n            notes_per_piece = []\n            # read the MIDI file\n            midi = converter.parse(file)            \n            \n            try: # file has instrument parts\n                s2 = instrument.partitionByInstrument(midi)\n                notes_to_parse = s2.parts[1].recurse() \n            except: # file has notes in a flat structure\n                notes_to_parse = midi.flat.notes\n            \n#             print(notes_to_parse)\n            for element in notes_to_parse:\n                if isinstance(element, note.Note):\n                    notes_per_piece.append(str(element.pitch))\n                elif isinstance(element, chord.Chord):\n                    notes_per_piece.append('.'.join(str(n) for n in element.normalOrder))\n            notes.append(notes_per_piece)\n    else:\n        assert os.path.exists(filename)\n        midi = converter.parse(filename)\n        try: # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[1].recurse() \n        except: # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n#         print(notes_to_parse)\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n        \n\n    with open('./notes', 'wb') as filepath:\n        pickle.dump(notes, filepath)\n#     print(notes)\n    return notes","metadata":{"execution":{"iopub.status.busy":"2021-08-29T15:39:57.254503Z","iopub.execute_input":"2021-08-29T15:39:57.254905Z","iopub.status.idle":"2021-08-29T15:39:57.268593Z","shell.execute_reply.started":"2021-08-29T15:39:57.254859Z","shell.execute_reply":"2021-08-29T15:39:57.266854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notes = []\nwith (open(\"../input/beethoven-notes/notes\", \"rb\")) as openfile:\n    while True:\n        try:\n            notes = (pickle.load(openfile))\n        except EOFError:\n            break\n# notes = ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:30:06.488151Z","iopub.execute_input":"2021-08-29T14:30:06.488517Z","iopub.status.idle":"2021-08-29T14:30:06.523884Z","shell.execute_reply.started":"2021-08-29T14:30:06.488486Z","shell.execute_reply":"2021-08-29T14:30:06.523092Z"},"trusted":true},"execution_count":null,"outputs":[]}]}