{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom io import open\nimport tensorflow as tf\nimport glob\nimport pickle\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-05T19:44:54.825918Z","iopub.execute_input":"2021-08-05T19:44:54.826289Z","iopub.status.idle":"2021-08-05T19:45:00.310149Z","shell.execute_reply.started":"2021-08-05T19:44:54.826201Z","shell.execute_reply":"2021-08-05T19:45:00.309301Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install music21","metadata":{"execution":{"iopub.status.busy":"2021-08-05T19:45:17.558147Z","iopub.execute_input":"2021-08-05T19:45:17.558514Z","iopub.status.idle":"2021-08-05T19:45:45.751827Z","shell.execute_reply.started":"2021-08-05T19:45:17.558480Z","shell.execute_reply":"2021-08-05T19:45:45.750714Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting music21\n  Downloading music21-6.7.1.tar.gz (19.2 MB)\n\u001b[K     |████████████████████████████████| 19.2 MB 910 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from music21) (4.0.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from music21) (1.0.1)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from music21) (8.8.0)\nCollecting webcolors\n  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: music21\n  Building wheel for music21 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for music21: filename=music21-6.7.1-py3-none-any.whl size=21941692 sha256=bb475f7266375c45da8a48b1265486f1fa7c69fab779bd9fa8fe398b0c528e14\n  Stored in directory: /root/.cache/pip/wheels/72/44/61/90e4e65262ca1b4d9f707527b540729ce3f64e00fc6b38d54c\nSuccessfully built music21\nInstalling collected packages: webcolors, music21\nSuccessfully installed music21-6.7.1 webcolors-1.11.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from music21 import converter, instrument, note, chord, stream","metadata":{"execution":{"iopub.status.busy":"2021-08-05T19:45:47.984773Z","iopub.execute_input":"2021-08-05T19:45:47.985119Z","iopub.status.idle":"2021-08-05T19:45:48.763422Z","shell.execute_reply.started":"2021-08-05T19:45:47.985083Z","shell.execute_reply":"2021-08-05T19:45:48.762551Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def preprocess_input(filename, folder=False):\n    # master list of notes\n    notes = []\n    \n    # converting folders with multiple MIDI files\n    if folder == True:\n        assert os.path.exists('../input/classical-music-midi/'+filename)\n        for file in glob.glob('../input/classical-music-midi/'+filename+'/*.mid'):\n            notes_per_piece = []\n            # read the MIDI file\n            midi = converter.parse(file)            \n            \n            try: # file has instrument parts\n                s2 = instrument.partitionByInstrument(midi)\n                notes_to_parse = s2.parts[1].recurse() \n            except: # file has notes in a flat structure\n                notes_to_parse = midi.flat.notes\n            \n#             print(notes_to_parse)\n            for element in notes_to_parse:\n                if isinstance(element, note.Note):\n                    notes_per_piece.append(str(element.pitch))\n                elif isinstance(element, chord.Chord):\n                    notes_per_piece.append('.'.join(str(n) for n in element.normalOrder))\n            notes.append(notes_per_piece)\n    else:\n        assert os.path.exists(filename)\n        midi = converter.parse(filename)\n        try: # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[1].recurse() \n        except: # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n#         print(notes_to_parse)\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n        \n\n    with open('./notes', 'wb') as filepath:\n        pickle.dump(notes, filepath)\n#     print(notes)\n    return notes","metadata":{"execution":{"iopub.status.busy":"2021-08-05T19:45:50.182161Z","iopub.execute_input":"2021-08-05T19:45:50.182561Z","iopub.status.idle":"2021-08-05T19:45:50.195115Z","shell.execute_reply.started":"2021-08-05T19:45:50.182527Z","shell.execute_reply":"2021-08-05T19:45:50.193997Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"notes = preprocess_input('beeth', folder=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T19:46:47.096673Z","iopub.execute_input":"2021-08-05T19:46:47.097054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(notes[:-6])\n        self.valid = self.tokenize(notes[-6:-3])\n        self.test = self.tokenize(notes[-3:])\n\n    def tokenize(self, notes):\n        \"\"\"Tokenizes a note sequence\"\"\"\n        assert len(notes) > 0\n        \n        # Add notes to the dictionary\n        for note in notes:\n            self.dictionary.add_word(note)\n#         # Add words to the dictionary\n#         with open(path, 'r', encoding=\"utf8\") as f:\n#             for line in f:\n#                 words = line.split() + ['<eos>']\n#                 for word in words:\n#                     self.dictionary.add_word(word)\n\n        # Tokenize file content\n        idss = []\n        ids = []\n        for note in notes:\n            ids.append(self.dictionary.word2idx[note])\n        idss.append(torch.tensor(ids).type(torch.int64))\n        ids = torch.cat(idss)\n            \n#         with open(path, 'r', encoding=\"utf8\") as f:\n#             idss = []\n#             for line in f:\n#                 words = line.split() + ['<eos>']\n#                 ids = []\n#                 for word in words:\n#                     ids.append(self.dictionary.word2idx[word])\n#                 idss.append(torch.tensor(ids).type(torch.int64))\n#             ids = torch.cat(idss)\n\n        return ids","metadata":{"execution":{"iopub.status.busy":"2021-08-03T19:42:40.387016Z","iopub.execute_input":"2021-08-03T19:42:40.387347Z","iopub.status.idle":"2021-08-03T19:42:40.396502Z","shell.execute_reply.started":"2021-08-03T19:42:40.387318Z","shell.execute_reply":"2021-08-03T19:42:40.395723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = Corpus('../input/classical-music-midi/bach')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T19:44:06.178581Z","iopub.execute_input":"2021-08-03T19:44:06.178912Z","iopub.status.idle":"2021-08-03T19:44:08.698348Z","shell.execute_reply.started":"2021-08-03T19:44:06.178884Z","shell.execute_reply":"2021-08-03T19:44:08.697555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\neval_batch_size = 10\ntrain_data = batchify(corpus.train, eval_batch_size)\nval_data = batchify(corpus.valid, eval_batch_size)\ntest_data = batchify(corpus.test, eval_batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T19:45:38.034301Z","iopub.execute_input":"2021-08-03T19:45:38.034657Z","iopub.status.idle":"2021-08-03T19:45:43.945618Z","shell.execute_reply.started":"2021-08-03T19:45:38.034625Z","shell.execute_reply":"2021-08-03T19:45:43.944785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    r\"\"\"Inject some information about the relative or absolute position of the tokens\n        in the sequence. The positional encodings have the same dimension as\n        the embeddings, so that the two can be summed. Here, we use sine and cosine\n        functions of different frequencies.\n    .. math::\n        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    Examples:\n        >>> pos_encoder = PositionalEncoding(d_model)\n    \"\"\"\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        r\"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        Examples:\n            >>> output = pos_encoder(x)\n        \"\"\"\n\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T19:54:08.834937Z","iopub.execute_input":"2021-08-03T19:54:08.835307Z","iopub.status.idle":"2021-08-03T19:54:08.84513Z","shell.execute_reply.started":"2021-08-03T19:54:08.835275Z","shell.execute_reply":"2021-08-03T19:54:08.844196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n        super(TransformerModel, self).__init__()\n        try:\n            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n        except:\n            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n        self.model_type = 'Transformer'\n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.ninp = ninp\n        self.decoder = nn.Linear(ninp, ntoken)\n\n        self.init_weights()\n\n    def _generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n        nn.init.zeros_(self.decoder.weight)\n        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n\n    def forward(self, src, has_mask=True):\n        if has_mask:\n            device = src.device\n            if self.src_mask is None or self.src_mask.size(0) != len(src):\n                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n                self.src_mask = mask\n        else:\n            self.src_mask = None\n\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, self.src_mask)\n        output = self.decoder(output)\n        return F.log_softmax(output, dim=-1)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T19:54:59.287396Z","iopub.execute_input":"2021-08-03T19:54:59.287756Z","iopub.status.idle":"2021-08-03T19:54:59.299194Z","shell.execute_reply.started":"2021-08-03T19:54:59.287727Z","shell.execute_reply":"2021-08-03T19:54:59.298294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T20:14:09.298528Z","iopub.execute_input":"2021-08-03T20:14:09.298867Z","iopub.status.idle":"2021-08-03T20:14:09.318811Z","shell.execute_reply.started":"2021-08-03T20:14:09.298839Z","shell.execute_reply":"2021-08-03T20:14:09.31797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.NLLLoss()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T20:14:09.791383Z","iopub.execute_input":"2021-08-03T20:14:09.791734Z","iopub.status.idle":"2021-08-03T20:14:09.797415Z","shell.execute_reply.started":"2021-08-03T20:14:09.791706Z","shell.execute_reply":"2021-08-03T20:14:09.796638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def repackage_hidden(h):\n    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T20:14:10.580767Z","iopub.execute_input":"2021-08-03T20:14:10.58108Z","iopub.status.idle":"2021-08-03T20:14:10.587045Z","shell.execute_reply.started":"2021-08-03T20:14:10.581052Z","shell.execute_reply":"2021-08-03T20:14:10.585807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_batch subdivides the source data into chunks of length args.bptt.\n# If source is equal to the example output of the batchify function, with\n# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n# ┌ a g m s ┐ ┌ b h n t ┐\n# └ b h n t ┘ └ c i o u ┘\n# Note that despite the name of the function, the subdivison of data is not\n# done along the batch dimension (i.e. dimension 1), since that was handled\n# by the batchify function. The chunks are along dimension 0, corresponding\n# to the seq_len dimension in the LSTM.\nseq_length = 35\ndef get_batch(source, i):\n    seq_len = min(seq_length, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    return data, target","metadata":{"execution":{"iopub.status.busy":"2021-08-03T20:14:11.246657Z","iopub.execute_input":"2021-08-03T20:14:11.246969Z","iopub.status.idle":"2021-08-03T20:14:11.251751Z","shell.execute_reply.started":"2021-08-03T20:14:11.246942Z","shell.execute_reply":"2021-08-03T20:14:11.250957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(data_source):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    total_loss = 0.\n    ntokens = len(corpus.dictionary)\n\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, seq_length):\n            data, targets = get_batch(data_source, i)\n            \n            output = model(data)\n            output = output.view(-1, ntokens)\n            \n            total_loss += len(data) * criterion(output, targets).item()\n    return total_loss / (len(data_source) - 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T20:14:11.889655Z","iopub.execute_input":"2021-08-03T20:14:11.889999Z","iopub.status.idle":"2021-08-03T20:14:11.898989Z","shell.execute_reply.started":"2021-08-03T20:14:11.88997Z","shell.execute_reply":"2021-08-03T20:14:11.89739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    # Turn on training mode which enables dropout.\n    model.train()\n    total_loss = 0.\n    start_time = time.time()\n    ntokens = len(corpus.dictionary)\n    \n    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_length)):\n        data, targets = get_batch(train_data, i)\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n        model.zero_grad()\n        \n        output = model(data)\n        output = output.view(-1, ntokens)\n     \n        loss = criterion(output, targets)\n        loss.backward()\n        \n        clip = 0.25\n        log_interval = 200\n        \n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        for p in model.parameters():\n            p.data.add_(p.grad, alpha=-lr)\n\n        total_loss += loss.item()\n\n        if batch % log_interval == 0 and batch > 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n                    'loss {:5.2f} | ppl {:8.2f}'.format(\n                epoch, batch, len(train_data) // seq_length, lr,\n                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n#         if dry_run:\n#             break","metadata":{"execution":{"iopub.status.busy":"2021-08-03T20:14:12.473678Z","iopub.execute_input":"2021-08-03T20:14:12.474002Z","iopub.status.idle":"2021-08-03T20:14:12.48222Z","shell.execute_reply.started":"2021-08-03T20:14:12.473972Z","shell.execute_reply":"2021-08-03T20:14:12.481411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ntokens = len(corpus.dictionary)\nemsize = 200\nnhead = 2\nnhid = 200\nnlayers = 2\ndropout = 0.2\n# Loop over epochs.\nlr = 6\nbest_val_loss = None\nepochs = 100\nsave = './model.pt'","metadata":{"execution":{"iopub.status.busy":"2021-08-03T20:15:24.859172Z","iopub.execute_input":"2021-08-03T20:15:24.859533Z","iopub.status.idle":"2021-08-03T20:15:24.864656Z","shell.execute_reply.started":"2021-08-03T20:15:24.859495Z","shell.execute_reply":"2021-08-03T20:15:24.863569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    for epoch in range(1, epochs+1):\n        epoch_start_time = time.time()\n        train()\n        val_loss = evaluate(val_data)\n        print('-' * 89)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                           val_loss, math.exp(val_loss)))\n        print('-' * 89)\n        # Save the model if the validation loss is the best we've seen so far.\n        if not best_val_loss or val_loss < best_val_loss:\n            with open(save, 'wb') as f:\n                torch.save(model, f)\n            best_val_loss = val_loss\n        else:\n            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n            lr /= 4.0\nexcept KeyboardInterrupt:\n    print('-' * 89)\n    print('Exiting from training early')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T20:15:26.148945Z","iopub.execute_input":"2021-08-03T20:15:26.149284Z","iopub.status.idle":"2021-08-03T20:15:31.357581Z","shell.execute_reply.started":"2021-08-03T20:15:26.149253Z","shell.execute_reply":"2021-08-03T20:15:31.356589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This recognises the events in a MIDI track\n# The type of events inclde: playing a note, stopping a note, or another system executive instruction\nclass MIDIEvent:\n\n    # defining an enumeration on the type of MIDI events\n    class Type(Enum):\n        noteOFF = 0\n        noteON = 1\n        other = 2\n\n    # A MIDI event has the following features\n    # Type = The type of event (from the above enumeration)\n    # Key = The note being played\n    # velocity = the speed of the note in the track\n    # deltaTick = the time difference between this and the previous event\n    def __init__(self, note, noteID=0, vel=0, delta=0):\n        self.type = note\n        self.key = noteID\n        self.velocity = vel\n        self.deltaTick = delta\n\n    def __repr__(self):\n        return (\"\\nEvent Type: \" + str(self.type) + \" Key: \" + str(self.key) +\n                \" Velocity: \" + str(self.velocity) + \" delta tick: \" +\n                str(self.deltaTick))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 hid_dim, \n                 n_layers, \n                 n_heads, \n                 pf_dim,\n                 dropout, \n                 device,\n                 max_length = 1000):\n        super().__init__()\n\n        self.device = device\n        \n        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n        \n        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n                                                  n_heads, \n                                                  pf_dim,\n                                                  dropout, \n                                                  device) \n                                     for _ in range(n_layers)])\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n        \n    def forward(self, src, src_mask):\n        \n        #src = [batch size, src len]\n        #src_mask = [batch size, 1, 1, src len]\n        \n        batch_size = src.shape[0]\n        src_len = src.shape[1]\n\n        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n        \n        #pos = [batch size, src len]\n        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n        \n        #src = [batch size, src len, hid dim]\n        \n        for layer in self.layers:\n            src = layer(src, src_mask)\n            \n        #src = [batch size, src len, hid dim]\n            \n        return src","metadata":{},"execution_count":null,"outputs":[]}]}