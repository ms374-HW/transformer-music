{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install music21\n!pip install miditoolkit \n!pip install miditok","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:21:59.001122Z","iopub.execute_input":"2021-08-31T15:21:59.001517Z","iopub.status.idle":"2021-08-31T15:22:22.850972Z","shell.execute_reply.started":"2021-08-31T15:21:59.001464Z","shell.execute_reply":"2021-08-31T15:22:22.85005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom io import open\nimport tensorflow as tf\nimport glob\nimport pickle\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport time\nimport copy\nfrom pathlib import Path, PurePath, PurePosixPath\nfrom music21 import converter, instrument, note, chord, stream\nfrom miditok import CPWordEncoding, REMIEncoding\nfrom miditoolkit import MidiFile","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-31T15:23:01.302593Z","iopub.execute_input":"2021-08-31T15:23:01.302973Z","iopub.status.idle":"2021-08-31T15:23:06.654903Z","shell.execute_reply.started":"2021-08-31T15:23:01.302938Z","shell.execute_reply":"2021-08-31T15:23:06.654055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creates the tokenizer and list the file paths\nremi_enc = REMIEncoding()  # uses defaults parameters\n# filename = 'chopin'\nfiles_paths = list(glob.glob('../input/blues-genre-midi-melodies/blues/*.mid'))","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:23:06.656242Z","iopub.execute_input":"2021-08-31T15:23:06.656581Z","iopub.status.idle":"2021-08-31T15:23:06.748403Z","shell.execute_reply.started":"2021-08-31T15:23:06.656524Z","shell.execute_reply":"2021-08-31T15:23:06.747679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notes = []\n\nremi_enc = REMIEncoding()\n\nfor file in files_paths:\n\n    # read the MIDI file\n    midi = MidiFile(file)\n\n    # Converts MIDI to tokens\n    tokens = remi_enc.midi_to_tokens(midi)\n    \n    notes.append(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:23:06.749981Z","iopub.execute_input":"2021-08-31T15:23:06.75033Z","iopub.status.idle":"2021-08-31T15:23:15.889114Z","shell.execute_reply.started":"2021-08-31T15:23:06.750294Z","shell.execute_reply":"2021-08-31T15:23:15.888238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all the beethoven files have 2 tracks (instruments) - Piano left and right\n# for now, we will only be using for piano right since it determines the melody\nmidi.instruments","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:23:55.255098Z","iopub.execute_input":"2021-08-31T15:23:55.25546Z","iopub.status.idle":"2021-08-31T15:23:55.263964Z","shell.execute_reply.started":"2021-08-31T15:23:55.255427Z","shell.execute_reply":"2021-08-31T15:23:55.262944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# piano_right_notes = [note[0] for note in notes]\n# piano_right_notes = [np.asarray(note) for note in piano_right_notes]\n# piano_right_notes = np.array(piano_right_notes, dtype='object')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:23:56.25163Z","iopub.execute_input":"2021-08-31T15:23:56.251962Z","iopub.status.idle":"2021-08-31T15:23:56.256015Z","shell.execute_reply.started":"2021-08-31T15:23:56.251933Z","shell.execute_reply":"2021-08-31T15:23:56.254518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(notes[0][0])","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:23:57.486099Z","iopub.execute_input":"2021-08-31T15:23:57.486426Z","iopub.status.idle":"2021-08-31T15:23:57.49195Z","shell.execute_reply.started":"2021-08-31T15:23:57.486396Z","shell.execute_reply":"2021-08-31T15:23:57.491096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notes = [note[0] for note in notes]","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:23:58.578568Z","iopub.execute_input":"2021-08-31T15:23:58.578887Z","iopub.status.idle":"2021-08-31T15:23:58.58316Z","shell.execute_reply.started":"2021-08-31T15:23:58.578858Z","shell.execute_reply":"2021-08-31T15:23:58.581993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(notes[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:23:59.638285Z","iopub.execute_input":"2021-08-31T15:23:59.638619Z","iopub.status.idle":"2021-08-31T15:23:59.644551Z","shell.execute_reply.started":"2021-08-31T15:23:59.638587Z","shell.execute_reply":"2021-08-31T15:23:59.643607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n    \n    def words(self):\n        return self.idx2word\n\n\nclass Corpus(object):\n    def __init__(self, notes):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(notes[:-22])\n        self.valid = self.tokenize(notes[-22:-11])\n        self.test = self.tokenize(notes[-11:])\n     \n    def tokenize(self, notes):\n        \"\"\"Tokenizes a note sequence\"\"\"\n        assert len(notes) > 0\n        \n        for song in notes:\n            for note in song:\n                self.dictionary.add_word(note)\n        idss = []\n        \n        for song in notes:\n            ids = []\n            for note in song:\n                ids.append(self.dictionary.word2idx[note])\n        idss.append(torch.tensor(ids).type(torch.int64))\n        ids = torch.cat(idss)\n            \n\n        return ids","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:24:01.223877Z","iopub.execute_input":"2021-08-31T15:24:01.224224Z","iopub.status.idle":"2021-08-31T15:24:01.234276Z","shell.execute_reply.started":"2021-08-31T15:24:01.224193Z","shell.execute_reply":"2021-08-31T15:24:01.233253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = Corpus(notes)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:24:12.599784Z","iopub.execute_input":"2021-08-31T15:24:12.600101Z","iopub.status.idle":"2021-08-31T15:24:12.697878Z","shell.execute_reply.started":"2021-08-31T15:24:12.600072Z","shell.execute_reply":"2021-08-31T15:24:12.697058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus.train.size(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:24:13.811365Z","iopub.execute_input":"2021-08-31T15:24:13.811783Z","iopub.status.idle":"2021-08-31T15:24:13.817258Z","shell.execute_reply.started":"2021-08-31T15:24:13.811734Z","shell.execute_reply":"2021-08-31T15:24:13.816341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cpu'\ndef batchify(data, bsz):\n    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n    that wouldn't cleanly fit.\n\n    Args:\n        data: Tensor, shape [N]\n        bsz: int, batch size\n\n    Returns:\n        Tensor of shape [N // bsz, bsz]\n    \"\"\"\n    seq_len = data.size(0) // bsz\n    data = data[:seq_len * bsz]\n    data = data.view(bsz, seq_len).t().contiguous()\n    return data.to(device)\n\neval_batch_size = 10\nbatch_size = 20\ntrain_data = batchify(corpus.train, batch_size)\nval_data = batchify(corpus.valid, batch_size)\ntest_data = batchify(corpus.test, batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:24:48.754525Z","iopub.execute_input":"2021-08-31T15:24:48.754896Z","iopub.status.idle":"2021-08-31T15:24:48.761629Z","shell.execute_reply.started":"2021-08-31T15:24:48.754865Z","shell.execute_reply":"2021-08-31T15:24:48.760619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:24:55.068417Z","iopub.execute_input":"2021-08-31T15:24:55.06878Z","iopub.status.idle":"2021-08-31T15:24:55.074752Z","shell.execute_reply.started":"2021-08-31T15:24:55.068747Z","shell.execute_reply":"2021-08-31T15:24:55.073638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"\"\"Inject some information about the relative or absolute position of the tokens\n        in the sequence. The positional encodings have the same dimension as\n        the embeddings, so that the two can be summed. Here, we use sine and cosine\n        functions of different frequencies.\n    .. math::\n        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    Examples:\n        >>> pos_encoder = PositionalEncoding(d_model)\n    \"\"\"\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.d_model = d_model\n\n        # PE is the Positional Encoding matrix \n        # THIS STORES THE POSITIONS OF THE SEQUENCE\n        pe = torch.zeros(max_len, d_model)\n        \n        # Arange - RETURNS A RANGE BETWEEN VALUES, HERE IT IS 0 - max_len\n        # unsqueeze - adds a dimension, 1 means that each element in the first list is now in a list\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        \n        # division term, here it is (10000 ** ((2 * i)/d_model))\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        # calculating the position encoding for the even and odd terms\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        # Unsqueeze 0 will put PE in one list\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        Examples:\n            >>> output = pos_encoder(x)\n        \"\"\"\n        # make embeddings relatively larger\n        # This is so we do not lose the importance of the embedding\n        x = x * math.sqrt(self.d_model)\n        # we add the embedding to the PE \n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:28.330626Z","iopub.execute_input":"2021-08-29T17:11:28.330944Z","iopub.status.idle":"2021-08-29T17:11:28.340154Z","shell.execute_reply.started":"2021-08-29T17:11:28.330916Z","shell.execute_reply":"2021-08-29T17:11:28.339348Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        \n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n        self.d_model = d_model\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        # make embeddings relatively larger\n        # This is so we do not lose the importance of the embedding\n        x = x * math.sqrt(self.d_model)\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:25:00.180133Z","iopub.execute_input":"2021-08-31T15:25:00.180603Z","iopub.status.idle":"2021-08-31T15:25:00.193698Z","shell.execute_reply.started":"2021-08-31T15:25:00.180561Z","shell.execute_reply":"2021-08-31T15:25:00.192841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\"Parallel self attention given the number of heads and size\n    .. math::\n        X * Wk = K\n        X * Wq = Q\n        X * Wv = V\n        \\text{attention} = softmax(Q * transpose(K) / sqrt(d_model)) * V\n    Args:\n        d_model: the embed dim (default = 256).\n        heads: number of heads (default = 4)\n        max_length = max length of sequences (default = 2048)\n        dropout: the dropout value (default = 0.1).\n    Examples:\n        >>> attention = MultiHeadAttention(d_model, heads)\n    \"\"\"\n    def __init__(self, d_model = 256, heads = 4, max_length = 2048, dropout = 0.1):\n        super().__init__()\n        \n        self.d = d_model\n        self.h = heads\n        self.dh = d_model // heads\n        self.max_length = max_length\n        self.E = torch.randn([heads, self.max_length, self.dh])\n        \n        self.q_linear = nn.Linear(self.dh, self.dh)\n        self.v_linear = nn.Linear(self.dh, self.dh)\n        self.k_linear = nn.Linear(self.dh, self.dh)\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(d_model, d_model)\n    \n    def forward(self, q, k, v, mask=None):\n        \n        batch_size = q.size(0)\n        T = q.size(1)\n        dh = q.size(2) // self.h\n        \n        k = k.view(batch_size, T, self.h, dh)\n        q = q.view(batch_size, T, self.h, dh)\n        v = v.view(batch_size, T, self.h, dh)\n        # perform linear operation and split into h heads\n        \n        K = self.k_linear(k)\n        Q = self.q_linear(q)\n        V = self.v_linear(v)\n        \n        # transpose to get dimensions bs * h * sl * d_model\n       \n        K = K.transpose(1,2)\n        Q = Q.transpose(1,2)\n        V = V.transpose(1,2)\n\n        #start index of position embedding\n        \n        embedding_start = self.max_length - T\n        \n        #apply same position embeddings across the batch\n        \n        Er  = self.E[:, embedding_start:, :].unsqueeze(0)\n        \n        QE = torch.matmul(Q, K.transpose(-2, -1))\n        QE = self.mask_attention_positions(QE)\n        \n        #Get relative position attention scores\n        #combine batch with head dimension\n        SRel = self.skew_padding_position(QE)\n\n#         Q = Q.contiguous().view(batch_size*self.h, T, self.dh)\n#         K = K.contiguous().view(batch_size*self.h, T, self.dh)\n#         V = V.contiguous().view(batch_size*self.h, T, self.dh)\n        \n        #Compute scaled dot-product self-attention\n        #scale pre-matrix multiplication   \n        Q = Q / (q.size(2) ** (1/4))\n        K = K / (q.size(2) ** (1/4))\n\n        # calculate attention \n        scores, weights = self.attention(QE, SRel, V, mask, self.dropout)\n        \n        # concatenate heads and put through final linear layer\n        concat = scores.transpose(1,2).contiguous().view(batch_size, -1, self.d)\n        \n        output = self.out(concat)\n    \n        return output\n\n    def attention(self, QE, Srel, V, mask=None, dropout=None):\n        log = QE + Srel\n        log = log / math.sqrt(self.dh)\n\n        if mask is not None:\n#             mask = mask.unsqueeze(1)\n            log += (mask.to(torch.int64) * -1e9).to(log.dtype)\n\n#             print(mask.shape)\n#             log = log.masked_fill(mask == 0, float('-inf'))\n\n        scores = F.softmax(log, -1)\n        \n        if dropout is not None:\n            scores = dropout(scores)\n            \n#         print(scores.shape, V.shape)\n        attention = torch.matmul(scores, V)\n        \n#         scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n\n#         if mask is not None:\n#             print(scores.shape)\n\n#     #         mask = mask.unsqueeze(1)\n#             print(mask.shape)\n#     #         scores = scores.masked_fill(mask == 0, float('-inf'))\n#         scores = F.softmax(scores, dim=-1)\n\n#         \n\n#         output = torch.matmul(scores, v)\n        return attention, scores\n    \n    def mask_attention_positions(self, qe):\n        # to avoid looking backward by masking the positions\n        index = qe.shape[-1]\n        mask = torch.triu(torch.ones(index, index), 1).flip(1)\n        return qe.masked_fill((mask == 1), 0)\n    \n    def skew_padding_position(self, qe):\n        # to add padding to the skewed result after masking the matrix\n        # column of zeros on left\n        padded_qe = F.pad(qe, [1,0])\n        s = padded_qe.shape\n        padded_qe = padded_qe.view(s[0], s[1], s[3], s[2])\n        #take out first (padded) row\n        return padded_qe[:,:,1:,:]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:29.221271Z","iopub.execute_input":"2021-08-29T17:11:29.221605Z","iopub.status.idle":"2021-08-29T17:11:29.240108Z","shell.execute_reply.started":"2021-08-29T17:11:29.221574Z","shell.execute_reply":"2021-08-29T17:11:29.239141Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n#     def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n#         super().__init__() \n#         # We set d_ff as a default to 2048\n#         self.linear_1 = nn.Linear(d_model, d_ff)\n#         self.dropout = nn.Dropout(dropout)\n#         self.linear_2 = nn.Linear(d_ff, d_model)\n#     def forward(self, x):\n#         x = self.dropout(F.relu(self.linear_1(x)))\n#         x = self.linear_2(x)\n#         return x\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear_2(self.dropout(F.relu(self.linear_1(x))))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:29.51977Z","iopub.execute_input":"2021-08-29T17:11:29.520083Z","iopub.status.idle":"2021-08-29T17:11:29.525789Z","shell.execute_reply.started":"2021-08-29T17:11:29.520034Z","shell.execute_reply":"2021-08-29T17:11:29.524865Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build an encoder layer with one multi-head attention layer and one # feed-forward layer\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout = 0.1):\n        super().__init__()\n        self.norm_1 = nn.LayerNorm(d_model)\n        self.norm_2 = nn.LayerNorm(d_model)\n        \n        self.attn = MultiHeadAttention(d_model, heads)\n        self.ff = FeedForward(d_model)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        x2 = self.norm_1(x)\n        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n        x2 = self.norm_2(x)\n        x = x + self.dropout_2(self.ff(x2))\n        return x\n    \n# build a decoder layer with two multi-head attention layers and\n# one feed-forward layer\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout=0.1):\n        super().__init__()\n        self.norm_1 = nn.LayerNorm(d_model)\n        self.norm_2 = nn.LayerNorm(d_model)\n        self.norm_3 = nn.LayerNorm(d_model)\n        \n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n        \n        self.attn_1 = MultiHeadAttention(d_model, heads)\n        self.attn_2 = MultiHeadAttention(d_model, heads)\n        self.ff = FeedForward(d_model)\n    def forward(self, x, memory, mask = None):\n#         x2 = self.norm_1(x)\n#         x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n#         x2 = self.norm_2(x)\n#         x = x + self.dropout_2(self.attn_2(x2, x2, x2,\n#         src_mask))\n#         x2 = self.norm_3(x)\n#         x = x + self.dropout_3(self.ff(x2))\n        \n        #perform masked attention on input\n        #masked so queries cannot attend to subsequent keys\n        #Pass through sublayers of attention and feedforward.\n        #Apply dropout to sublayer output, add it to input, and norm.\n        attn = self.attn_1(x, x, x, mask)\n        x = x + self.dropout_1(attn)\n        x = self.norm_1(x)\n        x = x + self.dropout_2(self.attn_2(x, memory, memory, mask))\n        ff = self.ff(x)\n        x = x + self.dropout_2(ff)\n        x = self.norm_2(x)\n\n        return x\n#         return x\n# We can then build a convenient cloning function that can generate multiple layers:\ndef get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:29.785832Z","iopub.execute_input":"2021-08-29T17:11:29.786141Z","iopub.status.idle":"2021-08-29T17:11:29.802339Z","shell.execute_reply.started":"2021-08-29T17:11:29.786111Z","shell.execute_reply":"2021-08-29T17:11:29.801479Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, d_model, N, heads):\n        super().__init__()\n        self.N = N\n        self.layers = get_clones(EncoderLayer(d_model, heads), self.N)\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, src, mask):\n        x = self.layers[0](src, mask)\n        for i in range(1,self.N):\n            x = self.layers[i](x, mask)\n        return self.norm(x)\n    \nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, d_model, N, heads):\n        super().__init__()\n        self.N = N\n        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, src, trg, src_mask = None):\n        x = self.layers[0](src, trg, src_mask)\n        for i in range(1,self.N):\n            x = self.layers[i](x, trg, src_mask)\n        return self.norm(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:11:30.089969Z","iopub.execute_input":"2021-08-29T17:11:30.090276Z","iopub.status.idle":"2021-08-29T17:11:30.099241Z","shell.execute_reply.started":"2021-08-29T17:11:30.090245Z","shell.execute_reply":"2021-08-29T17:11:30.09815Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, d_model, nhead, nhid, nlayers, dropout=0.5, max_length = 2048):\n        super(TransformerModel, self).__init__()\n        try:\n            from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n        except:\n            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n        self.model_type = 'Transformer'\n        # original mask\n        self.src_mask = None\n        self.max_length = max_length\n        self.d_model = d_model\n        \n        # embedding encoding\n        self.embedding = nn.Embedding(ntoken, d_model)\n        \n        # positional encoding\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        \n        # encoder\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, nhid, dropout)\n        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n        \n#         self.decoder = nn.Linear(d_model, ntoken)\n\n\n        \n#         self.encoder = Encoder(d_model, nlayers, nhead)\n#         self.encoder.eval()\n        # decoder\n#         decoder_layers = TransformerDecoderLayer(d_model, nhead, nhid, dropout)\n#         self.decoder = TransformerDecoder(decoder_layers, nlayers)\n#         self.decoder = Decoder(ntoken, d_model, nlayers, nhead)\n#         self.decoder.eval()\n        # classification layer\n        self.classification_layer = nn.Linear(d_model, ntoken)\n        \n        self.init_weights()\n    def _generate_square_subsequent_mask(self, sz):\n        \n        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n#         mask = utils.get_masked_with_pad_tensor(self.max_length, x, x, config.pad_token)\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        nn.init.uniform_(self.embedding.weight, -initrange, initrange)\n        self.classification_layer.bias.data.zero_()\n        self.classification_layer.weight.data.uniform_(-initrange, initrange)\n#         nn.init.zeros_(self.decoder.weight)\n#         nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n\n    def forward(self, src, src_mask):\n        self.src_mask = src_mask\n\n        src = self.pos_encoder(self.embedding(src))\n        output = self.encoder(src, self.src_mask)\n#         output = self.decoder(output, src)\n        \n        #Flatten:\n#         shape = output.shape\n#         tensor_reshaped = output.reshape(shape[0],-1)\n#         #Drop all rows containing any nan:\n#         tensor_reshaped = tensor_reshaped[~torch.any(tensor_reshaped.isnan(),dim=1)]\n#         #Reshape back:\n#         output = tensor_reshaped.reshape(tensor_reshaped.shape[0],*shape[1:])\n#         print(output)\n        output = self.classification_layer(output)\n#         print(output)\n        return F.log_softmax(output, dim=-1)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:25:37.611339Z","iopub.execute_input":"2021-08-31T15:25:37.611673Z","iopub.status.idle":"2021-08-31T15:25:37.623368Z","shell.execute_reply.started":"2021-08-31T15:25:37.611643Z","shell.execute_reply":"2021-08-31T15:25:37.622441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ntokens = len(corpus.dictionary)\nemsize = 256\nnhead = 2\nnhid = 2048\nnlayer = 2\ndropout = 0.2\n# Loop over epochs.\nlr = 5\nbest_val_loss = None\nepochs = 400\nsave = './model.pt'\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:25:53.729903Z","iopub.execute_input":"2021-08-31T15:25:53.730219Z","iopub.status.idle":"2021-08-31T15:25:53.73519Z","shell.execute_reply.started":"2021-08-31T15:25:53.730192Z","shell.execute_reply":"2021-08-31T15:25:53.734282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_losses = []\nemsizes = []\nnheads = []\nnhids = []\nnlayers = []\n\nfor em in emsize:\n    for head in nhead:\n        for hid in nhid:\n            for layer in nlayer:\n                emsizes.append(em)\n                nheads.append(head)\n                nhids.append(hid)\n                nlayers.append(layer)\n                best_val_loss = None\n\n                model = TransformerModel(ntokens, em, head, hid, layer, dropout).to(device)\n\n                # At any point you can hit Ctrl + C to break out of training early.\n                try:\n                    for epoch in range(1, epochs+1):\n                        epoch_start_time = time.time()\n                        train()\n                        val_loss = evaluate(test_data)\n                        train_loss = evaluate(train_data)\n                        print('-' * 89)\n                        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | train loss {:5.2f}'\n                                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                                           val_loss, train_loss, math.exp(val_loss)))\n                        print('-' * 89)\n                        # Save the model if the validation loss is the best we've seen so far.\n                        if not best_val_loss or val_loss < best_val_loss:\n                            with open(save, 'wb') as f:\n                                torch.save(model, f)\n                            best_val_loss = val_loss\n                        else:\n                            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n                            lr /= 2.0\n                except KeyboardInterrupt:\n                    print('-' * 89)\n                    print('Exiting from training early')\n                final_losses.append(best_val_loss)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T18:56:39.549881Z","iopub.execute_input":"2021-08-29T18:56:39.550198Z","iopub.status.idle":"2021-08-29T18:56:39.581371Z","shell.execute_reply.started":"2021-08-29T18:56:39.550154Z","shell.execute_reply":"2021-08-29T18:56:39.580269Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = list(zip(emsizes, nheads, nhids, nlayers, final_losses))\nlosses_df = pd.DataFrame(scores, columns=['embedding_size', 'num_heads', 'num_hidden', 'num_layers', 'best_loss'])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:12:10.237712Z","iopub.execute_input":"2021-08-29T17:12:10.238023Z","iopub.status.idle":"2021-08-29T17:12:10.251059Z","shell.execute_reply.started":"2021-08-29T17:12:10.237993Z","shell.execute_reply":"2021-08-29T17:12:10.250173Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses_df","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:12:12.034177Z","iopub.execute_input":"2021-08-29T17:12:12.034511Z","iopub.status.idle":"2021-08-29T17:12:12.05353Z","shell.execute_reply.started":"2021-08-29T17:12:12.03448Z","shell.execute_reply":"2021-08-29T17:12:12.052781Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TransformerModel(ntokens, emsize, nhead, nhid, nlayer, dropout).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:26:01.240254Z","iopub.execute_input":"2021-08-31T15:26:01.240694Z","iopub.status.idle":"2021-08-31T15:26:01.342483Z","shell.execute_reply.started":"2021-08-31T15:26:01.240649Z","shell.execute_reply":"2021-08-31T15:26:01.341668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def repackage_hidden(h):\n    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:26:02.465938Z","iopub.execute_input":"2021-08-31T15:26:02.466255Z","iopub.status.idle":"2021-08-31T15:26:02.470712Z","shell.execute_reply.started":"2021-08-31T15:26:02.46622Z","shell.execute_reply":"2021-08-31T15:26:02.46964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_batch subdivides the source data into chunks of length args.bptt.\n# If source is equal to the example output of the batchify function, with\n# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n# ┌ a g m s ┐ ┌ b h n t ┐\n# └ b h n t ┘ └ c i o u ┘\n# Note that despite the name of the function, the subdivison of data is not\n# done along the batch dimension (i.e. dimension 1), since that was handled\n# by the batchify function. The chunks are along dimension 0, corresponding\n# to the seq_len dimension in the LSTM.\nseq_length = 35\ndef get_batch(source, i):\n    seq_len = min(seq_length, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:26:07.28298Z","iopub.execute_input":"2021-08-31T15:26:07.283292Z","iopub.status.idle":"2021-08-31T15:26:07.290157Z","shell.execute_reply.started":"2021-08-31T15:26:07.283264Z","shell.execute_reply":"2021-08-31T15:26:07.289314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz):\n    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:26:08.12252Z","iopub.execute_input":"2021-08-31T15:26:08.12296Z","iopub.status.idle":"2021-08-31T15:26:08.127529Z","shell.execute_reply.started":"2021-08-31T15:26:08.122921Z","shell.execute_reply":"2021-08-31T15:26:08.126509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, eval_data):\n    model.eval()  # turn on evaluation mode\n    total_loss = 0.\n    src_mask = generate_square_subsequent_mask(seq_length).to(device)\n    with torch.no_grad():\n        for i in range(0, eval_data.size(0) - 1, seq_length):\n            data, targets = get_batch(eval_data, i)\n            batch_size = data.size(0)\n            if batch_size != seq_length:\n                src_mask = src_mask[:batch_size, :batch_size]\n            output = model(data, src_mask)\n            output_flat = output.view(-1, ntokens)\n            total_loss += batch_size * criterion(output_flat, targets).item()\n    return total_loss / (len(eval_data) - 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:26:09.08093Z","iopub.execute_input":"2021-08-31T15:26:09.08125Z","iopub.status.idle":"2021-08-31T15:26:09.087687Z","shell.execute_reply.started":"2021-08-31T15:26:09.081219Z","shell.execute_reply":"2021-08-31T15:26:09.086734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\ndef train(model):\n    model.train()  # turn on train mode\n    total_loss = 0.\n    log_interval = 200\n    start_time = time.time()\n    src_mask = generate_square_subsequent_mask(seq_length).to(device)\n\n    num_batches = len(train_data) // seq_length\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_length)):\n        data, targets = get_batch(train_data, i)\n        batch_size = data.size(0)\n        if batch_size != seq_length:  # only on last batch\n            src_mask = src_mask[:batch_size, :batch_size]\n        output = model(data, src_mask)\n        loss = criterion(output.view(-1, ntokens), targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        total_loss += loss.item()\n        if batch % log_interval == 0 and batch > 0:\n            lr = scheduler.get_last_lr()[0]\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            ppl = math.exp(cur_loss)\n            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n            total_loss = 0\n            start_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:26:09.849189Z","iopub.execute_input":"2021-08-31T15:26:09.849513Z","iopub.status.idle":"2021-08-31T15:26:09.86003Z","shell.execute_reply.started":"2021-08-31T15:26:09.849483Z","shell.execute_reply":"2021-08-31T15:26:09.858882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    for epoch in range(1, epochs+1):\n        epoch_start_time = time.time()\n        train(model)\n        val_loss = evaluate(model, val_data)\n        train_loss = evaluate(model, train_data)\n        print('-' * 89)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | train loss {:5.2f} | '\n                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                           val_loss, train_loss, math.exp(val_loss)))\n        print('-' * 89)\n        # Save the model if the validation loss is the best we've seen so far.\n        if not best_val_loss or val_loss < best_val_loss:\n            with open(save, 'wb') as f:\n                torch.save(model, f)\n            best_val_loss = val_loss\n        else:\n            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n            lr /= 2.0\nexcept KeyboardInterrupt:\n    print('-' * 89)\n    print('Exiting from training early')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-31T16:10:39.256897Z","iopub.execute_input":"2021-08-31T16:10:39.257339Z","iopub.status.idle":"2021-08-31T16:13:52.087826Z","shell.execute_reply.started":"2021-08-31T16:10:39.257297Z","shell.execute_reply":"2021-08-31T16:13:52.086949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_generate = 500\ntemperature = 1.0\nsequence = []\nlog_interval = 50 # interval between logs","metadata":{"execution":{"iopub.status.busy":"2021-08-31T15:30:17.728498Z","iopub.execute_input":"2021-08-31T15:30:17.72901Z","iopub.status.idle":"2021-08-31T15:30:17.733444Z","shell.execute_reply.started":"2021-08-31T15:30:17.728972Z","shell.execute_reply":"2021-08-31T15:30:17.732611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\ninput = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n\nwith open('./output', 'w') as outf:\n    with torch.no_grad():  # no tracking history\n        for i in range(n_generate):\n            \n            output = model(input, False)\n            word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n            word_idx = torch.multinomial(word_weights, 1)[0]\n            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n            input = torch.cat([input, word_tensor], 0)\n\n\n            word = corpus.dictionary.idx2word[word_idx]\n\n            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n            \n            sequence.append(word)\n\n            if i % log_interval == 0:\n                print('| Generated {}/{} notes'.format(i, n_generate))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:45:01.097636Z","iopub.execute_input":"2021-08-15T15:45:01.097953Z","iopub.status.idle":"2021-08-15T15:45:15.004493Z","shell.execute_reply.started":"2021-08-15T15:45:01.097922Z","shell.execute_reply":"2021-08-15T15:45:15.003504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a MIDI file from prediction\ndef create_MIDI(gen, name=\"\"):\n    \n    # the offset is the time difference between notes, we assume its 0.5 here\n    offset = 0\n    music = []\n    \n    for seq in gen:\n        # chords are seperated by .\n        if ('.' in seq) or seq.isdigit():\n            chordnotes = seq.split('.')\n            n = []\n            \n            for cur_note in chordnotes:\n                new_n = note.Note(int(cur_note))\n                new_n.storedInstrument = instrument.Piano() # single piano instrument only\n                n.append(new_n)\n            \n            new_c = chord.Chord(n)\n            new_c.offset = offset\n            music.append(new_c)\n        \n        else:            \n            new_n = note.Note(seq)\n            new_n.storedInstrument = instrument.Piano() # single piano instrument only\n            new_n.offset = offset\n            music.append(new_n)\n        \n        offset += 0.5\n    \n    print(notes)\n    # producing a MIDI stream\n    midi = stream.Stream(music)\n    \n    midi.write('midi', fp='new_music_'+name+'.mid')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:45:15.005909Z","iopub.execute_input":"2021-08-15T15:45:15.006314Z","iopub.status.idle":"2021-08-15T15:45:15.013928Z","shell.execute_reply.started":"2021-08-15T15:45:15.006275Z","shell.execute_reply":"2021-08-15T15:45:15.013135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_MIDI(sequence, name='beeth_all_4.31')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:45:20.113825Z","iopub.execute_input":"2021-08-15T15:45:20.114158Z","iopub.status.idle":"2021-08-15T15:45:20.539404Z","shell.execute_reply.started":"2021-08-15T15:45:20.114125Z","shell.execute_reply":"2021-08-15T15:45:20.538626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_input(filename, folder=False):\n    # master list of notes\n    notes = []\n    \n    # converting folders with multiple MIDI files\n    if folder == True:\n        assert os.path.exists('../input/classical-music-midi/'+filename)\n        for file in glob.glob('../input/classical-music-midi/'+filename+'/*.mid'):\n            notes_per_piece = []\n            # read the MIDI file\n            midi = converter.parse(file)            \n            \n            try: # file has instrument parts\n                s2 = instrument.partitionByInstrument(midi)\n                notes_to_parse = s2.parts[1].recurse() \n            except: # file has notes in a flat structure\n                notes_to_parse = midi.flat.notes\n            \n#             print(notes_to_parse)\n            for element in notes_to_parse:\n                if isinstance(element, note.Note):\n                    notes_per_piece.append(str(element.pitch))\n                elif isinstance(element, chord.Chord):\n                    notes_per_piece.append('.'.join(str(n) for n in element.normalOrder))\n            notes.append(notes_per_piece)\n    else:\n        assert os.path.exists(filename)\n        midi = converter.parse(filename)\n        try: # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[1].recurse() \n        except: # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n#         print(notes_to_parse)\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n        \n\n    with open('./notes', 'wb') as filepath:\n        pickle.dump(notes, filepath)\n#     print(notes)\n    return notes","metadata":{"execution":{"iopub.status.busy":"2021-08-29T15:39:57.254503Z","iopub.execute_input":"2021-08-29T15:39:57.254905Z","iopub.status.idle":"2021-08-29T15:39:57.268593Z","shell.execute_reply.started":"2021-08-29T15:39:57.254859Z","shell.execute_reply":"2021-08-29T15:39:57.266854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notes = []\nwith (open(\"../input/beethoven-notes/notes\", \"rb\")) as openfile:\n    while True:\n        try:\n            notes = (pickle.load(openfile))\n        except EOFError:\n            break\n# notes = ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T14:30:06.488151Z","iopub.execute_input":"2021-08-29T14:30:06.488517Z","iopub.status.idle":"2021-08-29T14:30:06.523884Z","shell.execute_reply.started":"2021-08-29T14:30:06.488486Z","shell.execute_reply":"2021-08-29T14:30:06.523092Z"},"trusted":true},"execution_count":null,"outputs":[]}]}