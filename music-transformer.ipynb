{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom io import open\nimport tensorflow as tf\nimport glob\nimport pickle\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport time\nimport copy\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-19T19:41:46.599395Z","iopub.execute_input":"2021-08-19T19:41:46.599738Z","iopub.status.idle":"2021-08-19T19:41:46.623003Z","shell.execute_reply.started":"2021-08-19T19:41:46.599707Z","shell.execute_reply":"2021-08-19T19:41:46.622232Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install music21\n!pip install miditoolkit \n!pip install miditok","metadata":{"execution":{"iopub.status.busy":"2021-08-19T19:40:11.812467Z","iopub.execute_input":"2021-08-19T19:40:11.812949Z","iopub.status.idle":"2021-08-19T19:40:59.753504Z","shell.execute_reply.started":"2021-08-19T19:40:11.812908Z","shell.execute_reply":"2021-08-19T19:40:59.752552Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting music21\n  Downloading music21-6.7.1.tar.gz (19.2 MB)\n\u001b[K     |████████████████████████████████| 19.2 MB 471 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from music21) (4.0.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from music21) (1.0.1)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from music21) (8.8.0)\nCollecting webcolors\n  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: music21\n  Building wheel for music21 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for music21: filename=music21-6.7.1-py3-none-any.whl size=21941692 sha256=e6155d37ab223874b13f7f9769da3adc62a37bc41b2a3b8b2e65bd390a8d1ae9\n  Stored in directory: /root/.cache/pip/wheels/72/44/61/90e4e65262ca1b4d9f707527b540729ce3f64e00fc6b38d54c\nSuccessfully built music21\nInstalling collected packages: webcolors, music21\nSuccessfully installed music21-6.7.1 webcolors-1.11.1\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting miditoolkit\n  Downloading miditoolkit-0.1.14.tar.gz (18 kB)\nRequirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from miditoolkit) (1.19.5)\nCollecting mido>=1.1.16\n  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n\u001b[K     |████████████████████████████████| 51 kB 525 kB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: miditoolkit\n  Building wheel for miditoolkit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for miditoolkit: filename=miditoolkit-0.1.14-py3-none-any.whl size=19537 sha256=36bd393d5c36ca9b8f3912c117c0e516553c503e6dd282f07c741291aafffe26\n  Stored in directory: /root/.cache/pip/wheels/00/ac/58/1527193cdf6d74e3d056086f1be24f4fcb465c45212624b60c\nSuccessfully built miditoolkit\nInstalling collected packages: mido, miditoolkit\nSuccessfully installed miditoolkit-0.1.14 mido-1.2.10\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting miditok\n  Downloading miditok-0.0.2-py3-none-any.whl (41 kB)\n\u001b[K     |████████████████████████████████| 41 kB 404 kB/s eta 0:00:01\n\u001b[?25hCollecting numpy>=1.21\n  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n\u001b[K     |████████████████████████████████| 15.7 MB 1.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: miditoolkit>=0.1.14 in /opt/conda/lib/python3.7/site-packages (from miditok) (0.1.14)\nRequirement already satisfied: mido>=1.1.16 in /opt/conda/lib/python3.7/site-packages (from miditoolkit>=0.1.14->miditok) (1.2.10)\nInstalling collected packages: numpy, miditok\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.19.5\n    Uninstalling numpy-1.19.5:\n      Successfully uninstalled numpy-1.19.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-cloud 0.1.13 requires tensorflow<3.0,>=1.15.0, which is not installed.\nfancyimpute 0.5.5 requires tensorflow, which is not installed.\ndask-cudf 21.6.1+2.g101fc0fda4 requires cupy-cuda112, which is not installed.\ncudf 21.6.1+2.g101fc0fda4 requires cupy-cuda110, which is not installed.\nyellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.2 which is incompatible.\ntensorflow-gpu 2.4.1 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.4.2 which is incompatible.\nmatrixprofile 1.1.10 requires protobuf==3.11.2, but you have protobuf 3.17.3 which is incompatible.\nkornia 0.5.5 requires numpy<=1.19, but you have numpy 1.21.2 which is incompatible.\nimbalanced-learn 0.8.0 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\ndask-cudf 21.6.1+2.g101fc0fda4 requires dask<=2021.5.1,>=2021.4.0, but you have dask 2021.6.2 which is incompatible.\ndask-cudf 21.6.1+2.g101fc0fda4 requires distributed<=2021.5.1,>=2.22.0, but you have distributed 2021.6.2 which is incompatible.\u001b[0m\nSuccessfully installed miditok-0.0.2 numpy-1.21.2\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from music21 import converter, instrument, note, chord, stream\nfrom miditok import REMIEncoding","metadata":{"execution":{"iopub.status.busy":"2021-08-19T19:41:53.562183Z","iopub.execute_input":"2021-08-19T19:41:53.562504Z","iopub.status.idle":"2021-08-19T19:41:53.568622Z","shell.execute_reply.started":"2021-08-19T19:41:53.562475Z","shell.execute_reply":"2021-08-19T19:41:53.567758Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Creates the tokenizer and list the file paths\nremi_enc = REMIEncoding()  # uses defaults parameters\nfilename = 'beeth'\nfiles_paths = list(glob.glob('../input/classical-music-midi/'+filename+'/*.mid'))\n\n# A validation method to make sure to discard MIDIs we do not want\ndef midi_valid(midi) -> bool:\n    if any(ts.numerator != 4 or ts.denominator !=4 for ts in midi.time_signature_changes):\n        return False  # time signature different from 4/4\n    if max(note.end for note in midi.instruments[0].notes) < 10 * midi.ticks_per_beat:\n        return False  # this MIDI is too short\n    return True\n\n# Converts MIDI files to tokens saved as JSON files\n# remi_enc.tokenize_midi_dataset(files_paths, './', midi_valid)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T19:47:05.560077Z","iopub.execute_input":"2021-08-19T19:47:05.560427Z","iopub.status.idle":"2021-08-19T19:47:05.576923Z","shell.execute_reply.started":"2021-08-19T19:47:05.560396Z","shell.execute_reply":"2021-08-19T19:47:05.576060Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"notes = []\nfor file in glob.glob('../input/classical-music-midi/'+filename+'/*.mid'):\n\n    # read the MIDI file\n    midi = converter.parse(file)           \n    # Converts MIDI to tokens\n    tokens = remi_enc.midi_to_tokens(midi)\n    \n    notes.append(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T19:47:06.735160Z","iopub.execute_input":"2021-08-19T19:47:06.735482Z","iopub.status.idle":"2021-08-19T19:47:11.976712Z","shell.execute_reply.started":"2021-08-19T19:47:06.735454Z","shell.execute_reply":"2021-08-19T19:47:11.974608Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-f43b92ece468>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmidi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Converts MIDI to tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremi_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmidi_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnotes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/miditok/midi_tokenizer_base.py\u001b[0m in \u001b[0;36mmidi_to_tokens\u001b[0;34m(self, midi)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \"\"\"\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdurations_ticks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmidi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticks_per_beat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             self.durations_ticks[midi.ticks_per_beat] = [(beat * res + pos) * midi.ticks_per_beat // res\n","\u001b[0;31mAttributeError\u001b[0m: 'Score' object has no attribute 'ticks_per_beat'"],"ename":"AttributeError","evalue":"'Score' object has no attribute 'ticks_per_beat'","output_type":"error"}]},{"cell_type":"code","source":"notes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_input(filename, folder=False):\n    # master list of notes\n    notes = []\n    \n    # converting folders with multiple MIDI files\n    if folder == True:\n        assert os.path.exists('../input/classical-music-midi/'+filename)\n        for file in glob.glob('../input/classical-music-midi/'+filename+'/*.mid'):\n            notes_per_piece = []\n            # read the MIDI file\n            midi = converter.parse(file)            \n            \n            try: # file has instrument parts\n                s2 = instrument.partitionByInstrument(midi)\n                notes_to_parse = s2.parts[1].recurse() \n            except: # file has notes in a flat structure\n                notes_to_parse = midi.flat.notes\n            \n#             print(notes_to_parse)\n            for element in notes_to_parse:\n                if isinstance(element, note.Note):\n                    notes_per_piece.append(str(element.pitch))\n                elif isinstance(element, chord.Chord):\n                    notes_per_piece.append('.'.join(str(n) for n in element.normalOrder))\n            notes.append(notes_per_piece)\n    else:\n        assert os.path.exists(filename)\n        midi = converter.parse(filename)\n        try: # file has instrument parts\n            s2 = instrument.partitionByInstrument(midi)\n            notes_to_parse = s2.parts[1].recurse() \n        except: # file has notes in a flat structure\n            notes_to_parse = midi.flat.notes\n#         print(notes_to_parse)\n        for element in notes_to_parse:\n            if isinstance(element, note.Note):\n                notes.append(str(element.pitch))\n            elif isinstance(element, chord.Chord):\n                notes.append('.'.join(str(n) for n in element.normalOrder))\n        \n\n    with open('./notes', 'wb') as filepath:\n        pickle.dump(notes, filepath)\n#     print(notes)\n    return notes","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:01.393422Z","iopub.execute_input":"2021-08-18T17:36:01.393904Z","iopub.status.idle":"2021-08-18T17:36:01.412971Z","shell.execute_reply.started":"2021-08-18T17:36:01.39387Z","shell.execute_reply":"2021-08-18T17:36:01.411153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notes = preprocess_input('debussy', folder=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T09:06:26.326533Z","iopub.execute_input":"2021-08-16T09:06:26.326869Z","iopub.status.idle":"2021-08-16T09:06:55.454982Z","shell.execute_reply.started":"2021-08-16T09:06:26.326837Z","shell.execute_reply":"2021-08-16T09:06:55.453907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notes = []\nwith (open(\"../input/beethoven-notes/notes\", \"rb\")) as openfile:\n    while True:\n        try:\n            notes = (pickle.load(openfile))\n        except EOFError:\n            break\n# notes = ","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:04.31335Z","iopub.execute_input":"2021-08-18T17:36:04.313699Z","iopub.status.idle":"2021-08-18T17:36:04.376276Z","shell.execute_reply.started":"2021-08-18T17:36:04.313669Z","shell.execute_reply":"2021-08-18T17:36:04.375006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(notes[-6:])","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:05.557246Z","iopub.execute_input":"2021-08-18T17:36:05.557673Z","iopub.status.idle":"2021-08-18T17:36:05.568693Z","shell.execute_reply.started":"2021-08-18T17:36:05.557639Z","shell.execute_reply":"2021-08-18T17:36:05.567249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n    \n    def words(self):\n        return self.idx2word\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(notes[:-12])\n        self.valid = self.tokenize(notes[-12:-6])\n        self.test = self.tokenize(notes[-6:])\n\n    def tokenize(self, notes):\n        \"\"\"Tokenizes a note sequence\"\"\"\n        assert len(notes) > 0\n        \n        # Add notes to the dictionary\n        for song in notes:\n            for note in song:\n                self.dictionary.add_word(note)\n#         # Add words to the dictionary\n#         with open(path, 'r', encoding=\"utf8\") as f:\n#             for line in f:\n#                 words = line.split() + ['<eos>']\n#                 for word in words:\n#                     self.dictionary.add_word(word)\n\n        # Tokenize file content\n        idss = []\n        \n        for song in notes:\n            ids = []\n            for note in song:\n#                 print(note)\n                ids.append(self.dictionary.word2idx[note])\n        idss.append(torch.tensor(ids).type(torch.int64))\n        ids = torch.cat(idss)\n            \n#         with open(path, 'r', encoding=\"utf8\") as f:\n#             idss = []\n#             for line in f:\n#                 words = line.split() + ['<eos>']\n#                 ids = []\n#                 for word in words:\n#                     ids.append(self.dictionary.word2idx[word])\n#                 idss.append(torch.tensor(ids).type(torch.int64))\n#             ids = torch.cat(idss)\n\n        return ids","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:12.85594Z","iopub.execute_input":"2021-08-18T17:36:12.85633Z","iopub.status.idle":"2021-08-18T17:36:12.869873Z","shell.execute_reply.started":"2021-08-18T17:36:12.856298Z","shell.execute_reply":"2021-08-18T17:36:12.868548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = Corpus('../input/classical-music-midi/beeth')","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:14.301328Z","iopub.execute_input":"2021-08-18T17:36:14.30172Z","iopub.status.idle":"2021-08-18T17:36:14.389324Z","shell.execute_reply.started":"2021-08-18T17:36:14.301672Z","shell.execute_reply":"2021-08-18T17:36:14.38838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cpu'\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\neval_batch_size = 10\ntrain_data = batchify(corpus.train, eval_batch_size)\nval_data = batchify(corpus.valid, eval_batch_size)\ntest_data = batchify(corpus.test, eval_batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:15.060843Z","iopub.execute_input":"2021-08-18T17:36:15.061218Z","iopub.status.idle":"2021-08-18T17:36:15.073023Z","shell.execute_reply.started":"2021-08-18T17:36:15.061173Z","shell.execute_reply":"2021-08-18T17:36:15.071677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"\"\"Inject some information about the relative or absolute position of the tokens\n        in the sequence. The positional encodings have the same dimension as\n        the embeddings, so that the two can be summed. Here, we use sine and cosine\n        functions of different frequencies.\n    .. math::\n        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    Examples:\n        >>> pos_encoder = PositionalEncoding(d_model)\n    \"\"\"\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.d_model = d_model\n\n        # PE is the Positional Encoding matrix \n        # THIS STORES THE POSITIONS OF THE SEQUENCE\n        pe = torch.zeros(max_len, d_model)\n        \n        # Arange - RETURNS A RANGE BETWEEN VALUES, HERE IT IS 0 - max_len\n        # unsqueeze - adds a dimension, 1 means that each element in the first list is now in a list\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        \n        # division term, here it is (10000 ** ((2 * i)/d_model))\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        # calculating the position encoding for the even and odd terms\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        # Unsqueeze 0 will put PE in one list\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        Examples:\n            >>> output = pos_encoder(x)\n        \"\"\"\n        # make embeddings relatively larger\n        # This is so we do not lose the importance of the embedding\n        x = x * math.sqrt(self.d_model)\n        # we add the embedding to the PE \n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:15.784868Z","iopub.execute_input":"2021-08-18T17:36:15.785233Z","iopub.status.idle":"2021-08-18T17:36:15.796018Z","shell.execute_reply.started":"2021-08-18T17:36:15.785201Z","shell.execute_reply":"2021-08-18T17:36:15.794782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\"Parallel self attention given the number of heads and size\n    .. math::\n        X * Wk = K\n        X * Wq = Q\n        X * Wv = V\n        \\text{attention} = softmax(Q * transpose(K) / sqrt(d_model)) * V\n    Args:\n        d_model: the embed dim (default = 256).\n        heads: number of heads (default = 4)\n        max_length = max length of sequences (default = 2048)\n        dropout: the dropout value (default = 0.1).\n    Examples:\n        >>> attention = MultiHeadAttention(d_model, heads)\n    \"\"\"\n    def __init__(self, d_model = 256, heads = 4, max_length = 2048, dropout = 0.1):\n        super().__init__()\n        \n        self.d = d_model\n        self.h = heads\n        self.dh = d_model // heads\n        self.max_length = max_length\n        self.E = torch.randn([heads, self.max_length, self.dh])\n        \n        self.q_linear = nn.Linear(self.dh, self.dh)\n        self.v_linear = nn.Linear(self.dh, self.dh)\n        self.k_linear = nn.Linear(self.dh, self.dh)\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(d_model, d_model)\n    \n    def forward(self, q, k, v, mask=None):\n        \n        batch_size = q.size(0)\n        T = q.size(1)\n        dh = q.size(2) // self.h\n        \n        k = k.view(batch_size, T, self.h, dh)\n        q = q.view(batch_size, T, self.h, dh)\n        v = v.view(batch_size, T, self.h, dh)\n        # perform linear operation and split into h heads\n        \n        K = self.k_linear(k)\n        Q = self.q_linear(q)\n        V = self.v_linear(v)\n        \n        # transpose to get dimensions bs * h * sl * d_model\n       \n        K = K.transpose(1,2)\n        Q = Q.transpose(1,2)\n        V = V.transpose(1,2)\n\n        #start index of position embedding\n        \n        embedding_start = self.max_length - T\n        \n        #apply same position embeddings across the batch\n        \n        Er  = self.E[:, embedding_start:, :].unsqueeze(0)\n        \n        QE = torch.matmul(Q, K.transpose(-2, -1))\n        QE = self.mask_attention_positions(QE)\n        \n        #Get relative position attention scores\n        #combine batch with head dimension\n        SRel = self.skew_padding_position(QE)\n\n#         Q = Q.contiguous().view(batch_size*self.h, T, self.dh)\n#         K = K.contiguous().view(batch_size*self.h, T, self.dh)\n#         V = V.contiguous().view(batch_size*self.h, T, self.dh)\n        \n        #Compute scaled dot-product self-attention\n        #scale pre-matrix multiplication   \n        Q = Q / (q.size(2) ** (1/4))\n        K = K / (q.size(2) ** (1/4))\n\n        # calculate attention \n        scores, weights = self.attention(QE, SRel, V, mask, self.dropout)\n        \n        # concatenate heads and put through final linear layer\n        concat = scores.transpose(1,2).contiguous().view(batch_size, -1, self.d)\n        \n        output = self.out(concat)\n    \n        return output\n\n    def attention(self, QE, Srel, V, mask=None, dropout=None):\n        log = QE + Srel\n        log = log / math.sqrt(self.dh)\n\n        if mask is not None:\n            log += (mask.to(torch.int64) * -1e9).to(log.dtype)\n#             mask = mask.unsqueeze(1)\n#             print(mask.shape)\n#             log = log.masked_fill(mask == 0, float('-inf'))\n\n        scores = F.softmax(log, -1)\n        \n        if dropout is not None:\n            scores = dropout(scores)\n            \n#         print(scores.shape, V.shape)\n        attention = torch.matmul(scores, V)\n        \n#         scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n\n#         if mask is not None:\n#             print(scores.shape)\n\n#     #         mask = mask.unsqueeze(1)\n#             print(mask.shape)\n#     #         scores = scores.masked_fill(mask == 0, float('-inf'))\n#         scores = F.softmax(scores, dim=-1)\n\n#         \n\n#         output = torch.matmul(scores, v)\n        return attention, scores\n    \n    def mask_attention_positions(self, qe):\n        # to avoid looking backward by masking the positions\n        index = qe.shape[-1]\n        mask = torch.triu(torch.ones(index, index), 1).flip(1)\n        return qe.masked_fill((mask == 1), 0)\n    \n    def skew_padding_position(self, qe):\n        # to add padding to the skewed result after masking the matrix\n        # column of zeros on left\n        padded_qe = F.pad(qe, [1,0])\n        s = padded_qe.shape\n        padded_qe = padded_qe.view(s[0], s[1], s[3], s[2])\n        #take out first (padded) row\n        return padded_qe[:,:,1:,:]","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:17.306283Z","iopub.execute_input":"2021-08-18T17:36:17.306762Z","iopub.status.idle":"2021-08-18T17:36:17.348926Z","shell.execute_reply.started":"2021-08-18T17:36:17.30672Z","shell.execute_reply":"2021-08-18T17:36:17.347909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n#     def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n#         super().__init__() \n#         # We set d_ff as a default to 2048\n#         self.linear_1 = nn.Linear(d_model, d_ff)\n#         self.dropout = nn.Dropout(dropout)\n#         self.linear_2 = nn.Linear(d_ff, d_model)\n#     def forward(self, x):\n#         x = self.dropout(F.relu(self.linear_1(x)))\n#         x = self.linear_2(x)\n#         return x\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear_2(self.dropout(F.relu(self.linear_1(x))))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:17.914922Z","iopub.execute_input":"2021-08-18T17:36:17.915335Z","iopub.status.idle":"2021-08-18T17:36:17.924259Z","shell.execute_reply.started":"2021-08-18T17:36:17.915303Z","shell.execute_reply":"2021-08-18T17:36:17.923004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build an encoder layer with one multi-head attention layer and one # feed-forward layer\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout = 0.1):\n        super().__init__()\n        self.norm_1 = nn.LayerNorm(d_model)\n        self.norm_2 = nn.LayerNorm(d_model)\n        \n        self.attn = MultiHeadAttention(d_model, heads)\n        self.ff = FeedForward(d_model)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        x2 = self.norm_1(x)\n        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n        x2 = self.norm_2(x)\n        x = x + self.dropout_2(self.ff(x2))\n        return x\n    \n# build a decoder layer with two multi-head attention layers and\n# one feed-forward layer\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout=0.1):\n        super().__init__()\n        self.norm_1 = nn.LayerNorm(d_model)\n        self.norm_2 = nn.LayerNorm(d_model)\n        self.norm_3 = nn.LayerNorm(d_model)\n        \n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n        \n        self.attn_1 = MultiHeadAttention(d_model, heads)\n        self.attn_2 = MultiHeadAttention(d_model, heads)\n        self.ff = FeedForward(d_model)\n    def forward(self, x, memory, mask = None):\n#         x2 = self.norm_1(x)\n#         x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n#         x2 = self.norm_2(x)\n#         x = x + self.dropout_2(self.attn_2(x2, x2, x2,\n#         src_mask))\n#         x2 = self.norm_3(x)\n#         x = x + self.dropout_3(self.ff(x2))\n        \n        #perform masked attention on input\n        #masked so queries cannot attend to subsequent keys\n        #Pass through sublayers of attention and feedforward.\n        #Apply dropout to sublayer output, add it to input, and norm.\n        attn = self.attn_1(x, x, x, mask)\n        x = x + self.dropout_1(attn)\n        x = self.norm_1(x)\n        x = x + self.dropout_2(self.attn_2(x, memory, memory, mask))\n        ff = self.ff(x)\n        x = x + self.dropout_2(ff)\n        x = self.norm_2(x)\n\n        return x\n#         return x\n# We can then build a convenient cloning function that can generate multiple layers:\ndef get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:24.523381Z","iopub.execute_input":"2021-08-18T17:36:24.52382Z","iopub.status.idle":"2021-08-18T17:36:24.540747Z","shell.execute_reply.started":"2021-08-18T17:36:24.523782Z","shell.execute_reply":"2021-08-18T17:36:24.539228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, N, heads):\n        super().__init__()\n        self.N = N\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pe = PositionalEncoding(d_model)\n        self.layers = get_clones(EncoderLayer(d_model, heads), self.N)\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, src, mask):\n#         x = self.embed(src)\n#         x = self.pe(x)\n        x = self.layers[0](src, mask)\n        for i in range(1,self.N):\n            x = self.layers[i](x, mask)\n        return self.norm(x)\n    \nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, d_model, N, heads):\n        super().__init__()\n        self.N = N\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pe = PositionalEncoding(d_model)\n        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, src, trg, src_mask = None):\n#         x = self.embed(trg)\n#         x = self.pe(x)\n        x = self.layers[0](src, trg, src_mask)\n        for i in range(1,self.N):\n            x = self.layers[i](x, trg, src_mask)\n        return self.norm(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:38.084477Z","iopub.execute_input":"2021-08-18T17:36:38.08486Z","iopub.status.idle":"2021-08-18T17:36:38.0973Z","shell.execute_reply.started":"2021-08-18T17:36:38.084828Z","shell.execute_reply":"2021-08-18T17:36:38.095736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, d_model, nhead, nhid, nlayers, dropout=0.5, max_length = 2048):\n        super(TransformerModel, self).__init__()\n        try:\n            from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n        except:\n            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n        self.model_type = 'Transformer'\n        # original mask\n        self.src_mask = None\n        self.max_length = max_length\n        self.d_model = d_model\n        # positional encoding\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        # encoder\n#         encoder_layers = TransformerEncoderLayer(d_model, nhead, nhid, dropout)\n#         self.encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = Encoder(ntoken, d_model, nlayers, nhead)\n#         self.encoder.eval()\n        # decoder\n#         decoder_layers = TransformerDecoderLayer(d_model, nhead, nhid, dropout)\n#         self.decoder = TransformerDecoder(decoder_layers, nlayers)\n        self.decoder = Decoder(ntoken, d_model, nlayers, nhead)\n#         self.decoder.eval()\n        # embedding encoding\n        self.embedding = nn.Embedding(ntoken, d_model)\n        self.ninp = d_model\n#         self.decoder = nn.Linear(ninp, ntoken)\n#\n        # classification layer\n        self.classification_layer = nn.Linear(d_model, ntoken)\n\n        self.init_weights()\n\n    def _generate_square_subsequent_mask(self, sz):\n        \n        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n#         mask = utils.get_masked_with_pad_tensor(self.max_length, x, x, config.pad_token)\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        nn.init.uniform_(self.embedding.weight, -initrange, initrange)\n        nn.init.zeros_(self.classification_layer.weight)\n        nn.init.uniform_(self.classification_layer.weight, -initrange, initrange)\n\n    def forward(self, src, has_mask=True):\n        if has_mask:\n            device = src.device\n            if self.src_mask is None or self.src_mask.size(0) != len(src):\n                mask = self._generate_square_subsequent_mask(src.shape[1]).to(device)\n#                 mask = utils.get_masked_with_pad_tensor(self.max_length, src, src, self.d_model)\n                self.src_mask = mask\n        else:\n            self.src_mask = None\n\n#         src = self.embedding(src) * math.sqrt(self.ninp)\n        src_embedding = self.embedding(src)\n        src_embedding = self.pos_encoder(src_embedding)\n        output = self.encoder(src_embedding, self.src_mask)\n        output = self.decoder(output, self.embedding(src), self.src_mask)\n        \n        output = self.classification_layer(output) # projection to vocab size\n\n        return F.log_softmax(output, dim=-1)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:42.29666Z","iopub.execute_input":"2021-08-18T17:36:42.297071Z","iopub.status.idle":"2021-08-18T17:36:42.315484Z","shell.execute_reply.started":"2021-08-18T17:36:42.297038Z","shell.execute_reply":"2021-08-18T17:36:42.313244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"200 / 8","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:43.865894Z","iopub.execute_input":"2021-08-18T17:36:43.866299Z","iopub.status.idle":"2021-08-18T17:36:43.876959Z","shell.execute_reply.started":"2021-08-18T17:36:43.866267Z","shell.execute_reply":"2021-08-18T17:36:43.875477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ntokens = len(corpus.dictionary)\nemsize = 200\nnhead = 2\nnhid = 100\nnlayer = 6\ndropout = 0.2\n# Loop over epochs.\nlr = 5\nbest_val_loss = None\nepochs = 400\nsave = './model.pt'\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:42:54.008253Z","iopub.execute_input":"2021-08-18T17:42:54.008632Z","iopub.status.idle":"2021-08-18T17:42:54.018078Z","shell.execute_reply.started":"2021-08-18T17:42:54.008573Z","shell.execute_reply":"2021-08-18T17:42:54.017007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_losses = []\nemsizes = []\nnheads = []\nnhids = []\nnlayers = []\n\nfor em in emsize:\n    for head in nhead:\n        for hid in nhid:\n            for layer in nlayer:\n                emsizes.append(em)\n                nheads.append(head)\n                nhids.append(hid)\n                nlayers.append(layer)\n                best_val_loss = None\n\n                model = TransformerModel(ntokens, em, head, hid, layer, dropout).to(device)\n\n                # At any point you can hit Ctrl + C to break out of training early.\n                try:\n                    for epoch in range(1, epochs+1):\n                        epoch_start_time = time.time()\n                        train()\n                        val_loss = evaluate(val_data)\n                        print('-' * 89)\n                        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n                                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                                           val_loss, math.exp(val_loss)))\n                        print('-' * 89)\n                        # Save the model if the validation loss is the best we've seen so far.\n                        if not best_val_loss or val_loss < best_val_loss:\n                            with open(save, 'wb') as f:\n                                torch.save(model, f)\n                            best_val_loss = val_loss\n                        else:\n                            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n                            lr /= 2.0\n                except KeyboardInterrupt:\n                    print('-' * 89)\n                    print('Exiting from training early')\n                final_losses.append(best_val_loss)\n\n    \nscores = list(zip(emsizes, nheads, nhids, nlayers, final_losses))\nlosses_df = pd.DataFrame(scores, columns=['embedding_size', 'num_heads', 'num_hidden', 'num_layers', 'best_loss'])","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:36:59.748759Z","iopub.execute_input":"2021-08-18T17:36:59.749109Z","iopub.status.idle":"2021-08-18T17:37:00.188366Z","shell.execute_reply.started":"2021-08-18T17:36:59.749078Z","shell.execute_reply":"2021-08-18T17:37:00.186281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = list(zip(emsizes, nheads, nhids, nlayers, final_losses))\nlosses_df = pd.DataFrame(scores, columns=['embedding_size', 'num_heads', 'num_hidden', 'num_layers', 'best_loss'])","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:30:14.679097Z","iopub.execute_input":"2021-08-18T14:30:14.679425Z","iopub.status.idle":"2021-08-18T14:30:14.714236Z","shell.execute_reply.started":"2021-08-18T14:30:14.679393Z","shell.execute_reply":"2021-08-18T14:30:14.713059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TransformerModel(ntokens, emsize, nhead, nhid, nlayer, dropout).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:37:08.722088Z","iopub.execute_input":"2021-08-18T17:37:08.722492Z","iopub.status.idle":"2021-08-18T17:37:08.961289Z","shell.execute_reply.started":"2021-08-18T17:37:08.722461Z","shell.execute_reply":"2021-08-18T17:37:08.960194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def repackage_hidden(h):\n    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:37:09.495378Z","iopub.execute_input":"2021-08-18T17:37:09.49603Z","iopub.status.idle":"2021-08-18T17:37:09.503896Z","shell.execute_reply.started":"2021-08-18T17:37:09.495985Z","shell.execute_reply":"2021-08-18T17:37:09.50257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_batch subdivides the source data into chunks of length args.bptt.\n# If source is equal to the example output of the batchify function, with\n# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n# ┌ a g m s ┐ ┌ b h n t ┐\n# └ b h n t ┘ └ c i o u ┘\n# Note that despite the name of the function, the subdivison of data is not\n# done along the batch dimension (i.e. dimension 1), since that was handled\n# by the batchify function. The chunks are along dimension 0, corresponding\n# to the seq_len dimension in the LSTM.\nseq_length = 40\ndef get_batch(source, i):\n    seq_len = min(seq_length, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    return data, target","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:37:09.969763Z","iopub.execute_input":"2021-08-18T17:37:09.970102Z","iopub.status.idle":"2021-08-18T17:37:09.978004Z","shell.execute_reply.started":"2021-08-18T17:37:09.970075Z","shell.execute_reply":"2021-08-18T17:37:09.976433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def evaluate(data_source):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    total_loss = 0.\n    ntokens = len(corpus.dictionary)\n\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, seq_length):\n            data, targets = get_batch(data_source, i)\n            \n            output = model(data)\n            output = output.view(-1, ntokens)\n            \n            total_loss += len(data) * criterion(output, targets).item()\n    return total_loss / (len(data_source) - 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:37:10.987582Z","iopub.execute_input":"2021-08-18T17:37:10.988033Z","iopub.status.idle":"2021-08-18T17:37:10.998095Z","shell.execute_reply.started":"2021-08-18T17:37:10.988Z","shell.execute_reply":"2021-08-18T17:37:10.99426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    # Turn on training mode which enables dropout.\n    model.train()\n    total_loss = 0.\n    start_time = time.time()\n    ntokens = len(corpus.dictionary)\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    log_interval = 100\n    \n    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_length)):\n        \n        data, targets = get_batch(train_data, i)\n        batch_size = data.size(0)\n\n        output = model(data)\n        loss = criterion(output.view(-1, ntokens), targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n        \n        \n        total_loss += loss.item()\n\n        if batch % log_interval == 0 and batch > 0:\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            ppl = math.exp(cur_loss)\n            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n            total_loss = 0\n            start_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:37:11.540225Z","iopub.execute_input":"2021-08-18T17:37:11.540664Z","iopub.status.idle":"2021-08-18T17:37:11.552087Z","shell.execute_reply.started":"2021-08-18T17:37:11.540629Z","shell.execute_reply":"2021-08-18T17:37:11.550069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    for epoch in range(1, epochs+1):\n        epoch_start_time = time.time()\n        train()\n        val_loss = evaluate(val_data)\n        print('-' * 89)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                           val_loss, math.exp(val_loss)))\n        print('-' * 89)\n        # Save the model if the validation loss is the best we've seen so far.\n        if not best_val_loss or val_loss < best_val_loss:\n            with open(save, 'wb') as f:\n                torch.save(model, f)\n            best_val_loss = val_loss\n        else:\n            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n            lr /= 2.0\nexcept KeyboardInterrupt:\n    print('-' * 89)\n    print('Exiting from training early')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-18T17:42:58.576221Z","iopub.execute_input":"2021-08-18T17:42:58.576661Z","iopub.status.idle":"2021-08-18T18:27:26.87961Z","shell.execute_reply.started":"2021-08-18T17:42:58.576622Z","shell.execute_reply":"2021-08-18T18:27:26.878338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_generate = 500\ntemperature = 1.0\nsequence = []\nlog_interval = 50 # interval between logs","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:44:59.677424Z","iopub.execute_input":"2021-08-15T15:44:59.67781Z","iopub.status.idle":"2021-08-15T15:44:59.68169Z","shell.execute_reply.started":"2021-08-15T15:44:59.677775Z","shell.execute_reply":"2021-08-15T15:44:59.680878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\ninput = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n\nwith open('./output', 'w') as outf:\n    with torch.no_grad():  # no tracking history\n        for i in range(n_generate):\n            \n            output = model(input, False)\n            word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n            word_idx = torch.multinomial(word_weights, 1)[0]\n            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n            input = torch.cat([input, word_tensor], 0)\n\n\n            word = corpus.dictionary.idx2word[word_idx]\n\n            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n            \n            sequence.append(word)\n\n            if i % log_interval == 0:\n                print('| Generated {}/{} notes'.format(i, n_generate))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:45:01.097636Z","iopub.execute_input":"2021-08-15T15:45:01.097953Z","iopub.status.idle":"2021-08-15T15:45:15.004493Z","shell.execute_reply.started":"2021-08-15T15:45:01.097922Z","shell.execute_reply":"2021-08-15T15:45:15.003504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a MIDI file from prediction\ndef create_MIDI(gen, name=\"\"):\n    \n    # the offset is the time difference between notes, we assume its 0.5 here\n    offset = 0\n    music = []\n    \n    for seq in gen:\n        # chords are seperated by .\n        if ('.' in seq) or seq.isdigit():\n            chordnotes = seq.split('.')\n            n = []\n            \n            for cur_note in chordnotes:\n                new_n = note.Note(int(cur_note))\n                new_n.storedInstrument = instrument.Piano() # single piano instrument only\n                n.append(new_n)\n            \n            new_c = chord.Chord(n)\n            new_c.offset = offset\n            music.append(new_c)\n        \n        else:            \n            new_n = note.Note(seq)\n            new_n.storedInstrument = instrument.Piano() # single piano instrument only\n            new_n.offset = offset\n            music.append(new_n)\n        \n        offset += 0.5\n    \n    print(notes)\n    # producing a MIDI stream\n    midi = stream.Stream(music)\n    \n    midi.write('midi', fp='new_music_'+name+'.mid')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:45:15.005909Z","iopub.execute_input":"2021-08-15T15:45:15.006314Z","iopub.status.idle":"2021-08-15T15:45:15.013928Z","shell.execute_reply.started":"2021-08-15T15:45:15.006275Z","shell.execute_reply":"2021-08-15T15:45:15.013135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_MIDI(sequence, name='beeth_all_4.31')","metadata":{"execution":{"iopub.status.busy":"2021-08-15T15:45:20.113825Z","iopub.execute_input":"2021-08-15T15:45:20.114158Z","iopub.status.idle":"2021-08-15T15:45:20.539404Z","shell.execute_reply.started":"2021-08-15T15:45:20.114125Z","shell.execute_reply":"2021-08-15T15:45:20.538626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}